//
// Generated by NVIDIA NVVM Compiler
//
// Compiler Build ID: CL-35813241
// Cuda compilation tools, release 12.9, V12.9.41
// Based on NVVM 7.0.1
//

.version 8.8
.target sm_52
.address_size 64

	// .globl	fill_u8

.visible .entry fill_u8(
	.param .u64 fill_u8_param_0,
	.param .u8 fill_u8_param_1,
	.param .u64 fill_u8_param_2
)
{
	.reg .pred 	%p<3>;
	.reg .b16 	%rs<2>;
	.reg .b32 	%r<10>;
	.reg .b64 	%rd<9>;


	ld.param.u8 	%rs1, [fill_u8_param_1];
	ld.param.u64 	%rd5, [fill_u8_param_0];
	ld.param.u64 	%rd6, [fill_u8_param_2];
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r6, %ctaid.x;
	mov.u32 	%r7, %tid.x;
	mad.lo.s32 	%r9, %r6, %r1, %r7;
	cvt.u64.u32 	%rd8, %r9;
	setp.ge.u64 	%p1, %rd8, %rd6;
	@%p1 bra 	$L__BB0_3;

	mov.u32 	%r8, %nctaid.x;
	mul.lo.s32 	%r3, %r1, %r8;
	cvta.to.global.u64 	%rd2, %rd5;

$L__BB0_2:
	add.s64 	%rd7, %rd2, %rd8;
	st.global.u8 	[%rd7], %rs1;
	add.s32 	%r9, %r9, %r3;
	cvt.u64.u32 	%rd8, %r9;
	setp.lt.u64 	%p2, %rd8, %rd6;
	@%p2 bra 	$L__BB0_2;

$L__BB0_3:
	ret;

}
	// .globl	fill_u32
.visible .entry fill_u32(
	.param .u64 fill_u32_param_0,
	.param .u32 fill_u32_param_1,
	.param .u64 fill_u32_param_2
)
{
	.reg .pred 	%p<3>;
	.reg .b32 	%r<11>;
	.reg .b64 	%rd<10>;


	ld.param.u64 	%rd5, [fill_u32_param_0];
	ld.param.u32 	%r6, [fill_u32_param_1];
	ld.param.u64 	%rd6, [fill_u32_param_2];
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r7, %ctaid.x;
	mov.u32 	%r8, %tid.x;
	mad.lo.s32 	%r10, %r7, %r1, %r8;
	cvt.u64.u32 	%rd9, %r10;
	setp.ge.u64 	%p1, %rd9, %rd6;
	@%p1 bra 	$L__BB1_3;

	mov.u32 	%r9, %nctaid.x;
	mul.lo.s32 	%r3, %r1, %r9;
	cvta.to.global.u64 	%rd2, %rd5;

$L__BB1_2:
	shl.b64 	%rd7, %rd9, 2;
	add.s64 	%rd8, %rd2, %rd7;
	st.global.u32 	[%rd8], %r6;
	add.s32 	%r10, %r10, %r3;
	cvt.u64.u32 	%rd9, %r10;
	setp.lt.u64 	%p2, %rd9, %rd6;
	@%p2 bra 	$L__BB1_2;

$L__BB1_3:
	ret;

}
	// .globl	fill_i64
.visible .entry fill_i64(
	.param .u64 fill_i64_param_0,
	.param .u64 fill_i64_param_1,
	.param .u64 fill_i64_param_2
)
{
	.reg .pred 	%p<3>;
	.reg .b32 	%r<10>;
	.reg .b64 	%rd<11>;


	ld.param.u64 	%rd5, [fill_i64_param_0];
	ld.param.u64 	%rd6, [fill_i64_param_1];
	ld.param.u64 	%rd7, [fill_i64_param_2];
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r6, %ctaid.x;
	mov.u32 	%r7, %tid.x;
	mad.lo.s32 	%r9, %r6, %r1, %r7;
	cvt.u64.u32 	%rd10, %r9;
	setp.ge.u64 	%p1, %rd10, %rd7;
	@%p1 bra 	$L__BB2_3;

	mov.u32 	%r8, %nctaid.x;
	mul.lo.s32 	%r3, %r1, %r8;
	cvta.to.global.u64 	%rd2, %rd5;

$L__BB2_2:
	shl.b64 	%rd8, %rd10, 3;
	add.s64 	%rd9, %rd2, %rd8;
	st.global.u64 	[%rd9], %rd6;
	add.s32 	%r9, %r9, %r3;
	cvt.u64.u32 	%rd10, %r9;
	setp.lt.u64 	%p2, %rd10, %rd7;
	@%p2 bra 	$L__BB2_2;

$L__BB2_3:
	ret;

}
	// .globl	fill_f32
.visible .entry fill_f32(
	.param .u64 fill_f32_param_0,
	.param .f32 fill_f32_param_1,
	.param .u64 fill_f32_param_2
)
{
	.reg .pred 	%p<3>;
	.reg .f32 	%f<2>;
	.reg .b32 	%r<10>;
	.reg .b64 	%rd<10>;


	ld.param.u64 	%rd5, [fill_f32_param_0];
	ld.param.f32 	%f1, [fill_f32_param_1];
	ld.param.u64 	%rd6, [fill_f32_param_2];
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r6, %ctaid.x;
	mov.u32 	%r7, %tid.x;
	mad.lo.s32 	%r9, %r6, %r1, %r7;
	cvt.u64.u32 	%rd9, %r9;
	setp.ge.u64 	%p1, %rd9, %rd6;
	@%p1 bra 	$L__BB3_3;

	mov.u32 	%r8, %nctaid.x;
	mul.lo.s32 	%r3, %r1, %r8;
	cvta.to.global.u64 	%rd2, %rd5;

$L__BB3_2:
	shl.b64 	%rd7, %rd9, 2;
	add.s64 	%rd8, %rd2, %rd7;
	st.global.f32 	[%rd8], %f1;
	add.s32 	%r9, %r9, %r3;
	cvt.u64.u32 	%rd9, %r9;
	setp.lt.u64 	%p2, %rd9, %rd6;
	@%p2 bra 	$L__BB3_2;

$L__BB3_3:
	ret;

}
	// .globl	fill_f64
.visible .entry fill_f64(
	.param .u64 fill_f64_param_0,
	.param .f64 fill_f64_param_1,
	.param .u64 fill_f64_param_2
)
{
	.reg .pred 	%p<3>;
	.reg .b32 	%r<10>;
	.reg .f64 	%fd<2>;
	.reg .b64 	%rd<10>;


	ld.param.u64 	%rd5, [fill_f64_param_0];
	ld.param.f64 	%fd1, [fill_f64_param_1];
	ld.param.u64 	%rd6, [fill_f64_param_2];
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r6, %ctaid.x;
	mov.u32 	%r7, %tid.x;
	mad.lo.s32 	%r9, %r6, %r1, %r7;
	cvt.u64.u32 	%rd9, %r9;
	setp.ge.u64 	%p1, %rd9, %rd6;
	@%p1 bra 	$L__BB4_3;

	mov.u32 	%r8, %nctaid.x;
	mul.lo.s32 	%r3, %r1, %r8;
	cvta.to.global.u64 	%rd2, %rd5;

$L__BB4_2:
	shl.b64 	%rd7, %rd9, 3;
	add.s64 	%rd8, %rd2, %rd7;
	st.global.f64 	[%rd8], %fd1;
	add.s32 	%r9, %r9, %r3;
	cvt.u64.u32 	%rd9, %r9;
	setp.lt.u64 	%p2, %rd9, %rd6;
	@%p2 bra 	$L__BB4_2;

$L__BB4_3:
	ret;

}
	// .globl	copy2d_f32
.visible .entry copy2d_f32(
	.param .u64 copy2d_f32_param_0,
	.param .u64 copy2d_f32_param_1,
	.param .u32 copy2d_f32_param_2,
	.param .u32 copy2d_f32_param_3,
	.param .u32 copy2d_f32_param_4,
	.param .u32 copy2d_f32_param_5
)
{
	.reg .pred 	%p<2>;
	.reg .f32 	%f<2>;
	.reg .b32 	%r<15>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [copy2d_f32_param_0];
	ld.param.u64 	%rd2, [copy2d_f32_param_1];
	ld.param.u32 	%r5, [copy2d_f32_param_2];
	ld.param.u32 	%r2, [copy2d_f32_param_3];
	ld.param.u32 	%r3, [copy2d_f32_param_4];
	ld.param.u32 	%r4, [copy2d_f32_param_5];
	mov.u32 	%r6, %ctaid.x;
	mov.u32 	%r7, %ntid.x;
	mov.u32 	%r8, %tid.x;
	mad.lo.s32 	%r1, %r6, %r7, %r8;
	mul.lo.s32 	%r9, %r2, %r5;
	setp.ge.u32 	%p1, %r1, %r9;
	@%p1 bra 	$L__BB5_2;

	cvta.to.global.u64 	%rd3, %rd1;
	div.u32 	%r10, %r1, %r2;
	mul.lo.s32 	%r11, %r10, %r2;
	sub.s32 	%r12, %r1, %r11;
	mad.lo.s32 	%r13, %r10, %r3, %r12;
	mul.wide.u32 	%rd4, %r13, 4;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.f32 	%f1, [%rd5];
	mad.lo.s32 	%r14, %r10, %r4, %r12;
	cvta.to.global.u64 	%rd6, %rd2;
	mul.wide.u32 	%rd7, %r14, 4;
	add.s64 	%rd8, %rd6, %rd7;
	st.global.f32 	[%rd8], %f1;

$L__BB5_2:
	ret;

}
	// .globl	copy2d_f64
.visible .entry copy2d_f64(
	.param .u64 copy2d_f64_param_0,
	.param .u64 copy2d_f64_param_1,
	.param .u32 copy2d_f64_param_2,
	.param .u32 copy2d_f64_param_3,
	.param .u32 copy2d_f64_param_4,
	.param .u32 copy2d_f64_param_5
)
{
	.reg .pred 	%p<2>;
	.reg .b32 	%r<15>;
	.reg .f64 	%fd<2>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [copy2d_f64_param_0];
	ld.param.u64 	%rd2, [copy2d_f64_param_1];
	ld.param.u32 	%r5, [copy2d_f64_param_2];
	ld.param.u32 	%r2, [copy2d_f64_param_3];
	ld.param.u32 	%r3, [copy2d_f64_param_4];
	ld.param.u32 	%r4, [copy2d_f64_param_5];
	mov.u32 	%r6, %ctaid.x;
	mov.u32 	%r7, %ntid.x;
	mov.u32 	%r8, %tid.x;
	mad.lo.s32 	%r1, %r6, %r7, %r8;
	mul.lo.s32 	%r9, %r2, %r5;
	setp.ge.u32 	%p1, %r1, %r9;
	@%p1 bra 	$L__BB6_2;

	cvta.to.global.u64 	%rd3, %rd1;
	div.u32 	%r10, %r1, %r2;
	mul.lo.s32 	%r11, %r10, %r2;
	sub.s32 	%r12, %r1, %r11;
	mad.lo.s32 	%r13, %r10, %r3, %r12;
	mul.wide.u32 	%rd4, %r13, 8;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.f64 	%fd1, [%rd5];
	mad.lo.s32 	%r14, %r10, %r4, %r12;
	cvta.to.global.u64 	%rd6, %rd2;
	mul.wide.u32 	%rd7, %r14, 8;
	add.s64 	%rd8, %rd6, %rd7;
	st.global.f64 	[%rd8], %fd1;

$L__BB6_2:
	ret;

}
	// .globl	copy2d_u8
.visible .entry copy2d_u8(
	.param .u64 copy2d_u8_param_0,
	.param .u64 copy2d_u8_param_1,
	.param .u32 copy2d_u8_param_2,
	.param .u32 copy2d_u8_param_3,
	.param .u32 copy2d_u8_param_4,
	.param .u32 copy2d_u8_param_5
)
{
	.reg .pred 	%p<2>;
	.reg .b16 	%rs<2>;
	.reg .b32 	%r<15>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [copy2d_u8_param_0];
	ld.param.u64 	%rd2, [copy2d_u8_param_1];
	ld.param.u32 	%r5, [copy2d_u8_param_2];
	ld.param.u32 	%r2, [copy2d_u8_param_3];
	ld.param.u32 	%r3, [copy2d_u8_param_4];
	ld.param.u32 	%r4, [copy2d_u8_param_5];
	mov.u32 	%r6, %ctaid.x;
	mov.u32 	%r7, %ntid.x;
	mov.u32 	%r8, %tid.x;
	mad.lo.s32 	%r1, %r6, %r7, %r8;
	mul.lo.s32 	%r9, %r2, %r5;
	setp.ge.u32 	%p1, %r1, %r9;
	@%p1 bra 	$L__BB7_2;

	cvta.to.global.u64 	%rd3, %rd1;
	div.u32 	%r10, %r1, %r2;
	mul.lo.s32 	%r11, %r10, %r2;
	sub.s32 	%r12, %r1, %r11;
	mad.lo.s32 	%r13, %r10, %r3, %r12;
	cvt.u64.u32 	%rd4, %r13;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.u8 	%rs1, [%rd5];
	mad.lo.s32 	%r14, %r10, %r4, %r12;
	cvt.u64.u32 	%rd6, %r14;
	cvta.to.global.u64 	%rd7, %rd2;
	add.s64 	%rd8, %rd7, %rd6;
	st.global.u8 	[%rd8], %rs1;

$L__BB7_2:
	ret;

}
	// .globl	copy2d_u32
.visible .entry copy2d_u32(
	.param .u64 copy2d_u32_param_0,
	.param .u64 copy2d_u32_param_1,
	.param .u32 copy2d_u32_param_2,
	.param .u32 copy2d_u32_param_3,
	.param .u32 copy2d_u32_param_4,
	.param .u32 copy2d_u32_param_5
)
{
	.reg .pred 	%p<2>;
	.reg .b32 	%r<16>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [copy2d_u32_param_0];
	ld.param.u64 	%rd2, [copy2d_u32_param_1];
	ld.param.u32 	%r5, [copy2d_u32_param_2];
	ld.param.u32 	%r2, [copy2d_u32_param_3];
	ld.param.u32 	%r3, [copy2d_u32_param_4];
	ld.param.u32 	%r4, [copy2d_u32_param_5];
	mov.u32 	%r6, %ctaid.x;
	mov.u32 	%r7, %ntid.x;
	mov.u32 	%r8, %tid.x;
	mad.lo.s32 	%r1, %r6, %r7, %r8;
	mul.lo.s32 	%r9, %r2, %r5;
	setp.ge.u32 	%p1, %r1, %r9;
	@%p1 bra 	$L__BB8_2;

	cvta.to.global.u64 	%rd3, %rd1;
	div.u32 	%r10, %r1, %r2;
	mul.lo.s32 	%r11, %r10, %r2;
	sub.s32 	%r12, %r1, %r11;
	mad.lo.s32 	%r13, %r10, %r3, %r12;
	mul.wide.u32 	%rd4, %r13, 4;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.u32 	%r14, [%rd5];
	mad.lo.s32 	%r15, %r10, %r4, %r12;
	cvta.to.global.u64 	%rd6, %rd2;
	mul.wide.u32 	%rd7, %r15, 4;
	add.s64 	%rd8, %rd6, %rd7;
	st.global.u32 	[%rd8], %r14;

$L__BB8_2:
	ret;

}
	// .globl	copy2d_i64
.visible .entry copy2d_i64(
	.param .u64 copy2d_i64_param_0,
	.param .u64 copy2d_i64_param_1,
	.param .u32 copy2d_i64_param_2,
	.param .u32 copy2d_i64_param_3,
	.param .u32 copy2d_i64_param_4,
	.param .u32 copy2d_i64_param_5
)
{
	.reg .pred 	%p<2>;
	.reg .b32 	%r<15>;
	.reg .b64 	%rd<10>;


	ld.param.u64 	%rd1, [copy2d_i64_param_0];
	ld.param.u64 	%rd2, [copy2d_i64_param_1];
	ld.param.u32 	%r5, [copy2d_i64_param_2];
	ld.param.u32 	%r2, [copy2d_i64_param_3];
	ld.param.u32 	%r3, [copy2d_i64_param_4];
	ld.param.u32 	%r4, [copy2d_i64_param_5];
	mov.u32 	%r6, %ctaid.x;
	mov.u32 	%r7, %ntid.x;
	mov.u32 	%r8, %tid.x;
	mad.lo.s32 	%r1, %r6, %r7, %r8;
	mul.lo.s32 	%r9, %r2, %r5;
	setp.ge.u32 	%p1, %r1, %r9;
	@%p1 bra 	$L__BB9_2;

	cvta.to.global.u64 	%rd3, %rd1;
	div.u32 	%r10, %r1, %r2;
	mul.lo.s32 	%r11, %r10, %r2;
	sub.s32 	%r12, %r1, %r11;
	mad.lo.s32 	%r13, %r10, %r3, %r12;
	mul.wide.u32 	%rd4, %r13, 8;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.u64 	%rd6, [%rd5];
	mad.lo.s32 	%r14, %r10, %r4, %r12;
	cvta.to.global.u64 	%rd7, %rd2;
	mul.wide.u32 	%rd8, %r14, 8;
	add.s64 	%rd9, %rd7, %rd8;
	st.global.u64 	[%rd9], %rd6;

$L__BB9_2:
	ret;

}
	// .globl	const_set_f32
.visible .entry const_set_f32(
	.param .u64 const_set_f32_param_0,
	.param .u64 const_set_f32_param_1,
	.param .u64 const_set_f32_param_2,
	.param .f32 const_set_f32_param_3,
	.param .u64 const_set_f32_param_4
)
{
	.reg .pred 	%p<20>;
	.reg .f32 	%f<2>;
	.reg .b32 	%r<41>;
	.reg .b64 	%rd<53>;


	ld.param.u64 	%rd19, [const_set_f32_param_0];
	ld.param.u64 	%rd20, [const_set_f32_param_1];
	ld.param.u64 	%rd21, [const_set_f32_param_2];
	ld.param.f32 	%f1, [const_set_f32_param_3];
	ld.param.u64 	%rd22, [const_set_f32_param_4];
	cvta.to.global.u64 	%rd1, %rd22;
	cvta.to.global.u64 	%rd2, %rd21;
	setp.eq.s64 	%p3, %rd20, 0;
	setp.eq.s64 	%p4, %rd21, 0;
	or.pred  	%p5, %p4, %p3;
	mov.pred 	%p2, -1;
	mov.pred 	%p19, %p2;
	@%p5 bra 	$L__BB10_5;

	mov.u64 	%rd49, 1;
	mov.u32 	%r34, 0;

$L__BB10_2:
	not.b32 	%r20, %r34;
	cvt.u64.u32 	%rd24, %r20;
	add.s64 	%rd25, %rd24, %rd20;
	shl.b64 	%rd26, %rd25, 3;
	and.b64  	%rd27, %rd26, 34359738360;
	add.s64 	%rd4, %rd2, %rd27;
	ld.global.u64 	%rd5, [%rd4];
	setp.lt.u64 	%p6, %rd5, 2;
	@%p6 bra 	$L__BB10_4;

	shl.b64 	%rd28, %rd20, 3;
	add.s64 	%rd29, %rd4, %rd28;
	ld.global.u64 	%rd30, [%rd29];
	setp.ne.s64 	%p8, %rd49, %rd30;
	mov.pred 	%p19, 0;
	@%p8 bra 	$L__BB10_5;

$L__BB10_4:
	mul.lo.s64 	%rd49, %rd5, %rd49;
	add.s32 	%r34, %r34, 1;
	cvt.u64.u32 	%rd31, %r34;
	setp.lt.u64 	%p10, %rd31, %rd20;
	mov.pred 	%p19, %p2;
	@%p10 bra 	$L__BB10_2;

$L__BB10_5:
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r21, %ctaid.x;
	mov.u32 	%r22, %tid.x;
	mad.lo.s32 	%r35, %r21, %r3, %r22;
	cvt.u64.u32 	%rd52, %r35;
	@%p19 bra 	$L__BB10_16;
	bra.uni 	$L__BB10_6;

$L__BB10_16:
	setp.ge.u64 	%p17, %rd52, %rd19;
	@%p17 bra 	$L__BB10_19;

	mov.u32 	%r33, %nctaid.x;
	mul.lo.s32 	%r16, %r3, %r33;

$L__BB10_18:
	shl.b64 	%rd47, %rd52, 2;
	add.s64 	%rd48, %rd1, %rd47;
	st.global.f32 	[%rd48], %f1;
	add.s32 	%r35, %r35, %r16;
	cvt.u64.u32 	%rd52, %r35;
	setp.lt.u64 	%p18, %rd52, %rd19;
	@%p18 bra 	$L__BB10_18;
	bra.uni 	$L__BB10_19;

$L__BB10_6:
	setp.ge.u64 	%p11, %rd52, %rd19;
	@%p11 bra 	$L__BB10_19;

	mov.u32 	%r23, %nctaid.x;
	mul.lo.s32 	%r5, %r3, %r23;
	@%p3 bra 	$L__BB10_14;

$L__BB10_8:
	mov.u32 	%r36, 0;
	mov.u32 	%r37, %r35;
	mov.u32 	%r38, %r36;

$L__BB10_9:
	not.b32 	%r26, %r36;
	cvt.u64.u32 	%rd32, %r26;
	add.s64 	%rd33, %rd32, %rd20;
	cvt.u64.u32 	%rd8, %r37;
	shl.b64 	%rd34, %rd33, 3;
	and.b64  	%rd35, %rd34, 34359738360;
	add.s64 	%rd9, %rd2, %rd35;
	ld.global.u64 	%rd10, [%rd9];
	and.b64  	%rd36, %rd10, -4294967296;
	setp.eq.s64 	%p13, %rd36, 0;
	@%p13 bra 	$L__BB10_11;

	div.u64 	%rd50, %rd8, %rd10;
	mul.lo.s64 	%rd37, %rd50, %rd10;
	sub.s64 	%rd51, %rd8, %rd37;
	bra.uni 	$L__BB10_12;

$L__BB10_11:
	cvt.u32.u64 	%r27, %rd10;
	cvt.u32.u64 	%r28, %rd8;
	div.u32 	%r29, %r28, %r27;
	mul.lo.s32 	%r30, %r29, %r27;
	sub.s32 	%r31, %r28, %r30;
	cvt.u64.u32 	%rd50, %r29;
	cvt.u64.u32 	%rd51, %r31;

$L__BB10_12:
	shl.b64 	%rd38, %rd20, 3;
	add.s64 	%rd39, %rd9, %rd38;
	ld.global.u64 	%rd40, [%rd39];
	mul.lo.s64 	%rd41, %rd40, %rd51;
	cvt.u32.u64 	%r32, %rd41;
	add.s32 	%r38, %r38, %r32;
	cvt.u32.u64 	%r37, %rd50;
	add.s32 	%r36, %r36, 1;
	cvt.u64.u32 	%rd42, %r36;
	setp.lt.u64 	%p14, %rd42, %rd20;
	@%p14 bra 	$L__BB10_9;

	mul.wide.u32 	%rd43, %r38, 4;
	add.s64 	%rd44, %rd1, %rd43;
	st.global.f32 	[%rd44], %f1;
	add.s32 	%r35, %r35, %r5;
	cvt.u64.u32 	%rd45, %r35;
	setp.lt.u64 	%p15, %rd45, %rd19;
	@%p15 bra 	$L__BB10_8;
	bra.uni 	$L__BB10_19;

$L__BB10_14:
	add.s32 	%r35, %r35, %r5;
	cvt.u64.u32 	%rd46, %r35;
	setp.lt.u64 	%p16, %rd46, %rd19;
	@%p16 bra 	$L__BB10_14;

	st.global.f32 	[%rd1], %f1;

$L__BB10_19:
	ret;

}
	// .globl	const_set_f64
.visible .entry const_set_f64(
	.param .u64 const_set_f64_param_0,
	.param .u64 const_set_f64_param_1,
	.param .u64 const_set_f64_param_2,
	.param .f64 const_set_f64_param_3,
	.param .u64 const_set_f64_param_4
)
{
	.reg .pred 	%p<20>;
	.reg .b32 	%r<41>;
	.reg .f64 	%fd<2>;
	.reg .b64 	%rd<53>;


	ld.param.u64 	%rd19, [const_set_f64_param_0];
	ld.param.u64 	%rd20, [const_set_f64_param_1];
	ld.param.u64 	%rd21, [const_set_f64_param_2];
	ld.param.f64 	%fd1, [const_set_f64_param_3];
	ld.param.u64 	%rd22, [const_set_f64_param_4];
	cvta.to.global.u64 	%rd1, %rd22;
	cvta.to.global.u64 	%rd2, %rd21;
	setp.eq.s64 	%p3, %rd20, 0;
	setp.eq.s64 	%p4, %rd21, 0;
	or.pred  	%p5, %p4, %p3;
	mov.pred 	%p2, -1;
	mov.pred 	%p19, %p2;
	@%p5 bra 	$L__BB11_5;

	mov.u64 	%rd49, 1;
	mov.u32 	%r34, 0;

$L__BB11_2:
	not.b32 	%r20, %r34;
	cvt.u64.u32 	%rd24, %r20;
	add.s64 	%rd25, %rd24, %rd20;
	shl.b64 	%rd26, %rd25, 3;
	and.b64  	%rd27, %rd26, 34359738360;
	add.s64 	%rd4, %rd2, %rd27;
	ld.global.u64 	%rd5, [%rd4];
	setp.lt.u64 	%p6, %rd5, 2;
	@%p6 bra 	$L__BB11_4;

	shl.b64 	%rd28, %rd20, 3;
	add.s64 	%rd29, %rd4, %rd28;
	ld.global.u64 	%rd30, [%rd29];
	setp.ne.s64 	%p8, %rd49, %rd30;
	mov.pred 	%p19, 0;
	@%p8 bra 	$L__BB11_5;

$L__BB11_4:
	mul.lo.s64 	%rd49, %rd5, %rd49;
	add.s32 	%r34, %r34, 1;
	cvt.u64.u32 	%rd31, %r34;
	setp.lt.u64 	%p10, %rd31, %rd20;
	mov.pred 	%p19, %p2;
	@%p10 bra 	$L__BB11_2;

$L__BB11_5:
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r21, %ctaid.x;
	mov.u32 	%r22, %tid.x;
	mad.lo.s32 	%r35, %r21, %r3, %r22;
	cvt.u64.u32 	%rd52, %r35;
	@%p19 bra 	$L__BB11_16;
	bra.uni 	$L__BB11_6;

$L__BB11_16:
	setp.ge.u64 	%p17, %rd52, %rd19;
	@%p17 bra 	$L__BB11_19;

	mov.u32 	%r33, %nctaid.x;
	mul.lo.s32 	%r16, %r3, %r33;

$L__BB11_18:
	shl.b64 	%rd47, %rd52, 3;
	add.s64 	%rd48, %rd1, %rd47;
	st.global.f64 	[%rd48], %fd1;
	add.s32 	%r35, %r35, %r16;
	cvt.u64.u32 	%rd52, %r35;
	setp.lt.u64 	%p18, %rd52, %rd19;
	@%p18 bra 	$L__BB11_18;
	bra.uni 	$L__BB11_19;

$L__BB11_6:
	setp.ge.u64 	%p11, %rd52, %rd19;
	@%p11 bra 	$L__BB11_19;

	mov.u32 	%r23, %nctaid.x;
	mul.lo.s32 	%r5, %r3, %r23;
	@%p3 bra 	$L__BB11_14;

$L__BB11_8:
	mov.u32 	%r36, 0;
	mov.u32 	%r37, %r35;
	mov.u32 	%r38, %r36;

$L__BB11_9:
	not.b32 	%r26, %r36;
	cvt.u64.u32 	%rd32, %r26;
	add.s64 	%rd33, %rd32, %rd20;
	cvt.u64.u32 	%rd8, %r37;
	shl.b64 	%rd34, %rd33, 3;
	and.b64  	%rd35, %rd34, 34359738360;
	add.s64 	%rd9, %rd2, %rd35;
	ld.global.u64 	%rd10, [%rd9];
	and.b64  	%rd36, %rd10, -4294967296;
	setp.eq.s64 	%p13, %rd36, 0;
	@%p13 bra 	$L__BB11_11;

	div.u64 	%rd50, %rd8, %rd10;
	mul.lo.s64 	%rd37, %rd50, %rd10;
	sub.s64 	%rd51, %rd8, %rd37;
	bra.uni 	$L__BB11_12;

$L__BB11_11:
	cvt.u32.u64 	%r27, %rd10;
	cvt.u32.u64 	%r28, %rd8;
	div.u32 	%r29, %r28, %r27;
	mul.lo.s32 	%r30, %r29, %r27;
	sub.s32 	%r31, %r28, %r30;
	cvt.u64.u32 	%rd50, %r29;
	cvt.u64.u32 	%rd51, %r31;

$L__BB11_12:
	shl.b64 	%rd38, %rd20, 3;
	add.s64 	%rd39, %rd9, %rd38;
	ld.global.u64 	%rd40, [%rd39];
	mul.lo.s64 	%rd41, %rd40, %rd51;
	cvt.u32.u64 	%r32, %rd41;
	add.s32 	%r38, %r38, %r32;
	cvt.u32.u64 	%r37, %rd50;
	add.s32 	%r36, %r36, 1;
	cvt.u64.u32 	%rd42, %r36;
	setp.lt.u64 	%p14, %rd42, %rd20;
	@%p14 bra 	$L__BB11_9;

	mul.wide.u32 	%rd43, %r38, 8;
	add.s64 	%rd44, %rd1, %rd43;
	st.global.f64 	[%rd44], %fd1;
	add.s32 	%r35, %r35, %r5;
	cvt.u64.u32 	%rd45, %r35;
	setp.lt.u64 	%p15, %rd45, %rd19;
	@%p15 bra 	$L__BB11_8;
	bra.uni 	$L__BB11_19;

$L__BB11_14:
	add.s32 	%r35, %r35, %r5;
	cvt.u64.u32 	%rd46, %r35;
	setp.lt.u64 	%p16, %rd46, %rd19;
	@%p16 bra 	$L__BB11_14;

	st.global.f64 	[%rd1], %fd1;

$L__BB11_19:
	ret;

}
	// .globl	const_set_u8
.visible .entry const_set_u8(
	.param .u64 const_set_u8_param_0,
	.param .u64 const_set_u8_param_1,
	.param .u64 const_set_u8_param_2,
	.param .u8 const_set_u8_param_3,
	.param .u64 const_set_u8_param_4
)
{
	.reg .pred 	%p<20>;
	.reg .b16 	%rs<2>;
	.reg .b32 	%r<41>;
	.reg .b64 	%rd<52>;


	ld.param.u8 	%rs1, [const_set_u8_param_3];
	ld.param.u64 	%rd19, [const_set_u8_param_0];
	ld.param.u64 	%rd20, [const_set_u8_param_1];
	ld.param.u64 	%rd21, [const_set_u8_param_2];
	ld.param.u64 	%rd22, [const_set_u8_param_4];
	cvta.to.global.u64 	%rd1, %rd22;
	cvta.to.global.u64 	%rd2, %rd21;
	setp.eq.s64 	%p3, %rd20, 0;
	setp.eq.s64 	%p4, %rd21, 0;
	or.pred  	%p5, %p4, %p3;
	mov.pred 	%p2, -1;
	mov.pred 	%p19, %p2;
	@%p5 bra 	$L__BB12_5;

	mov.u64 	%rd48, 1;
	mov.u32 	%r34, 0;

$L__BB12_2:
	not.b32 	%r20, %r34;
	cvt.u64.u32 	%rd24, %r20;
	add.s64 	%rd25, %rd24, %rd20;
	shl.b64 	%rd26, %rd25, 3;
	and.b64  	%rd27, %rd26, 34359738360;
	add.s64 	%rd4, %rd2, %rd27;
	ld.global.u64 	%rd5, [%rd4];
	setp.lt.u64 	%p6, %rd5, 2;
	@%p6 bra 	$L__BB12_4;

	shl.b64 	%rd28, %rd20, 3;
	add.s64 	%rd29, %rd4, %rd28;
	ld.global.u64 	%rd30, [%rd29];
	setp.ne.s64 	%p8, %rd48, %rd30;
	mov.pred 	%p19, 0;
	@%p8 bra 	$L__BB12_5;

$L__BB12_4:
	mul.lo.s64 	%rd48, %rd5, %rd48;
	add.s32 	%r34, %r34, 1;
	cvt.u64.u32 	%rd31, %r34;
	setp.lt.u64 	%p10, %rd31, %rd20;
	mov.pred 	%p19, %p2;
	@%p10 bra 	$L__BB12_2;

$L__BB12_5:
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r21, %ctaid.x;
	mov.u32 	%r22, %tid.x;
	mad.lo.s32 	%r35, %r21, %r3, %r22;
	cvt.u64.u32 	%rd51, %r35;
	@%p19 bra 	$L__BB12_16;
	bra.uni 	$L__BB12_6;

$L__BB12_16:
	setp.ge.u64 	%p17, %rd51, %rd19;
	@%p17 bra 	$L__BB12_19;

	mov.u32 	%r33, %nctaid.x;
	mul.lo.s32 	%r16, %r3, %r33;

$L__BB12_18:
	add.s64 	%rd47, %rd1, %rd51;
	st.global.u8 	[%rd47], %rs1;
	add.s32 	%r35, %r35, %r16;
	cvt.u64.u32 	%rd51, %r35;
	setp.lt.u64 	%p18, %rd51, %rd19;
	@%p18 bra 	$L__BB12_18;
	bra.uni 	$L__BB12_19;

$L__BB12_6:
	setp.ge.u64 	%p11, %rd51, %rd19;
	@%p11 bra 	$L__BB12_19;

	mov.u32 	%r23, %nctaid.x;
	mul.lo.s32 	%r5, %r3, %r23;
	@%p3 bra 	$L__BB12_14;

$L__BB12_8:
	mov.u32 	%r36, 0;
	mov.u32 	%r37, %r35;
	mov.u32 	%r38, %r36;

$L__BB12_9:
	not.b32 	%r26, %r36;
	cvt.u64.u32 	%rd32, %r26;
	add.s64 	%rd33, %rd32, %rd20;
	cvt.u64.u32 	%rd8, %r37;
	shl.b64 	%rd34, %rd33, 3;
	and.b64  	%rd35, %rd34, 34359738360;
	add.s64 	%rd9, %rd2, %rd35;
	ld.global.u64 	%rd10, [%rd9];
	and.b64  	%rd36, %rd10, -4294967296;
	setp.eq.s64 	%p13, %rd36, 0;
	@%p13 bra 	$L__BB12_11;

	div.u64 	%rd49, %rd8, %rd10;
	mul.lo.s64 	%rd37, %rd49, %rd10;
	sub.s64 	%rd50, %rd8, %rd37;
	bra.uni 	$L__BB12_12;

$L__BB12_11:
	cvt.u32.u64 	%r27, %rd10;
	cvt.u32.u64 	%r28, %rd8;
	div.u32 	%r29, %r28, %r27;
	mul.lo.s32 	%r30, %r29, %r27;
	sub.s32 	%r31, %r28, %r30;
	cvt.u64.u32 	%rd49, %r29;
	cvt.u64.u32 	%rd50, %r31;

$L__BB12_12:
	shl.b64 	%rd38, %rd20, 3;
	add.s64 	%rd39, %rd9, %rd38;
	ld.global.u64 	%rd40, [%rd39];
	mul.lo.s64 	%rd41, %rd40, %rd50;
	cvt.u32.u64 	%r32, %rd41;
	add.s32 	%r38, %r38, %r32;
	cvt.u32.u64 	%r37, %rd49;
	add.s32 	%r36, %r36, 1;
	cvt.u64.u32 	%rd42, %r36;
	setp.lt.u64 	%p14, %rd42, %rd20;
	@%p14 bra 	$L__BB12_9;

	cvt.u64.u32 	%rd43, %r38;
	add.s64 	%rd44, %rd1, %rd43;
	st.global.u8 	[%rd44], %rs1;
	add.s32 	%r35, %r35, %r5;
	cvt.u64.u32 	%rd45, %r35;
	setp.lt.u64 	%p15, %rd45, %rd19;
	@%p15 bra 	$L__BB12_8;
	bra.uni 	$L__BB12_19;

$L__BB12_14:
	add.s32 	%r35, %r35, %r5;
	cvt.u64.u32 	%rd46, %r35;
	setp.lt.u64 	%p16, %rd46, %rd19;
	@%p16 bra 	$L__BB12_14;

	st.global.u8 	[%rd1], %rs1;

$L__BB12_19:
	ret;

}
	// .globl	const_set_u32
.visible .entry const_set_u32(
	.param .u64 const_set_u32_param_0,
	.param .u64 const_set_u32_param_1,
	.param .u64 const_set_u32_param_2,
	.param .u32 const_set_u32_param_3,
	.param .u64 const_set_u32_param_4
)
{
	.reg .pred 	%p<20>;
	.reg .b32 	%r<42>;
	.reg .b64 	%rd<53>;


	ld.param.u64 	%rd19, [const_set_u32_param_0];
	ld.param.u64 	%rd20, [const_set_u32_param_1];
	ld.param.u64 	%rd21, [const_set_u32_param_2];
	ld.param.u32 	%r19, [const_set_u32_param_3];
	ld.param.u64 	%rd22, [const_set_u32_param_4];
	cvta.to.global.u64 	%rd1, %rd22;
	cvta.to.global.u64 	%rd2, %rd21;
	setp.eq.s64 	%p3, %rd20, 0;
	setp.eq.s64 	%p4, %rd21, 0;
	or.pred  	%p5, %p4, %p3;
	mov.pred 	%p2, -1;
	mov.pred 	%p19, %p2;
	@%p5 bra 	$L__BB13_5;

	mov.u64 	%rd49, 1;
	mov.u32 	%r35, 0;

$L__BB13_2:
	not.b32 	%r21, %r35;
	cvt.u64.u32 	%rd24, %r21;
	add.s64 	%rd25, %rd24, %rd20;
	shl.b64 	%rd26, %rd25, 3;
	and.b64  	%rd27, %rd26, 34359738360;
	add.s64 	%rd4, %rd2, %rd27;
	ld.global.u64 	%rd5, [%rd4];
	setp.lt.u64 	%p6, %rd5, 2;
	@%p6 bra 	$L__BB13_4;

	shl.b64 	%rd28, %rd20, 3;
	add.s64 	%rd29, %rd4, %rd28;
	ld.global.u64 	%rd30, [%rd29];
	setp.ne.s64 	%p8, %rd49, %rd30;
	mov.pred 	%p19, 0;
	@%p8 bra 	$L__BB13_5;

$L__BB13_4:
	mul.lo.s64 	%rd49, %rd5, %rd49;
	add.s32 	%r35, %r35, 1;
	cvt.u64.u32 	%rd31, %r35;
	setp.lt.u64 	%p10, %rd31, %rd20;
	mov.pred 	%p19, %p2;
	@%p10 bra 	$L__BB13_2;

$L__BB13_5:
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r22, %ctaid.x;
	mov.u32 	%r23, %tid.x;
	mad.lo.s32 	%r36, %r22, %r3, %r23;
	cvt.u64.u32 	%rd52, %r36;
	@%p19 bra 	$L__BB13_16;
	bra.uni 	$L__BB13_6;

$L__BB13_16:
	setp.ge.u64 	%p17, %rd52, %rd19;
	@%p17 bra 	$L__BB13_19;

	mov.u32 	%r34, %nctaid.x;
	mul.lo.s32 	%r16, %r3, %r34;

$L__BB13_18:
	shl.b64 	%rd47, %rd52, 2;
	add.s64 	%rd48, %rd1, %rd47;
	st.global.u32 	[%rd48], %r19;
	add.s32 	%r36, %r36, %r16;
	cvt.u64.u32 	%rd52, %r36;
	setp.lt.u64 	%p18, %rd52, %rd19;
	@%p18 bra 	$L__BB13_18;
	bra.uni 	$L__BB13_19;

$L__BB13_6:
	setp.ge.u64 	%p11, %rd52, %rd19;
	@%p11 bra 	$L__BB13_19;

	mov.u32 	%r24, %nctaid.x;
	mul.lo.s32 	%r5, %r3, %r24;
	@%p3 bra 	$L__BB13_14;

$L__BB13_8:
	mov.u32 	%r37, 0;
	mov.u32 	%r38, %r36;
	mov.u32 	%r39, %r37;

$L__BB13_9:
	not.b32 	%r27, %r37;
	cvt.u64.u32 	%rd32, %r27;
	add.s64 	%rd33, %rd32, %rd20;
	cvt.u64.u32 	%rd8, %r38;
	shl.b64 	%rd34, %rd33, 3;
	and.b64  	%rd35, %rd34, 34359738360;
	add.s64 	%rd9, %rd2, %rd35;
	ld.global.u64 	%rd10, [%rd9];
	and.b64  	%rd36, %rd10, -4294967296;
	setp.eq.s64 	%p13, %rd36, 0;
	@%p13 bra 	$L__BB13_11;

	div.u64 	%rd50, %rd8, %rd10;
	mul.lo.s64 	%rd37, %rd50, %rd10;
	sub.s64 	%rd51, %rd8, %rd37;
	bra.uni 	$L__BB13_12;

$L__BB13_11:
	cvt.u32.u64 	%r28, %rd10;
	cvt.u32.u64 	%r29, %rd8;
	div.u32 	%r30, %r29, %r28;
	mul.lo.s32 	%r31, %r30, %r28;
	sub.s32 	%r32, %r29, %r31;
	cvt.u64.u32 	%rd50, %r30;
	cvt.u64.u32 	%rd51, %r32;

$L__BB13_12:
	shl.b64 	%rd38, %rd20, 3;
	add.s64 	%rd39, %rd9, %rd38;
	ld.global.u64 	%rd40, [%rd39];
	mul.lo.s64 	%rd41, %rd40, %rd51;
	cvt.u32.u64 	%r33, %rd41;
	add.s32 	%r39, %r39, %r33;
	cvt.u32.u64 	%r38, %rd50;
	add.s32 	%r37, %r37, 1;
	cvt.u64.u32 	%rd42, %r37;
	setp.lt.u64 	%p14, %rd42, %rd20;
	@%p14 bra 	$L__BB13_9;

	mul.wide.u32 	%rd43, %r39, 4;
	add.s64 	%rd44, %rd1, %rd43;
	st.global.u32 	[%rd44], %r19;
	add.s32 	%r36, %r36, %r5;
	cvt.u64.u32 	%rd45, %r36;
	setp.lt.u64 	%p15, %rd45, %rd19;
	@%p15 bra 	$L__BB13_8;
	bra.uni 	$L__BB13_19;

$L__BB13_14:
	add.s32 	%r36, %r36, %r5;
	cvt.u64.u32 	%rd46, %r36;
	setp.lt.u64 	%p16, %rd46, %rd19;
	@%p16 bra 	$L__BB13_14;

	st.global.u32 	[%rd1], %r19;

$L__BB13_19:
	ret;

}
	// .globl	const_set_i64
.visible .entry const_set_i64(
	.param .u64 const_set_i64_param_0,
	.param .u64 const_set_i64_param_1,
	.param .u64 const_set_i64_param_2,
	.param .u64 const_set_i64_param_3,
	.param .u64 const_set_i64_param_4
)
{
	.reg .pred 	%p<20>;
	.reg .b32 	%r<41>;
	.reg .b64 	%rd<54>;


	ld.param.u64 	%rd19, [const_set_i64_param_0];
	ld.param.u64 	%rd20, [const_set_i64_param_1];
	ld.param.u64 	%rd22, [const_set_i64_param_2];
	ld.param.u64 	%rd21, [const_set_i64_param_3];
	ld.param.u64 	%rd23, [const_set_i64_param_4];
	cvta.to.global.u64 	%rd1, %rd23;
	cvta.to.global.u64 	%rd2, %rd22;
	setp.eq.s64 	%p3, %rd20, 0;
	setp.eq.s64 	%p4, %rd22, 0;
	or.pred  	%p5, %p4, %p3;
	mov.pred 	%p2, -1;
	mov.pred 	%p19, %p2;
	@%p5 bra 	$L__BB14_5;

	mov.u64 	%rd50, 1;
	mov.u32 	%r34, 0;

$L__BB14_2:
	not.b32 	%r20, %r34;
	cvt.u64.u32 	%rd25, %r20;
	add.s64 	%rd26, %rd25, %rd20;
	shl.b64 	%rd27, %rd26, 3;
	and.b64  	%rd28, %rd27, 34359738360;
	add.s64 	%rd4, %rd2, %rd28;
	ld.global.u64 	%rd5, [%rd4];
	setp.lt.u64 	%p6, %rd5, 2;
	@%p6 bra 	$L__BB14_4;

	shl.b64 	%rd29, %rd20, 3;
	add.s64 	%rd30, %rd4, %rd29;
	ld.global.u64 	%rd31, [%rd30];
	setp.ne.s64 	%p8, %rd50, %rd31;
	mov.pred 	%p19, 0;
	@%p8 bra 	$L__BB14_5;

$L__BB14_4:
	mul.lo.s64 	%rd50, %rd5, %rd50;
	add.s32 	%r34, %r34, 1;
	cvt.u64.u32 	%rd32, %r34;
	setp.lt.u64 	%p10, %rd32, %rd20;
	mov.pred 	%p19, %p2;
	@%p10 bra 	$L__BB14_2;

$L__BB14_5:
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r21, %ctaid.x;
	mov.u32 	%r22, %tid.x;
	mad.lo.s32 	%r35, %r21, %r3, %r22;
	cvt.u64.u32 	%rd53, %r35;
	@%p19 bra 	$L__BB14_16;
	bra.uni 	$L__BB14_6;

$L__BB14_16:
	setp.ge.u64 	%p17, %rd53, %rd19;
	@%p17 bra 	$L__BB14_19;

	mov.u32 	%r33, %nctaid.x;
	mul.lo.s32 	%r16, %r3, %r33;

$L__BB14_18:
	shl.b64 	%rd48, %rd53, 3;
	add.s64 	%rd49, %rd1, %rd48;
	st.global.u64 	[%rd49], %rd21;
	add.s32 	%r35, %r35, %r16;
	cvt.u64.u32 	%rd53, %r35;
	setp.lt.u64 	%p18, %rd53, %rd19;
	@%p18 bra 	$L__BB14_18;
	bra.uni 	$L__BB14_19;

$L__BB14_6:
	setp.ge.u64 	%p11, %rd53, %rd19;
	@%p11 bra 	$L__BB14_19;

	mov.u32 	%r23, %nctaid.x;
	mul.lo.s32 	%r5, %r3, %r23;
	@%p3 bra 	$L__BB14_14;

$L__BB14_8:
	mov.u32 	%r36, 0;
	mov.u32 	%r37, %r35;
	mov.u32 	%r38, %r36;

$L__BB14_9:
	not.b32 	%r26, %r36;
	cvt.u64.u32 	%rd33, %r26;
	add.s64 	%rd34, %rd33, %rd20;
	cvt.u64.u32 	%rd8, %r37;
	shl.b64 	%rd35, %rd34, 3;
	and.b64  	%rd36, %rd35, 34359738360;
	add.s64 	%rd9, %rd2, %rd36;
	ld.global.u64 	%rd10, [%rd9];
	and.b64  	%rd37, %rd10, -4294967296;
	setp.eq.s64 	%p13, %rd37, 0;
	@%p13 bra 	$L__BB14_11;

	div.u64 	%rd51, %rd8, %rd10;
	mul.lo.s64 	%rd38, %rd51, %rd10;
	sub.s64 	%rd52, %rd8, %rd38;
	bra.uni 	$L__BB14_12;

$L__BB14_11:
	cvt.u32.u64 	%r27, %rd10;
	cvt.u32.u64 	%r28, %rd8;
	div.u32 	%r29, %r28, %r27;
	mul.lo.s32 	%r30, %r29, %r27;
	sub.s32 	%r31, %r28, %r30;
	cvt.u64.u32 	%rd51, %r29;
	cvt.u64.u32 	%rd52, %r31;

$L__BB14_12:
	shl.b64 	%rd39, %rd20, 3;
	add.s64 	%rd40, %rd9, %rd39;
	ld.global.u64 	%rd41, [%rd40];
	mul.lo.s64 	%rd42, %rd41, %rd52;
	cvt.u32.u64 	%r32, %rd42;
	add.s32 	%r38, %r38, %r32;
	cvt.u32.u64 	%r37, %rd51;
	add.s32 	%r36, %r36, 1;
	cvt.u64.u32 	%rd43, %r36;
	setp.lt.u64 	%p14, %rd43, %rd20;
	@%p14 bra 	$L__BB14_9;

	mul.wide.u32 	%rd44, %r38, 8;
	add.s64 	%rd45, %rd1, %rd44;
	st.global.u64 	[%rd45], %rd21;
	add.s32 	%r35, %r35, %r5;
	cvt.u64.u32 	%rd46, %r35;
	setp.lt.u64 	%p15, %rd46, %rd19;
	@%p15 bra 	$L__BB14_8;
	bra.uni 	$L__BB14_19;

$L__BB14_14:
	add.s32 	%r35, %r35, %r5;
	cvt.u64.u32 	%rd47, %r35;
	setp.lt.u64 	%p16, %rd47, %rd19;
	@%p16 bra 	$L__BB14_14;

	st.global.u64 	[%rd1], %rd21;

$L__BB14_19:
	ret;

}

