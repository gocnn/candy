//
// Generated by NVIDIA NVVM Compiler
//
// Compiler Build ID: CL-35813241
// Cuda compilation tools, release 12.9, V12.9.41
// Based on NVVM 7.0.1
//

.version 8.8
.target sm_52
.address_size 64

	// .globl	cast_bf16_f32

.visible .entry cast_bf16_f32(
	.param .u64 cast_bf16_f32_param_0,
	.param .u64 cast_bf16_f32_param_1,
	.param .u64 cast_bf16_f32_param_2,
	.param .u64 cast_bf16_f32_param_3,
	.param .u64 cast_bf16_f32_param_4
)
{
	.reg .pred 	%p<20>;
	.reg .b16 	%rs<4>;
	.reg .f32 	%f<4>;
	.reg .b32 	%r<41>;
	.reg .b64 	%rd<65>;


	ld.param.u64 	%rd24, [cast_bf16_f32_param_0];
	ld.param.u64 	%rd25, [cast_bf16_f32_param_1];
	ld.param.u64 	%rd26, [cast_bf16_f32_param_2];
	ld.param.u64 	%rd27, [cast_bf16_f32_param_3];
	ld.param.u64 	%rd28, [cast_bf16_f32_param_4];
	cvta.to.global.u64 	%rd1, %rd28;
	cvta.to.global.u64 	%rd2, %rd27;
	cvta.to.global.u64 	%rd3, %rd26;
	setp.eq.s64 	%p3, %rd25, 0;
	setp.eq.s64 	%p4, %rd26, 0;
	or.pred  	%p5, %p4, %p3;
	mov.pred 	%p2, -1;
	mov.pred 	%p19, %p2;
	@%p5 bra 	$L__BB0_5;

	mov.u64 	%rd59, 1;
	mov.u32 	%r34, 0;

$L__BB0_2:
	not.b32 	%r20, %r34;
	cvt.u64.u32 	%rd30, %r20;
	add.s64 	%rd31, %rd30, %rd25;
	shl.b64 	%rd32, %rd31, 3;
	and.b64  	%rd33, %rd32, 34359738360;
	add.s64 	%rd5, %rd3, %rd33;
	ld.global.u64 	%rd6, [%rd5];
	setp.lt.u64 	%p6, %rd6, 2;
	@%p6 bra 	$L__BB0_4;

	shl.b64 	%rd34, %rd25, 3;
	add.s64 	%rd35, %rd5, %rd34;
	ld.global.u64 	%rd36, [%rd35];
	setp.ne.s64 	%p8, %rd59, %rd36;
	mov.pred 	%p19, 0;
	@%p8 bra 	$L__BB0_5;

$L__BB0_4:
	mul.lo.s64 	%rd59, %rd6, %rd59;
	add.s32 	%r34, %r34, 1;
	cvt.u64.u32 	%rd37, %r34;
	setp.lt.u64 	%p10, %rd37, %rd25;
	mov.pred 	%p19, %p2;
	@%p10 bra 	$L__BB0_2;

$L__BB0_5:
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r21, %ctaid.x;
	mov.u32 	%r22, %tid.x;
	mad.lo.s32 	%r35, %r21, %r3, %r22;
	cvt.u64.u32 	%rd60, %r35;
	@%p19 bra 	$L__BB0_15;
	bra.uni 	$L__BB0_6;

$L__BB0_15:
	setp.ge.u64 	%p17, %rd60, %rd24;
	@%p17 bra 	$L__BB0_18;

	mov.u32 	%r33, %nctaid.x;
	mul.lo.s32 	%r16, %r3, %r33;

$L__BB0_17:
	shl.b64 	%rd55, %rd60, 1;
	add.s64 	%rd56, %rd2, %rd55;
	ld.global.u16 	%rs3, [%rd56];
	// begin inline asm
	{ mov.b32 %f3, {0,%rs3};}

	// end inline asm
	shl.b64 	%rd57, %rd60, 2;
	add.s64 	%rd58, %rd1, %rd57;
	st.global.f32 	[%rd58], %f3;
	add.s32 	%r35, %r35, %r16;
	cvt.u64.u32 	%rd60, %r35;
	setp.lt.u64 	%p18, %rd60, %rd24;
	@%p18 bra 	$L__BB0_17;
	bra.uni 	$L__BB0_18;

$L__BB0_6:
	setp.ge.u64 	%p11, %rd60, %rd24;
	@%p11 bra 	$L__BB0_18;

	mov.u32 	%r23, %nctaid.x;
	mul.lo.s32 	%r5, %r3, %r23;
	@%p3 bra 	$L__BB0_14;

$L__BB0_8:
	mov.u32 	%r36, 0;
	mov.u32 	%r37, %r35;
	mov.u32 	%r38, %r36;

$L__BB0_9:
	not.b32 	%r26, %r36;
	cvt.u64.u32 	%rd38, %r26;
	add.s64 	%rd39, %rd38, %rd25;
	cvt.u64.u32 	%rd10, %r37;
	shl.b64 	%rd40, %rd39, 3;
	and.b64  	%rd41, %rd40, 34359738360;
	add.s64 	%rd11, %rd3, %rd41;
	ld.global.u64 	%rd12, [%rd11];
	and.b64  	%rd42, %rd12, -4294967296;
	setp.eq.s64 	%p13, %rd42, 0;
	@%p13 bra 	$L__BB0_11;

	div.u64 	%rd61, %rd10, %rd12;
	mul.lo.s64 	%rd43, %rd61, %rd12;
	sub.s64 	%rd62, %rd10, %rd43;
	bra.uni 	$L__BB0_12;

$L__BB0_11:
	cvt.u32.u64 	%r27, %rd12;
	cvt.u32.u64 	%r28, %rd10;
	div.u32 	%r29, %r28, %r27;
	mul.lo.s32 	%r30, %r29, %r27;
	sub.s32 	%r31, %r28, %r30;
	cvt.u64.u32 	%rd61, %r29;
	cvt.u64.u32 	%rd62, %r31;

$L__BB0_12:
	shl.b64 	%rd44, %rd25, 3;
	add.s64 	%rd45, %rd11, %rd44;
	ld.global.u64 	%rd46, [%rd45];
	mul.lo.s64 	%rd47, %rd46, %rd62;
	cvt.u32.u64 	%r32, %rd47;
	add.s32 	%r38, %r38, %r32;
	cvt.u32.u64 	%r37, %rd61;
	add.s32 	%r36, %r36, 1;
	cvt.u64.u32 	%rd48, %r36;
	setp.lt.u64 	%p14, %rd48, %rd25;
	@%p14 bra 	$L__BB0_9;

	mul.wide.u32 	%rd49, %r38, 2;
	add.s64 	%rd50, %rd2, %rd49;
	ld.global.u16 	%rs1, [%rd50];
	// begin inline asm
	{ mov.b32 %f1, {0,%rs1};}

	// end inline asm
	shl.b64 	%rd51, %rd60, 2;
	add.s64 	%rd52, %rd1, %rd51;
	st.global.f32 	[%rd52], %f1;
	add.s32 	%r35, %r35, %r5;
	cvt.u64.u32 	%rd60, %r35;
	setp.lt.u64 	%p15, %rd60, %rd24;
	@%p15 bra 	$L__BB0_8;
	bra.uni 	$L__BB0_18;

$L__BB0_14:
	ld.global.u16 	%rs2, [%rd2];
	// begin inline asm
	{ mov.b32 %f2, {0,%rs2};}

	// end inline asm
	shl.b64 	%rd53, %rd60, 2;
	add.s64 	%rd54, %rd1, %rd53;
	st.global.f32 	[%rd54], %f2;
	add.s32 	%r35, %r35, %r5;
	cvt.u64.u32 	%rd60, %r35;
	setp.lt.u64 	%p16, %rd60, %rd24;
	@%p16 bra 	$L__BB0_14;

$L__BB0_18:
	ret;

}
	// .globl	cast_f32_bf16
.visible .entry cast_f32_bf16(
	.param .u64 cast_f32_bf16_param_0,
	.param .u64 cast_f32_bf16_param_1,
	.param .u64 cast_f32_bf16_param_2,
	.param .u64 cast_f32_bf16_param_3,
	.param .u64 cast_f32_bf16_param_4
)
{
	.reg .pred 	%p<38>;
	.reg .b16 	%rs<19>;
	.reg .b32 	%r<56>;
	.reg .b64 	%rd<65>;


	ld.param.u64 	%rd24, [cast_f32_bf16_param_0];
	ld.param.u64 	%rd25, [cast_f32_bf16_param_1];
	ld.param.u64 	%rd26, [cast_f32_bf16_param_2];
	ld.param.u64 	%rd27, [cast_f32_bf16_param_3];
	ld.param.u64 	%rd28, [cast_f32_bf16_param_4];
	cvta.to.global.u64 	%rd1, %rd28;
	cvta.to.global.u64 	%rd2, %rd27;
	cvta.to.global.u64 	%rd3, %rd26;
	setp.eq.s64 	%p3, %rd25, 0;
	setp.eq.s64 	%p4, %rd26, 0;
	or.pred  	%p5, %p4, %p3;
	mov.pred 	%p2, -1;
	mov.pred 	%p37, %p2;
	@%p5 bra 	$L__BB1_5;

	mov.u64 	%rd59, 1;
	mov.u32 	%r49, 0;

$L__BB1_2:
	not.b32 	%r23, %r49;
	cvt.u64.u32 	%rd30, %r23;
	add.s64 	%rd31, %rd30, %rd25;
	shl.b64 	%rd32, %rd31, 3;
	and.b64  	%rd33, %rd32, 34359738360;
	add.s64 	%rd5, %rd3, %rd33;
	ld.global.u64 	%rd6, [%rd5];
	setp.lt.u64 	%p6, %rd6, 2;
	@%p6 bra 	$L__BB1_4;

	shl.b64 	%rd34, %rd25, 3;
	add.s64 	%rd35, %rd5, %rd34;
	ld.global.u64 	%rd36, [%rd35];
	setp.ne.s64 	%p8, %rd59, %rd36;
	mov.pred 	%p37, 0;
	@%p8 bra 	$L__BB1_5;

$L__BB1_4:
	mul.lo.s64 	%rd59, %rd6, %rd59;
	add.s32 	%r49, %r49, 1;
	cvt.u64.u32 	%rd37, %r49;
	setp.lt.u64 	%p10, %rd37, %rd25;
	mov.pred 	%p37, %p2;
	@%p10 bra 	$L__BB1_2;

$L__BB1_5:
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r24, %ctaid.x;
	mov.u32 	%r25, %tid.x;
	mad.lo.s32 	%r50, %r24, %r3, %r25;
	cvt.u64.u32 	%rd60, %r50;
	@%p37 bra 	$L__BB1_21;
	bra.uni 	$L__BB1_6;

$L__BB1_21:
	setp.ge.u64 	%p29, %rd60, %rd24;
	@%p29 bra 	$L__BB1_27;

	mov.u32 	%r44, %nctaid.x;
	mul.lo.s32 	%r18, %r3, %r44;

$L__BB1_23:
	shl.b64 	%rd55, %rd60, 2;
	add.s64 	%rd56, %rd2, %rd55;
	ld.global.u32 	%r45, [%rd56];
	and.b32  	%r46, %r45, 2147483647;
	setp.gt.u32 	%p30, %r46, 2139095040;
	shl.b32 	%r47, %r45, 16;
	shr.u32 	%r48, %r45, 16;
	cvt.u16.u32 	%rs14, %r48;
	selp.b32 	%r20, 0, %r47, %p30;
	selp.b16 	%rs18, 32767, %rs14, %p30;
	setp.gt.u32 	%p31, %r20, -2147483648;
	@%p31 bra 	$L__BB1_25;

	setp.ne.s32 	%p32, %r20, -2147483648;
	and.b16  	%rs15, %rs18, 1;
	setp.eq.b16 	%p33, %rs15, 1;
	not.pred 	%p34, %p33;
	or.pred  	%p35, %p32, %p34;
	@%p35 bra 	$L__BB1_26;

$L__BB1_25:
	add.s16 	%rs18, %rs18, 1;

$L__BB1_26:
	shl.b64 	%rd57, %rd60, 1;
	add.s64 	%rd58, %rd1, %rd57;
	st.global.u16 	[%rd58], %rs18;
	add.s32 	%r50, %r50, %r18;
	cvt.u64.u32 	%rd60, %r50;
	setp.lt.u64 	%p36, %rd60, %rd24;
	@%p36 bra 	$L__BB1_23;
	bra.uni 	$L__BB1_27;

$L__BB1_6:
	setp.ge.u64 	%p11, %rd60, %rd24;
	@%p11 bra 	$L__BB1_27;

	mov.u32 	%r26, %nctaid.x;
	mul.lo.s32 	%r5, %r3, %r26;
	@%p3 bra 	$L__BB1_17;

$L__BB1_8:
	mov.u32 	%r51, 0;
	mov.u32 	%r52, %r50;
	mov.u32 	%r53, %r51;

$L__BB1_9:
	not.b32 	%r29, %r51;
	cvt.u64.u32 	%rd38, %r29;
	add.s64 	%rd39, %rd38, %rd25;
	cvt.u64.u32 	%rd10, %r52;
	shl.b64 	%rd40, %rd39, 3;
	and.b64  	%rd41, %rd40, 34359738360;
	add.s64 	%rd11, %rd3, %rd41;
	ld.global.u64 	%rd12, [%rd11];
	and.b64  	%rd42, %rd12, -4294967296;
	setp.eq.s64 	%p13, %rd42, 0;
	@%p13 bra 	$L__BB1_11;

	div.u64 	%rd61, %rd10, %rd12;
	mul.lo.s64 	%rd43, %rd61, %rd12;
	sub.s64 	%rd62, %rd10, %rd43;
	bra.uni 	$L__BB1_12;

$L__BB1_11:
	cvt.u32.u64 	%r30, %rd12;
	cvt.u32.u64 	%r31, %rd10;
	div.u32 	%r32, %r31, %r30;
	mul.lo.s32 	%r33, %r32, %r30;
	sub.s32 	%r34, %r31, %r33;
	cvt.u64.u32 	%rd61, %r32;
	cvt.u64.u32 	%rd62, %r34;

$L__BB1_12:
	shl.b64 	%rd44, %rd25, 3;
	add.s64 	%rd45, %rd11, %rd44;
	ld.global.u64 	%rd46, [%rd45];
	mul.lo.s64 	%rd47, %rd46, %rd62;
	cvt.u32.u64 	%r35, %rd47;
	add.s32 	%r53, %r53, %r35;
	cvt.u32.u64 	%r52, %rd61;
	add.s32 	%r51, %r51, 1;
	cvt.u64.u32 	%rd48, %r51;
	setp.lt.u64 	%p14, %rd48, %rd25;
	@%p14 bra 	$L__BB1_9;

	mul.wide.u32 	%rd49, %r53, 4;
	add.s64 	%rd50, %rd2, %rd49;
	ld.global.u32 	%r36, [%rd50];
	and.b32  	%r37, %r36, 2147483647;
	setp.gt.u32 	%p15, %r37, 2139095040;
	shl.b32 	%r38, %r36, 16;
	shr.u32 	%r39, %r36, 16;
	cvt.u16.u32 	%rs10, %r39;
	selp.b32 	%r13, 0, %r38, %p15;
	selp.b16 	%rs16, 32767, %rs10, %p15;
	setp.gt.u32 	%p16, %r13, -2147483648;
	@%p16 bra 	$L__BB1_15;

	setp.ne.s32 	%p17, %r13, -2147483648;
	and.b16  	%rs11, %rs16, 1;
	setp.eq.b16 	%p18, %rs11, 1;
	not.pred 	%p19, %p18;
	or.pred  	%p20, %p17, %p19;
	@%p20 bra 	$L__BB1_16;

$L__BB1_15:
	add.s16 	%rs16, %rs16, 1;

$L__BB1_16:
	shl.b64 	%rd51, %rd60, 1;
	add.s64 	%rd52, %rd1, %rd51;
	st.global.u16 	[%rd52], %rs16;
	add.s32 	%r50, %r50, %r5;
	cvt.u64.u32 	%rd60, %r50;
	setp.lt.u64 	%p21, %rd60, %rd24;
	@%p21 bra 	$L__BB1_8;
	bra.uni 	$L__BB1_27;

$L__BB1_17:
	ld.global.u32 	%r40, [%rd2];
	and.b32  	%r41, %r40, 2147483647;
	setp.gt.u32 	%p22, %r41, 2139095040;
	shl.b32 	%r42, %r40, 16;
	shr.u32 	%r43, %r40, 16;
	cvt.u16.u32 	%rs12, %r43;
	selp.b32 	%r16, 0, %r42, %p22;
	selp.b16 	%rs17, 32767, %rs12, %p22;
	setp.gt.u32 	%p23, %r16, -2147483648;
	@%p23 bra 	$L__BB1_19;

	setp.ne.s32 	%p24, %r16, -2147483648;
	and.b16  	%rs13, %rs17, 1;
	setp.eq.b16 	%p25, %rs13, 1;
	not.pred 	%p26, %p25;
	or.pred  	%p27, %p24, %p26;
	@%p27 bra 	$L__BB1_20;

$L__BB1_19:
	add.s16 	%rs17, %rs17, 1;

$L__BB1_20:
	shl.b64 	%rd53, %rd60, 1;
	add.s64 	%rd54, %rd1, %rd53;
	st.global.u16 	[%rd54], %rs17;
	add.s32 	%r50, %r50, %r5;
	cvt.u64.u32 	%rd60, %r50;
	setp.lt.u64 	%p28, %rd60, %rd24;
	@%p28 bra 	$L__BB1_17;

$L__BB1_27:
	ret;

}
	// .globl	cast_bf16_u8
.visible .entry cast_bf16_u8(
	.param .u64 cast_bf16_u8_param_0,
	.param .u64 cast_bf16_u8_param_1,
	.param .u64 cast_bf16_u8_param_2,
	.param .u64 cast_bf16_u8_param_3,
	.param .u64 cast_bf16_u8_param_4
)
{
	.reg .pred 	%p<20>;
	.reg .b16 	%rs<4>;
	.reg .f32 	%f<4>;
	.reg .b32 	%r<44>;
	.reg .b64 	%rd<62>;


	ld.param.u64 	%rd24, [cast_bf16_u8_param_0];
	ld.param.u64 	%rd25, [cast_bf16_u8_param_1];
	ld.param.u64 	%rd26, [cast_bf16_u8_param_2];
	ld.param.u64 	%rd27, [cast_bf16_u8_param_3];
	ld.param.u64 	%rd28, [cast_bf16_u8_param_4];
	cvta.to.global.u64 	%rd1, %rd28;
	cvta.to.global.u64 	%rd2, %rd27;
	cvta.to.global.u64 	%rd3, %rd26;
	setp.eq.s64 	%p3, %rd25, 0;
	setp.eq.s64 	%p4, %rd26, 0;
	or.pred  	%p5, %p4, %p3;
	mov.pred 	%p2, -1;
	mov.pred 	%p19, %p2;
	@%p5 bra 	$L__BB2_5;

	mov.u64 	%rd56, 1;
	mov.u32 	%r37, 0;

$L__BB2_2:
	not.b32 	%r20, %r37;
	cvt.u64.u32 	%rd30, %r20;
	add.s64 	%rd31, %rd30, %rd25;
	shl.b64 	%rd32, %rd31, 3;
	and.b64  	%rd33, %rd32, 34359738360;
	add.s64 	%rd5, %rd3, %rd33;
	ld.global.u64 	%rd6, [%rd5];
	setp.lt.u64 	%p6, %rd6, 2;
	@%p6 bra 	$L__BB2_4;

	shl.b64 	%rd34, %rd25, 3;
	add.s64 	%rd35, %rd5, %rd34;
	ld.global.u64 	%rd36, [%rd35];
	setp.ne.s64 	%p8, %rd56, %rd36;
	mov.pred 	%p19, 0;
	@%p8 bra 	$L__BB2_5;

$L__BB2_4:
	mul.lo.s64 	%rd56, %rd6, %rd56;
	add.s32 	%r37, %r37, 1;
	cvt.u64.u32 	%rd37, %r37;
	setp.lt.u64 	%p10, %rd37, %rd25;
	mov.pred 	%p19, %p2;
	@%p10 bra 	$L__BB2_2;

$L__BB2_5:
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r21, %ctaid.x;
	mov.u32 	%r22, %tid.x;
	mad.lo.s32 	%r38, %r21, %r3, %r22;
	cvt.u64.u32 	%rd57, %r38;
	@%p19 bra 	$L__BB2_15;
	bra.uni 	$L__BB2_6;

$L__BB2_15:
	setp.ge.u64 	%p17, %rd57, %rd24;
	@%p17 bra 	$L__BB2_18;

	mov.u32 	%r35, %nctaid.x;
	mul.lo.s32 	%r16, %r3, %r35;

$L__BB2_17:
	shl.b64 	%rd53, %rd57, 1;
	add.s64 	%rd54, %rd2, %rd53;
	ld.global.u16 	%rs3, [%rd54];
	// begin inline asm
	{ mov.b32 %f3, {0,%rs3};}

	// end inline asm
	cvt.rzi.u32.f32 	%r36, %f3;
	add.s64 	%rd55, %rd1, %rd57;
	st.global.u8 	[%rd55], %r36;
	add.s32 	%r38, %r38, %r16;
	cvt.u64.u32 	%rd57, %r38;
	setp.lt.u64 	%p18, %rd57, %rd24;
	@%p18 bra 	$L__BB2_17;
	bra.uni 	$L__BB2_18;

$L__BB2_6:
	setp.ge.u64 	%p11, %rd57, %rd24;
	@%p11 bra 	$L__BB2_18;

	mov.u32 	%r23, %nctaid.x;
	mul.lo.s32 	%r5, %r3, %r23;
	@%p3 bra 	$L__BB2_14;

$L__BB2_8:
	mov.u32 	%r39, 0;
	mov.u32 	%r40, %r38;
	mov.u32 	%r41, %r39;

$L__BB2_9:
	not.b32 	%r26, %r39;
	cvt.u64.u32 	%rd38, %r26;
	add.s64 	%rd39, %rd38, %rd25;
	cvt.u64.u32 	%rd10, %r40;
	shl.b64 	%rd40, %rd39, 3;
	and.b64  	%rd41, %rd40, 34359738360;
	add.s64 	%rd11, %rd3, %rd41;
	ld.global.u64 	%rd12, [%rd11];
	and.b64  	%rd42, %rd12, -4294967296;
	setp.eq.s64 	%p13, %rd42, 0;
	@%p13 bra 	$L__BB2_11;

	div.u64 	%rd58, %rd10, %rd12;
	mul.lo.s64 	%rd43, %rd58, %rd12;
	sub.s64 	%rd59, %rd10, %rd43;
	bra.uni 	$L__BB2_12;

$L__BB2_11:
	cvt.u32.u64 	%r27, %rd12;
	cvt.u32.u64 	%r28, %rd10;
	div.u32 	%r29, %r28, %r27;
	mul.lo.s32 	%r30, %r29, %r27;
	sub.s32 	%r31, %r28, %r30;
	cvt.u64.u32 	%rd58, %r29;
	cvt.u64.u32 	%rd59, %r31;

$L__BB2_12:
	shl.b64 	%rd44, %rd25, 3;
	add.s64 	%rd45, %rd11, %rd44;
	ld.global.u64 	%rd46, [%rd45];
	mul.lo.s64 	%rd47, %rd46, %rd59;
	cvt.u32.u64 	%r32, %rd47;
	add.s32 	%r41, %r41, %r32;
	cvt.u32.u64 	%r40, %rd58;
	add.s32 	%r39, %r39, 1;
	cvt.u64.u32 	%rd48, %r39;
	setp.lt.u64 	%p14, %rd48, %rd25;
	@%p14 bra 	$L__BB2_9;

	mul.wide.u32 	%rd49, %r41, 2;
	add.s64 	%rd50, %rd2, %rd49;
	ld.global.u16 	%rs1, [%rd50];
	// begin inline asm
	{ mov.b32 %f1, {0,%rs1};}

	// end inline asm
	cvt.rzi.u32.f32 	%r33, %f1;
	add.s64 	%rd51, %rd1, %rd57;
	st.global.u8 	[%rd51], %r33;
	add.s32 	%r38, %r38, %r5;
	cvt.u64.u32 	%rd57, %r38;
	setp.lt.u64 	%p15, %rd57, %rd24;
	@%p15 bra 	$L__BB2_8;
	bra.uni 	$L__BB2_18;

$L__BB2_14:
	ld.global.u16 	%rs2, [%rd2];
	// begin inline asm
	{ mov.b32 %f2, {0,%rs2};}

	// end inline asm
	cvt.rzi.u32.f32 	%r34, %f2;
	add.s64 	%rd52, %rd1, %rd57;
	st.global.u8 	[%rd52], %r34;
	add.s32 	%r38, %r38, %r5;
	cvt.u64.u32 	%rd57, %r38;
	setp.lt.u64 	%p16, %rd57, %rd24;
	@%p16 bra 	$L__BB2_14;

$L__BB2_18:
	ret;

}
	// .globl	cast_bf16_f16
.visible .entry cast_bf16_f16(
	.param .u64 cast_bf16_f16_param_0,
	.param .u64 cast_bf16_f16_param_1,
	.param .u64 cast_bf16_f16_param_2,
	.param .u64 cast_bf16_f16_param_3,
	.param .u64 cast_bf16_f16_param_4
)
{
	.reg .pred 	%p<20>;
	.reg .b16 	%rs<7>;
	.reg .f32 	%f<7>;
	.reg .b32 	%r<41>;
	.reg .b64 	%rd<64>;


	ld.param.u64 	%rd24, [cast_bf16_f16_param_0];
	ld.param.u64 	%rd25, [cast_bf16_f16_param_1];
	ld.param.u64 	%rd26, [cast_bf16_f16_param_2];
	ld.param.u64 	%rd27, [cast_bf16_f16_param_3];
	ld.param.u64 	%rd28, [cast_bf16_f16_param_4];
	cvta.to.global.u64 	%rd1, %rd28;
	cvta.to.global.u64 	%rd2, %rd27;
	cvta.to.global.u64 	%rd3, %rd26;
	setp.eq.s64 	%p3, %rd25, 0;
	setp.eq.s64 	%p4, %rd26, 0;
	or.pred  	%p5, %p4, %p3;
	mov.pred 	%p2, -1;
	mov.pred 	%p19, %p2;
	@%p5 bra 	$L__BB3_5;

	mov.u64 	%rd58, 1;
	mov.u32 	%r34, 0;

$L__BB3_2:
	not.b32 	%r20, %r34;
	cvt.u64.u32 	%rd30, %r20;
	add.s64 	%rd31, %rd30, %rd25;
	shl.b64 	%rd32, %rd31, 3;
	and.b64  	%rd33, %rd32, 34359738360;
	add.s64 	%rd5, %rd3, %rd33;
	ld.global.u64 	%rd6, [%rd5];
	setp.lt.u64 	%p6, %rd6, 2;
	@%p6 bra 	$L__BB3_4;

	shl.b64 	%rd34, %rd25, 3;
	add.s64 	%rd35, %rd5, %rd34;
	ld.global.u64 	%rd36, [%rd35];
	setp.ne.s64 	%p8, %rd58, %rd36;
	mov.pred 	%p19, 0;
	@%p8 bra 	$L__BB3_5;

$L__BB3_4:
	mul.lo.s64 	%rd58, %rd6, %rd58;
	add.s32 	%r34, %r34, 1;
	cvt.u64.u32 	%rd37, %r34;
	setp.lt.u64 	%p10, %rd37, %rd25;
	mov.pred 	%p19, %p2;
	@%p10 bra 	$L__BB3_2;

$L__BB3_5:
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r21, %ctaid.x;
	mov.u32 	%r22, %tid.x;
	mad.lo.s32 	%r35, %r21, %r3, %r22;
	cvt.u64.u32 	%rd59, %r35;
	@%p19 bra 	$L__BB3_15;
	bra.uni 	$L__BB3_6;

$L__BB3_15:
	setp.ge.u64 	%p17, %rd59, %rd24;
	@%p17 bra 	$L__BB3_18;

	mov.u32 	%r33, %nctaid.x;
	mul.lo.s32 	%r16, %r3, %r33;

$L__BB3_17:
	shl.b64 	%rd55, %rd59, 1;
	add.s64 	%rd56, %rd2, %rd55;
	ld.global.u16 	%rs5, [%rd56];
	// begin inline asm
	{ mov.b32 %f5, {0,%rs5};}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs6, %f5;}

	// end inline asm
	add.s64 	%rd57, %rd1, %rd55;
	st.global.u16 	[%rd57], %rs6;
	add.s32 	%r35, %r35, %r16;
	cvt.u64.u32 	%rd59, %r35;
	setp.lt.u64 	%p18, %rd59, %rd24;
	@%p18 bra 	$L__BB3_17;
	bra.uni 	$L__BB3_18;

$L__BB3_6:
	setp.ge.u64 	%p11, %rd59, %rd24;
	@%p11 bra 	$L__BB3_18;

	mov.u32 	%r23, %nctaid.x;
	mul.lo.s32 	%r5, %r3, %r23;
	@%p3 bra 	$L__BB3_14;

$L__BB3_8:
	mov.u32 	%r36, 0;
	mov.u32 	%r37, %r35;
	mov.u32 	%r38, %r36;

$L__BB3_9:
	not.b32 	%r26, %r36;
	cvt.u64.u32 	%rd38, %r26;
	add.s64 	%rd39, %rd38, %rd25;
	cvt.u64.u32 	%rd10, %r37;
	shl.b64 	%rd40, %rd39, 3;
	and.b64  	%rd41, %rd40, 34359738360;
	add.s64 	%rd11, %rd3, %rd41;
	ld.global.u64 	%rd12, [%rd11];
	and.b64  	%rd42, %rd12, -4294967296;
	setp.eq.s64 	%p13, %rd42, 0;
	@%p13 bra 	$L__BB3_11;

	div.u64 	%rd60, %rd10, %rd12;
	mul.lo.s64 	%rd43, %rd60, %rd12;
	sub.s64 	%rd61, %rd10, %rd43;
	bra.uni 	$L__BB3_12;

$L__BB3_11:
	cvt.u32.u64 	%r27, %rd12;
	cvt.u32.u64 	%r28, %rd10;
	div.u32 	%r29, %r28, %r27;
	mul.lo.s32 	%r30, %r29, %r27;
	sub.s32 	%r31, %r28, %r30;
	cvt.u64.u32 	%rd60, %r29;
	cvt.u64.u32 	%rd61, %r31;

$L__BB3_12:
	shl.b64 	%rd44, %rd25, 3;
	add.s64 	%rd45, %rd11, %rd44;
	ld.global.u64 	%rd46, [%rd45];
	mul.lo.s64 	%rd47, %rd46, %rd61;
	cvt.u32.u64 	%r32, %rd47;
	add.s32 	%r38, %r38, %r32;
	cvt.u32.u64 	%r37, %rd60;
	add.s32 	%r36, %r36, 1;
	cvt.u64.u32 	%rd48, %r36;
	setp.lt.u64 	%p14, %rd48, %rd25;
	@%p14 bra 	$L__BB3_9;

	mul.wide.u32 	%rd49, %r38, 2;
	add.s64 	%rd50, %rd2, %rd49;
	ld.global.u16 	%rs1, [%rd50];
	// begin inline asm
	{ mov.b32 %f1, {0,%rs1};}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs2, %f1;}

	// end inline asm
	shl.b64 	%rd51, %rd59, 1;
	add.s64 	%rd52, %rd1, %rd51;
	st.global.u16 	[%rd52], %rs2;
	add.s32 	%r35, %r35, %r5;
	cvt.u64.u32 	%rd59, %r35;
	setp.lt.u64 	%p15, %rd59, %rd24;
	@%p15 bra 	$L__BB3_8;
	bra.uni 	$L__BB3_18;

$L__BB3_14:
	ld.global.u16 	%rs3, [%rd2];
	// begin inline asm
	{ mov.b32 %f3, {0,%rs3};}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs4, %f3;}

	// end inline asm
	shl.b64 	%rd53, %rd59, 1;
	add.s64 	%rd54, %rd1, %rd53;
	st.global.u16 	[%rd54], %rs4;
	add.s32 	%r35, %r35, %r5;
	cvt.u64.u32 	%rd59, %r35;
	setp.lt.u64 	%p16, %rd59, %rd24;
	@%p16 bra 	$L__BB3_14;

$L__BB3_18:
	ret;

}
	// .globl	cast_bf16_f64
.visible .entry cast_bf16_f64(
	.param .u64 cast_bf16_f64_param_0,
	.param .u64 cast_bf16_f64_param_1,
	.param .u64 cast_bf16_f64_param_2,
	.param .u64 cast_bf16_f64_param_3,
	.param .u64 cast_bf16_f64_param_4
)
{
	.reg .pred 	%p<20>;
	.reg .b16 	%rs<4>;
	.reg .f32 	%f<4>;
	.reg .b32 	%r<41>;
	.reg .f64 	%fd<4>;
	.reg .b64 	%rd<65>;


	ld.param.u64 	%rd24, [cast_bf16_f64_param_0];
	ld.param.u64 	%rd25, [cast_bf16_f64_param_1];
	ld.param.u64 	%rd26, [cast_bf16_f64_param_2];
	ld.param.u64 	%rd27, [cast_bf16_f64_param_3];
	ld.param.u64 	%rd28, [cast_bf16_f64_param_4];
	cvta.to.global.u64 	%rd1, %rd28;
	cvta.to.global.u64 	%rd2, %rd27;
	cvta.to.global.u64 	%rd3, %rd26;
	setp.eq.s64 	%p3, %rd25, 0;
	setp.eq.s64 	%p4, %rd26, 0;
	or.pred  	%p5, %p4, %p3;
	mov.pred 	%p2, -1;
	mov.pred 	%p19, %p2;
	@%p5 bra 	$L__BB4_5;

	mov.u64 	%rd59, 1;
	mov.u32 	%r34, 0;

$L__BB4_2:
	not.b32 	%r20, %r34;
	cvt.u64.u32 	%rd30, %r20;
	add.s64 	%rd31, %rd30, %rd25;
	shl.b64 	%rd32, %rd31, 3;
	and.b64  	%rd33, %rd32, 34359738360;
	add.s64 	%rd5, %rd3, %rd33;
	ld.global.u64 	%rd6, [%rd5];
	setp.lt.u64 	%p6, %rd6, 2;
	@%p6 bra 	$L__BB4_4;

	shl.b64 	%rd34, %rd25, 3;
	add.s64 	%rd35, %rd5, %rd34;
	ld.global.u64 	%rd36, [%rd35];
	setp.ne.s64 	%p8, %rd59, %rd36;
	mov.pred 	%p19, 0;
	@%p8 bra 	$L__BB4_5;

$L__BB4_4:
	mul.lo.s64 	%rd59, %rd6, %rd59;
	add.s32 	%r34, %r34, 1;
	cvt.u64.u32 	%rd37, %r34;
	setp.lt.u64 	%p10, %rd37, %rd25;
	mov.pred 	%p19, %p2;
	@%p10 bra 	$L__BB4_2;

$L__BB4_5:
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r21, %ctaid.x;
	mov.u32 	%r22, %tid.x;
	mad.lo.s32 	%r35, %r21, %r3, %r22;
	cvt.u64.u32 	%rd60, %r35;
	@%p19 bra 	$L__BB4_15;
	bra.uni 	$L__BB4_6;

$L__BB4_15:
	setp.ge.u64 	%p17, %rd60, %rd24;
	@%p17 bra 	$L__BB4_18;

	mov.u32 	%r33, %nctaid.x;
	mul.lo.s32 	%r16, %r3, %r33;

$L__BB4_17:
	shl.b64 	%rd55, %rd60, 1;
	add.s64 	%rd56, %rd2, %rd55;
	ld.global.u16 	%rs3, [%rd56];
	// begin inline asm
	{ mov.b32 %f3, {0,%rs3};}

	// end inline asm
	cvt.f64.f32 	%fd3, %f3;
	shl.b64 	%rd57, %rd60, 3;
	add.s64 	%rd58, %rd1, %rd57;
	st.global.f64 	[%rd58], %fd3;
	add.s32 	%r35, %r35, %r16;
	cvt.u64.u32 	%rd60, %r35;
	setp.lt.u64 	%p18, %rd60, %rd24;
	@%p18 bra 	$L__BB4_17;
	bra.uni 	$L__BB4_18;

$L__BB4_6:
	setp.ge.u64 	%p11, %rd60, %rd24;
	@%p11 bra 	$L__BB4_18;

	mov.u32 	%r23, %nctaid.x;
	mul.lo.s32 	%r5, %r3, %r23;
	@%p3 bra 	$L__BB4_14;

$L__BB4_8:
	mov.u32 	%r36, 0;
	mov.u32 	%r37, %r35;
	mov.u32 	%r38, %r36;

$L__BB4_9:
	not.b32 	%r26, %r36;
	cvt.u64.u32 	%rd38, %r26;
	add.s64 	%rd39, %rd38, %rd25;
	cvt.u64.u32 	%rd10, %r37;
	shl.b64 	%rd40, %rd39, 3;
	and.b64  	%rd41, %rd40, 34359738360;
	add.s64 	%rd11, %rd3, %rd41;
	ld.global.u64 	%rd12, [%rd11];
	and.b64  	%rd42, %rd12, -4294967296;
	setp.eq.s64 	%p13, %rd42, 0;
	@%p13 bra 	$L__BB4_11;

	div.u64 	%rd61, %rd10, %rd12;
	mul.lo.s64 	%rd43, %rd61, %rd12;
	sub.s64 	%rd62, %rd10, %rd43;
	bra.uni 	$L__BB4_12;

$L__BB4_11:
	cvt.u32.u64 	%r27, %rd12;
	cvt.u32.u64 	%r28, %rd10;
	div.u32 	%r29, %r28, %r27;
	mul.lo.s32 	%r30, %r29, %r27;
	sub.s32 	%r31, %r28, %r30;
	cvt.u64.u32 	%rd61, %r29;
	cvt.u64.u32 	%rd62, %r31;

$L__BB4_12:
	shl.b64 	%rd44, %rd25, 3;
	add.s64 	%rd45, %rd11, %rd44;
	ld.global.u64 	%rd46, [%rd45];
	mul.lo.s64 	%rd47, %rd46, %rd62;
	cvt.u32.u64 	%r32, %rd47;
	add.s32 	%r38, %r38, %r32;
	cvt.u32.u64 	%r37, %rd61;
	add.s32 	%r36, %r36, 1;
	cvt.u64.u32 	%rd48, %r36;
	setp.lt.u64 	%p14, %rd48, %rd25;
	@%p14 bra 	$L__BB4_9;

	mul.wide.u32 	%rd49, %r38, 2;
	add.s64 	%rd50, %rd2, %rd49;
	ld.global.u16 	%rs1, [%rd50];
	// begin inline asm
	{ mov.b32 %f1, {0,%rs1};}

	// end inline asm
	cvt.f64.f32 	%fd1, %f1;
	shl.b64 	%rd51, %rd60, 3;
	add.s64 	%rd52, %rd1, %rd51;
	st.global.f64 	[%rd52], %fd1;
	add.s32 	%r35, %r35, %r5;
	cvt.u64.u32 	%rd60, %r35;
	setp.lt.u64 	%p15, %rd60, %rd24;
	@%p15 bra 	$L__BB4_8;
	bra.uni 	$L__BB4_18;

$L__BB4_14:
	ld.global.u16 	%rs2, [%rd2];
	// begin inline asm
	{ mov.b32 %f2, {0,%rs2};}

	// end inline asm
	cvt.f64.f32 	%fd2, %f2;
	shl.b64 	%rd53, %rd60, 3;
	add.s64 	%rd54, %rd1, %rd53;
	st.global.f64 	[%rd54], %fd2;
	add.s32 	%r35, %r35, %r5;
	cvt.u64.u32 	%rd60, %r35;
	setp.lt.u64 	%p16, %rd60, %rd24;
	@%p16 bra 	$L__BB4_14;

$L__BB4_18:
	ret;

}
	// .globl	cast_f16_bf16
.visible .entry cast_f16_bf16(
	.param .u64 cast_f16_bf16_param_0,
	.param .u64 cast_f16_bf16_param_1,
	.param .u64 cast_f16_bf16_param_2,
	.param .u64 cast_f16_bf16_param_3,
	.param .u64 cast_f16_bf16_param_4
)
{
	.reg .pred 	%p<38>;
	.reg .b16 	%rs<22>;
	.reg .f32 	%f<4>;
	.reg .b32 	%r<56>;
	.reg .b64 	%rd<65>;


	ld.param.u64 	%rd24, [cast_f16_bf16_param_0];
	ld.param.u64 	%rd25, [cast_f16_bf16_param_1];
	ld.param.u64 	%rd26, [cast_f16_bf16_param_2];
	ld.param.u64 	%rd27, [cast_f16_bf16_param_3];
	ld.param.u64 	%rd28, [cast_f16_bf16_param_4];
	cvta.to.global.u64 	%rd1, %rd28;
	cvta.to.global.u64 	%rd2, %rd27;
	cvta.to.global.u64 	%rd3, %rd26;
	setp.eq.s64 	%p3, %rd25, 0;
	setp.eq.s64 	%p4, %rd26, 0;
	or.pred  	%p5, %p4, %p3;
	mov.pred 	%p2, -1;
	mov.pred 	%p37, %p2;
	@%p5 bra 	$L__BB5_5;

	mov.u64 	%rd59, 1;
	mov.u32 	%r49, 0;

$L__BB5_2:
	not.b32 	%r23, %r49;
	cvt.u64.u32 	%rd30, %r23;
	add.s64 	%rd31, %rd30, %rd25;
	shl.b64 	%rd32, %rd31, 3;
	and.b64  	%rd33, %rd32, 34359738360;
	add.s64 	%rd5, %rd3, %rd33;
	ld.global.u64 	%rd6, [%rd5];
	setp.lt.u64 	%p6, %rd6, 2;
	@%p6 bra 	$L__BB5_4;

	shl.b64 	%rd34, %rd25, 3;
	add.s64 	%rd35, %rd5, %rd34;
	ld.global.u64 	%rd36, [%rd35];
	setp.ne.s64 	%p8, %rd59, %rd36;
	mov.pred 	%p37, 0;
	@%p8 bra 	$L__BB5_5;

$L__BB5_4:
	mul.lo.s64 	%rd59, %rd6, %rd59;
	add.s32 	%r49, %r49, 1;
	cvt.u64.u32 	%rd37, %r49;
	setp.lt.u64 	%p10, %rd37, %rd25;
	mov.pred 	%p37, %p2;
	@%p10 bra 	$L__BB5_2;

$L__BB5_5:
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r24, %ctaid.x;
	mov.u32 	%r25, %tid.x;
	mad.lo.s32 	%r50, %r24, %r3, %r25;
	cvt.u64.u32 	%rd60, %r50;
	@%p37 bra 	$L__BB5_21;
	bra.uni 	$L__BB5_6;

$L__BB5_21:
	setp.ge.u64 	%p29, %rd60, %rd24;
	@%p29 bra 	$L__BB5_27;

	mov.u32 	%r44, %nctaid.x;
	mul.lo.s32 	%r18, %r3, %r44;

$L__BB5_23:
	shl.b64 	%rd55, %rd60, 1;
	add.s64 	%rd56, %rd2, %rd55;
	ld.global.u16 	%rs16, [%rd56];
	// begin inline asm
	{  cvt.f32.f16 %f3, %rs16;}

	// end inline asm
	mov.b32 	%r45, %f3;
	and.b32  	%r46, %r45, 2147483647;
	setp.gt.u32 	%p30, %r46, 2139095040;
	shl.b32 	%r47, %r45, 16;
	shr.u32 	%r48, %r45, 16;
	cvt.u16.u32 	%rs17, %r48;
	selp.b32 	%r20, 0, %r47, %p30;
	selp.b16 	%rs21, 32767, %rs17, %p30;
	setp.gt.u32 	%p31, %r20, -2147483648;
	@%p31 bra 	$L__BB5_25;

	setp.ne.s32 	%p32, %r20, -2147483648;
	and.b16  	%rs18, %rs21, 1;
	setp.eq.b16 	%p33, %rs18, 1;
	not.pred 	%p34, %p33;
	or.pred  	%p35, %p32, %p34;
	@%p35 bra 	$L__BB5_26;

$L__BB5_25:
	add.s16 	%rs21, %rs21, 1;

$L__BB5_26:
	add.s64 	%rd58, %rd1, %rd55;
	st.global.u16 	[%rd58], %rs21;
	add.s32 	%r50, %r50, %r18;
	cvt.u64.u32 	%rd60, %r50;
	setp.lt.u64 	%p36, %rd60, %rd24;
	@%p36 bra 	$L__BB5_23;
	bra.uni 	$L__BB5_27;

$L__BB5_6:
	setp.ge.u64 	%p11, %rd60, %rd24;
	@%p11 bra 	$L__BB5_27;

	mov.u32 	%r26, %nctaid.x;
	mul.lo.s32 	%r5, %r3, %r26;
	@%p3 bra 	$L__BB5_17;

$L__BB5_8:
	mov.u32 	%r51, 0;
	mov.u32 	%r52, %r50;
	mov.u32 	%r53, %r51;

$L__BB5_9:
	not.b32 	%r29, %r51;
	cvt.u64.u32 	%rd38, %r29;
	add.s64 	%rd39, %rd38, %rd25;
	cvt.u64.u32 	%rd10, %r52;
	shl.b64 	%rd40, %rd39, 3;
	and.b64  	%rd41, %rd40, 34359738360;
	add.s64 	%rd11, %rd3, %rd41;
	ld.global.u64 	%rd12, [%rd11];
	and.b64  	%rd42, %rd12, -4294967296;
	setp.eq.s64 	%p13, %rd42, 0;
	@%p13 bra 	$L__BB5_11;

	div.u64 	%rd61, %rd10, %rd12;
	mul.lo.s64 	%rd43, %rd61, %rd12;
	sub.s64 	%rd62, %rd10, %rd43;
	bra.uni 	$L__BB5_12;

$L__BB5_11:
	cvt.u32.u64 	%r30, %rd12;
	cvt.u32.u64 	%r31, %rd10;
	div.u32 	%r32, %r31, %r30;
	mul.lo.s32 	%r33, %r32, %r30;
	sub.s32 	%r34, %r31, %r33;
	cvt.u64.u32 	%rd61, %r32;
	cvt.u64.u32 	%rd62, %r34;

$L__BB5_12:
	shl.b64 	%rd44, %rd25, 3;
	add.s64 	%rd45, %rd11, %rd44;
	ld.global.u64 	%rd46, [%rd45];
	mul.lo.s64 	%rd47, %rd46, %rd62;
	cvt.u32.u64 	%r35, %rd47;
	add.s32 	%r53, %r53, %r35;
	cvt.u32.u64 	%r52, %rd61;
	add.s32 	%r51, %r51, 1;
	cvt.u64.u32 	%rd48, %r51;
	setp.lt.u64 	%p14, %rd48, %rd25;
	@%p14 bra 	$L__BB5_9;

	mul.wide.u32 	%rd49, %r53, 2;
	add.s64 	%rd50, %rd2, %rd49;
	ld.global.u16 	%rs10, [%rd50];
	// begin inline asm
	{  cvt.f32.f16 %f1, %rs10;}

	// end inline asm
	mov.b32 	%r36, %f1;
	and.b32  	%r37, %r36, 2147483647;
	setp.gt.u32 	%p15, %r37, 2139095040;
	shl.b32 	%r38, %r36, 16;
	shr.u32 	%r39, %r36, 16;
	cvt.u16.u32 	%rs11, %r39;
	selp.b32 	%r13, 0, %r38, %p15;
	selp.b16 	%rs19, 32767, %rs11, %p15;
	setp.gt.u32 	%p16, %r13, -2147483648;
	@%p16 bra 	$L__BB5_15;

	setp.ne.s32 	%p17, %r13, -2147483648;
	and.b16  	%rs12, %rs19, 1;
	setp.eq.b16 	%p18, %rs12, 1;
	not.pred 	%p19, %p18;
	or.pred  	%p20, %p17, %p19;
	@%p20 bra 	$L__BB5_16;

$L__BB5_15:
	add.s16 	%rs19, %rs19, 1;

$L__BB5_16:
	shl.b64 	%rd51, %rd60, 1;
	add.s64 	%rd52, %rd1, %rd51;
	st.global.u16 	[%rd52], %rs19;
	add.s32 	%r50, %r50, %r5;
	cvt.u64.u32 	%rd60, %r50;
	setp.lt.u64 	%p21, %rd60, %rd24;
	@%p21 bra 	$L__BB5_8;
	bra.uni 	$L__BB5_27;

$L__BB5_17:
	ld.global.u16 	%rs13, [%rd2];
	// begin inline asm
	{  cvt.f32.f16 %f2, %rs13;}

	// end inline asm
	mov.b32 	%r40, %f2;
	and.b32  	%r41, %r40, 2147483647;
	setp.gt.u32 	%p22, %r41, 2139095040;
	shl.b32 	%r42, %r40, 16;
	shr.u32 	%r43, %r40, 16;
	cvt.u16.u32 	%rs14, %r43;
	selp.b32 	%r16, 0, %r42, %p22;
	selp.b16 	%rs20, 32767, %rs14, %p22;
	setp.gt.u32 	%p23, %r16, -2147483648;
	@%p23 bra 	$L__BB5_19;

	setp.ne.s32 	%p24, %r16, -2147483648;
	and.b16  	%rs15, %rs20, 1;
	setp.eq.b16 	%p25, %rs15, 1;
	not.pred 	%p26, %p25;
	or.pred  	%p27, %p24, %p26;
	@%p27 bra 	$L__BB5_20;

$L__BB5_19:
	add.s16 	%rs20, %rs20, 1;

$L__BB5_20:
	shl.b64 	%rd53, %rd60, 1;
	add.s64 	%rd54, %rd1, %rd53;
	st.global.u16 	[%rd54], %rs20;
	add.s32 	%r50, %r50, %r5;
	cvt.u64.u32 	%rd60, %r50;
	setp.lt.u64 	%p28, %rd60, %rd24;
	@%p28 bra 	$L__BB5_17;

$L__BB5_27:
	ret;

}
	// .globl	cast_f64_bf16
.visible .entry cast_f64_bf16(
	.param .u64 cast_f64_bf16_param_0,
	.param .u64 cast_f64_bf16_param_1,
	.param .u64 cast_f64_bf16_param_2,
	.param .u64 cast_f64_bf16_param_3,
	.param .u64 cast_f64_bf16_param_4
)
{
	.reg .pred 	%p<38>;
	.reg .b16 	%rs<19>;
	.reg .f32 	%f<4>;
	.reg .b32 	%r<56>;
	.reg .f64 	%fd<4>;
	.reg .b64 	%rd<65>;


	ld.param.u64 	%rd24, [cast_f64_bf16_param_0];
	ld.param.u64 	%rd25, [cast_f64_bf16_param_1];
	ld.param.u64 	%rd26, [cast_f64_bf16_param_2];
	ld.param.u64 	%rd27, [cast_f64_bf16_param_3];
	ld.param.u64 	%rd28, [cast_f64_bf16_param_4];
	cvta.to.global.u64 	%rd1, %rd28;
	cvta.to.global.u64 	%rd2, %rd27;
	cvta.to.global.u64 	%rd3, %rd26;
	setp.eq.s64 	%p3, %rd25, 0;
	setp.eq.s64 	%p4, %rd26, 0;
	or.pred  	%p5, %p4, %p3;
	mov.pred 	%p2, -1;
	mov.pred 	%p37, %p2;
	@%p5 bra 	$L__BB6_5;

	mov.u64 	%rd59, 1;
	mov.u32 	%r49, 0;

$L__BB6_2:
	not.b32 	%r23, %r49;
	cvt.u64.u32 	%rd30, %r23;
	add.s64 	%rd31, %rd30, %rd25;
	shl.b64 	%rd32, %rd31, 3;
	and.b64  	%rd33, %rd32, 34359738360;
	add.s64 	%rd5, %rd3, %rd33;
	ld.global.u64 	%rd6, [%rd5];
	setp.lt.u64 	%p6, %rd6, 2;
	@%p6 bra 	$L__BB6_4;

	shl.b64 	%rd34, %rd25, 3;
	add.s64 	%rd35, %rd5, %rd34;
	ld.global.u64 	%rd36, [%rd35];
	setp.ne.s64 	%p8, %rd59, %rd36;
	mov.pred 	%p37, 0;
	@%p8 bra 	$L__BB6_5;

$L__BB6_4:
	mul.lo.s64 	%rd59, %rd6, %rd59;
	add.s32 	%r49, %r49, 1;
	cvt.u64.u32 	%rd37, %r49;
	setp.lt.u64 	%p10, %rd37, %rd25;
	mov.pred 	%p37, %p2;
	@%p10 bra 	$L__BB6_2;

$L__BB6_5:
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r24, %ctaid.x;
	mov.u32 	%r25, %tid.x;
	mad.lo.s32 	%r50, %r24, %r3, %r25;
	cvt.u64.u32 	%rd60, %r50;
	@%p37 bra 	$L__BB6_21;
	bra.uni 	$L__BB6_6;

$L__BB6_21:
	setp.ge.u64 	%p29, %rd60, %rd24;
	@%p29 bra 	$L__BB6_27;

	mov.u32 	%r44, %nctaid.x;
	mul.lo.s32 	%r18, %r3, %r44;

$L__BB6_23:
	shl.b64 	%rd55, %rd60, 3;
	add.s64 	%rd56, %rd2, %rd55;
	ld.global.f64 	%fd3, [%rd56];
	cvt.rn.f32.f64 	%f3, %fd3;
	mov.b32 	%r45, %f3;
	and.b32  	%r46, %r45, 2147483647;
	setp.gt.u32 	%p30, %r46, 2139095040;
	shl.b32 	%r47, %r45, 16;
	shr.u32 	%r48, %r45, 16;
	cvt.u16.u32 	%rs14, %r48;
	selp.b32 	%r20, 0, %r47, %p30;
	selp.b16 	%rs18, 32767, %rs14, %p30;
	setp.gt.u32 	%p31, %r20, -2147483648;
	@%p31 bra 	$L__BB6_25;

	setp.ne.s32 	%p32, %r20, -2147483648;
	and.b16  	%rs15, %rs18, 1;
	setp.eq.b16 	%p33, %rs15, 1;
	not.pred 	%p34, %p33;
	or.pred  	%p35, %p32, %p34;
	@%p35 bra 	$L__BB6_26;

$L__BB6_25:
	add.s16 	%rs18, %rs18, 1;

$L__BB6_26:
	shl.b64 	%rd57, %rd60, 1;
	add.s64 	%rd58, %rd1, %rd57;
	st.global.u16 	[%rd58], %rs18;
	add.s32 	%r50, %r50, %r18;
	cvt.u64.u32 	%rd60, %r50;
	setp.lt.u64 	%p36, %rd60, %rd24;
	@%p36 bra 	$L__BB6_23;
	bra.uni 	$L__BB6_27;

$L__BB6_6:
	setp.ge.u64 	%p11, %rd60, %rd24;
	@%p11 bra 	$L__BB6_27;

	mov.u32 	%r26, %nctaid.x;
	mul.lo.s32 	%r5, %r3, %r26;
	@%p3 bra 	$L__BB6_17;

$L__BB6_8:
	mov.u32 	%r51, 0;
	mov.u32 	%r52, %r50;
	mov.u32 	%r53, %r51;

$L__BB6_9:
	not.b32 	%r29, %r51;
	cvt.u64.u32 	%rd38, %r29;
	add.s64 	%rd39, %rd38, %rd25;
	cvt.u64.u32 	%rd10, %r52;
	shl.b64 	%rd40, %rd39, 3;
	and.b64  	%rd41, %rd40, 34359738360;
	add.s64 	%rd11, %rd3, %rd41;
	ld.global.u64 	%rd12, [%rd11];
	and.b64  	%rd42, %rd12, -4294967296;
	setp.eq.s64 	%p13, %rd42, 0;
	@%p13 bra 	$L__BB6_11;

	div.u64 	%rd61, %rd10, %rd12;
	mul.lo.s64 	%rd43, %rd61, %rd12;
	sub.s64 	%rd62, %rd10, %rd43;
	bra.uni 	$L__BB6_12;

$L__BB6_11:
	cvt.u32.u64 	%r30, %rd12;
	cvt.u32.u64 	%r31, %rd10;
	div.u32 	%r32, %r31, %r30;
	mul.lo.s32 	%r33, %r32, %r30;
	sub.s32 	%r34, %r31, %r33;
	cvt.u64.u32 	%rd61, %r32;
	cvt.u64.u32 	%rd62, %r34;

$L__BB6_12:
	shl.b64 	%rd44, %rd25, 3;
	add.s64 	%rd45, %rd11, %rd44;
	ld.global.u64 	%rd46, [%rd45];
	mul.lo.s64 	%rd47, %rd46, %rd62;
	cvt.u32.u64 	%r35, %rd47;
	add.s32 	%r53, %r53, %r35;
	cvt.u32.u64 	%r52, %rd61;
	add.s32 	%r51, %r51, 1;
	cvt.u64.u32 	%rd48, %r51;
	setp.lt.u64 	%p14, %rd48, %rd25;
	@%p14 bra 	$L__BB6_9;

	mul.wide.u32 	%rd49, %r53, 8;
	add.s64 	%rd50, %rd2, %rd49;
	ld.global.f64 	%fd1, [%rd50];
	cvt.rn.f32.f64 	%f1, %fd1;
	mov.b32 	%r36, %f1;
	and.b32  	%r37, %r36, 2147483647;
	setp.gt.u32 	%p15, %r37, 2139095040;
	shl.b32 	%r38, %r36, 16;
	shr.u32 	%r39, %r36, 16;
	cvt.u16.u32 	%rs10, %r39;
	selp.b32 	%r13, 0, %r38, %p15;
	selp.b16 	%rs16, 32767, %rs10, %p15;
	setp.gt.u32 	%p16, %r13, -2147483648;
	@%p16 bra 	$L__BB6_15;

	setp.ne.s32 	%p17, %r13, -2147483648;
	and.b16  	%rs11, %rs16, 1;
	setp.eq.b16 	%p18, %rs11, 1;
	not.pred 	%p19, %p18;
	or.pred  	%p20, %p17, %p19;
	@%p20 bra 	$L__BB6_16;

$L__BB6_15:
	add.s16 	%rs16, %rs16, 1;

$L__BB6_16:
	shl.b64 	%rd51, %rd60, 1;
	add.s64 	%rd52, %rd1, %rd51;
	st.global.u16 	[%rd52], %rs16;
	add.s32 	%r50, %r50, %r5;
	cvt.u64.u32 	%rd60, %r50;
	setp.lt.u64 	%p21, %rd60, %rd24;
	@%p21 bra 	$L__BB6_8;
	bra.uni 	$L__BB6_27;

$L__BB6_17:
	ld.global.f64 	%fd2, [%rd2];
	cvt.rn.f32.f64 	%f2, %fd2;
	mov.b32 	%r40, %f2;
	and.b32  	%r41, %r40, 2147483647;
	setp.gt.u32 	%p22, %r41, 2139095040;
	shl.b32 	%r42, %r40, 16;
	shr.u32 	%r43, %r40, 16;
	cvt.u16.u32 	%rs12, %r43;
	selp.b32 	%r16, 0, %r42, %p22;
	selp.b16 	%rs17, 32767, %rs12, %p22;
	setp.gt.u32 	%p23, %r16, -2147483648;
	@%p23 bra 	$L__BB6_19;

	setp.ne.s32 	%p24, %r16, -2147483648;
	and.b16  	%rs13, %rs17, 1;
	setp.eq.b16 	%p25, %rs13, 1;
	not.pred 	%p26, %p25;
	or.pred  	%p27, %p24, %p26;
	@%p27 bra 	$L__BB6_20;

$L__BB6_19:
	add.s16 	%rs17, %rs17, 1;

$L__BB6_20:
	shl.b64 	%rd53, %rd60, 1;
	add.s64 	%rd54, %rd1, %rd53;
	st.global.u16 	[%rd54], %rs17;
	add.s32 	%r50, %r50, %r5;
	cvt.u64.u32 	%rd60, %r50;
	setp.lt.u64 	%p28, %rd60, %rd24;
	@%p28 bra 	$L__BB6_17;

$L__BB6_27:
	ret;

}
	// .globl	cast_u8_bf16
.visible .entry cast_u8_bf16(
	.param .u64 cast_u8_bf16_param_0,
	.param .u64 cast_u8_bf16_param_1,
	.param .u64 cast_u8_bf16_param_2,
	.param .u64 cast_u8_bf16_param_3,
	.param .u64 cast_u8_bf16_param_4
)
{
	.reg .pred 	%p<38>;
	.reg .b16 	%rs<22>;
	.reg .f32 	%f<4>;
	.reg .b32 	%r<56>;
	.reg .b64 	%rd<64>;


	ld.param.u64 	%rd24, [cast_u8_bf16_param_0];
	ld.param.u64 	%rd25, [cast_u8_bf16_param_1];
	ld.param.u64 	%rd26, [cast_u8_bf16_param_2];
	ld.param.u64 	%rd27, [cast_u8_bf16_param_3];
	ld.param.u64 	%rd28, [cast_u8_bf16_param_4];
	cvta.to.global.u64 	%rd1, %rd28;
	cvta.to.global.u64 	%rd2, %rd27;
	cvta.to.global.u64 	%rd3, %rd26;
	setp.eq.s64 	%p3, %rd25, 0;
	setp.eq.s64 	%p4, %rd26, 0;
	or.pred  	%p5, %p4, %p3;
	mov.pred 	%p2, -1;
	mov.pred 	%p37, %p2;
	@%p5 bra 	$L__BB7_5;

	mov.u64 	%rd58, 1;
	mov.u32 	%r49, 0;

$L__BB7_2:
	not.b32 	%r23, %r49;
	cvt.u64.u32 	%rd30, %r23;
	add.s64 	%rd31, %rd30, %rd25;
	shl.b64 	%rd32, %rd31, 3;
	and.b64  	%rd33, %rd32, 34359738360;
	add.s64 	%rd5, %rd3, %rd33;
	ld.global.u64 	%rd6, [%rd5];
	setp.lt.u64 	%p6, %rd6, 2;
	@%p6 bra 	$L__BB7_4;

	shl.b64 	%rd34, %rd25, 3;
	add.s64 	%rd35, %rd5, %rd34;
	ld.global.u64 	%rd36, [%rd35];
	setp.ne.s64 	%p8, %rd58, %rd36;
	mov.pred 	%p37, 0;
	@%p8 bra 	$L__BB7_5;

$L__BB7_4:
	mul.lo.s64 	%rd58, %rd6, %rd58;
	add.s32 	%r49, %r49, 1;
	cvt.u64.u32 	%rd37, %r49;
	setp.lt.u64 	%p10, %rd37, %rd25;
	mov.pred 	%p37, %p2;
	@%p10 bra 	$L__BB7_2;

$L__BB7_5:
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r24, %ctaid.x;
	mov.u32 	%r25, %tid.x;
	mad.lo.s32 	%r50, %r24, %r3, %r25;
	cvt.u64.u32 	%rd59, %r50;
	@%p37 bra 	$L__BB7_21;
	bra.uni 	$L__BB7_6;

$L__BB7_21:
	setp.ge.u64 	%p29, %rd59, %rd24;
	@%p29 bra 	$L__BB7_27;

	mov.u32 	%r44, %nctaid.x;
	mul.lo.s32 	%r18, %r3, %r44;

$L__BB7_23:
	add.s64 	%rd55, %rd2, %rd59;
	ld.global.u8 	%rs16, [%rd55];
	cvt.rn.f32.u16 	%f3, %rs16;
	mov.b32 	%r45, %f3;
	and.b32  	%r46, %r45, 2147483647;
	setp.gt.u32 	%p30, %r46, 2139095040;
	shl.b32 	%r47, %r45, 16;
	shr.u32 	%r48, %r45, 16;
	cvt.u16.u32 	%rs17, %r48;
	selp.b32 	%r20, 0, %r47, %p30;
	selp.b16 	%rs21, 32767, %rs17, %p30;
	setp.gt.u32 	%p31, %r20, -2147483648;
	@%p31 bra 	$L__BB7_25;

	setp.ne.s32 	%p32, %r20, -2147483648;
	and.b16  	%rs18, %rs21, 1;
	setp.eq.b16 	%p33, %rs18, 1;
	not.pred 	%p34, %p33;
	or.pred  	%p35, %p32, %p34;
	@%p35 bra 	$L__BB7_26;

$L__BB7_25:
	add.s16 	%rs21, %rs21, 1;

$L__BB7_26:
	shl.b64 	%rd56, %rd59, 1;
	add.s64 	%rd57, %rd1, %rd56;
	st.global.u16 	[%rd57], %rs21;
	add.s32 	%r50, %r50, %r18;
	cvt.u64.u32 	%rd59, %r50;
	setp.lt.u64 	%p36, %rd59, %rd24;
	@%p36 bra 	$L__BB7_23;
	bra.uni 	$L__BB7_27;

$L__BB7_6:
	setp.ge.u64 	%p11, %rd59, %rd24;
	@%p11 bra 	$L__BB7_27;

	mov.u32 	%r26, %nctaid.x;
	mul.lo.s32 	%r5, %r3, %r26;
	@%p3 bra 	$L__BB7_17;

$L__BB7_8:
	mov.u32 	%r51, 0;
	mov.u32 	%r52, %r50;
	mov.u32 	%r53, %r51;

$L__BB7_9:
	not.b32 	%r29, %r51;
	cvt.u64.u32 	%rd38, %r29;
	add.s64 	%rd39, %rd38, %rd25;
	cvt.u64.u32 	%rd10, %r52;
	shl.b64 	%rd40, %rd39, 3;
	and.b64  	%rd41, %rd40, 34359738360;
	add.s64 	%rd11, %rd3, %rd41;
	ld.global.u64 	%rd12, [%rd11];
	and.b64  	%rd42, %rd12, -4294967296;
	setp.eq.s64 	%p13, %rd42, 0;
	@%p13 bra 	$L__BB7_11;

	div.u64 	%rd60, %rd10, %rd12;
	mul.lo.s64 	%rd43, %rd60, %rd12;
	sub.s64 	%rd61, %rd10, %rd43;
	bra.uni 	$L__BB7_12;

$L__BB7_11:
	cvt.u32.u64 	%r30, %rd12;
	cvt.u32.u64 	%r31, %rd10;
	div.u32 	%r32, %r31, %r30;
	mul.lo.s32 	%r33, %r32, %r30;
	sub.s32 	%r34, %r31, %r33;
	cvt.u64.u32 	%rd60, %r32;
	cvt.u64.u32 	%rd61, %r34;

$L__BB7_12:
	shl.b64 	%rd44, %rd25, 3;
	add.s64 	%rd45, %rd11, %rd44;
	ld.global.u64 	%rd46, [%rd45];
	mul.lo.s64 	%rd47, %rd46, %rd61;
	cvt.u32.u64 	%r35, %rd47;
	add.s32 	%r53, %r53, %r35;
	cvt.u32.u64 	%r52, %rd60;
	add.s32 	%r51, %r51, 1;
	cvt.u64.u32 	%rd48, %r51;
	setp.lt.u64 	%p14, %rd48, %rd25;
	@%p14 bra 	$L__BB7_9;

	cvt.u64.u32 	%rd49, %r53;
	add.s64 	%rd50, %rd2, %rd49;
	ld.global.u8 	%rs10, [%rd50];
	cvt.rn.f32.u16 	%f1, %rs10;
	mov.b32 	%r36, %f1;
	and.b32  	%r37, %r36, 2147483647;
	setp.gt.u32 	%p15, %r37, 2139095040;
	shl.b32 	%r38, %r36, 16;
	shr.u32 	%r39, %r36, 16;
	cvt.u16.u32 	%rs11, %r39;
	selp.b32 	%r13, 0, %r38, %p15;
	selp.b16 	%rs19, 32767, %rs11, %p15;
	setp.gt.u32 	%p16, %r13, -2147483648;
	@%p16 bra 	$L__BB7_15;

	setp.ne.s32 	%p17, %r13, -2147483648;
	and.b16  	%rs12, %rs19, 1;
	setp.eq.b16 	%p18, %rs12, 1;
	not.pred 	%p19, %p18;
	or.pred  	%p20, %p17, %p19;
	@%p20 bra 	$L__BB7_16;

$L__BB7_15:
	add.s16 	%rs19, %rs19, 1;

$L__BB7_16:
	shl.b64 	%rd51, %rd59, 1;
	add.s64 	%rd52, %rd1, %rd51;
	st.global.u16 	[%rd52], %rs19;
	add.s32 	%r50, %r50, %r5;
	cvt.u64.u32 	%rd59, %r50;
	setp.lt.u64 	%p21, %rd59, %rd24;
	@%p21 bra 	$L__BB7_8;
	bra.uni 	$L__BB7_27;

$L__BB7_17:
	ld.global.u8 	%rs13, [%rd2];
	cvt.rn.f32.u16 	%f2, %rs13;
	mov.b32 	%r40, %f2;
	and.b32  	%r41, %r40, 2147483647;
	setp.gt.u32 	%p22, %r41, 2139095040;
	shl.b32 	%r42, %r40, 16;
	shr.u32 	%r43, %r40, 16;
	cvt.u16.u32 	%rs14, %r43;
	selp.b32 	%r16, 0, %r42, %p22;
	selp.b16 	%rs20, 32767, %rs14, %p22;
	setp.gt.u32 	%p23, %r16, -2147483648;
	@%p23 bra 	$L__BB7_19;

	setp.ne.s32 	%p24, %r16, -2147483648;
	and.b16  	%rs15, %rs20, 1;
	setp.eq.b16 	%p25, %rs15, 1;
	not.pred 	%p26, %p25;
	or.pred  	%p27, %p24, %p26;
	@%p27 bra 	$L__BB7_20;

$L__BB7_19:
	add.s16 	%rs20, %rs20, 1;

$L__BB7_20:
	shl.b64 	%rd53, %rd59, 1;
	add.s64 	%rd54, %rd1, %rd53;
	st.global.u16 	[%rd54], %rs20;
	add.s32 	%r50, %r50, %r5;
	cvt.u64.u32 	%rd59, %r50;
	setp.lt.u64 	%p28, %rd59, %rd24;
	@%p28 bra 	$L__BB7_17;

$L__BB7_27:
	ret;

}
	// .globl	cast_bf16_f8_e4m3
.visible .entry cast_bf16_f8_e4m3(
	.param .u64 cast_bf16_f8_e4m3_param_0,
	.param .u64 cast_bf16_f8_e4m3_param_1,
	.param .u64 cast_bf16_f8_e4m3_param_2,
	.param .u64 cast_bf16_f8_e4m3_param_3,
	.param .u64 cast_bf16_f8_e4m3_param_4
)
{
	.reg .pred 	%p<65>;
	.reg .b16 	%rs<13>;
	.reg .f32 	%f<4>;
	.reg .b32 	%r<125>;
	.reg .f64 	%fd<4>;
	.reg .b64 	%rd<104>;


	ld.param.u64 	%rd36, [cast_bf16_f8_e4m3_param_0];
	ld.param.u64 	%rd37, [cast_bf16_f8_e4m3_param_1];
	ld.param.u64 	%rd38, [cast_bf16_f8_e4m3_param_2];
	ld.param.u64 	%rd39, [cast_bf16_f8_e4m3_param_3];
	ld.param.u64 	%rd40, [cast_bf16_f8_e4m3_param_4];
	cvta.to.global.u64 	%rd1, %rd40;
	cvta.to.global.u64 	%rd2, %rd39;
	cvta.to.global.u64 	%rd3, %rd38;
	setp.eq.s64 	%p3, %rd37, 0;
	setp.eq.s64 	%p4, %rd38, 0;
	or.pred  	%p5, %p4, %p3;
	mov.pred 	%p2, -1;
	mov.pred 	%p64, %p2;
	@%p5 bra 	$L__BB8_5;

	mov.u64 	%rd98, 1;
	mov.u32 	%r115, 0;

$L__BB8_2:
	not.b32 	%r41, %r115;
	cvt.u64.u32 	%rd42, %r41;
	add.s64 	%rd43, %rd42, %rd37;
	shl.b64 	%rd44, %rd43, 3;
	and.b64  	%rd45, %rd44, 34359738360;
	add.s64 	%rd5, %rd3, %rd45;
	ld.global.u64 	%rd6, [%rd5];
	setp.lt.u64 	%p6, %rd6, 2;
	@%p6 bra 	$L__BB8_4;

	shl.b64 	%rd46, %rd37, 3;
	add.s64 	%rd47, %rd5, %rd46;
	ld.global.u64 	%rd48, [%rd47];
	setp.ne.s64 	%p8, %rd98, %rd48;
	mov.pred 	%p64, 0;
	@%p8 bra 	$L__BB8_5;

$L__BB8_4:
	mul.lo.s64 	%rd98, %rd6, %rd98;
	add.s32 	%r115, %r115, 1;
	cvt.u64.u32 	%rd49, %r115;
	setp.lt.u64 	%p10, %rd49, %rd37;
	mov.pred 	%p64, %p2;
	@%p10 bra 	$L__BB8_2;

$L__BB8_5:
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r42, %ctaid.x;
	mov.u32 	%r43, %tid.x;
	mad.lo.s32 	%r116, %r42, %r3, %r43;
	cvt.u64.u32 	%rd99, %r116;
	@%p64 bra 	$L__BB8_31;
	bra.uni 	$L__BB8_6;

$L__BB8_31:
	setp.ge.u64 	%p47, %rd99, %rd36;
	@%p47 bra 	$L__BB8_42;

	mov.u32 	%r94, %nctaid.x;
	mul.lo.s32 	%r30, %r3, %r94;

$L__BB8_33:
	shl.b64 	%rd85, %rd99, 1;
	add.s64 	%rd86, %rd2, %rd85;
	ld.global.u16 	%rs9, [%rd86];
	// begin inline asm
	{ mov.b32 %f3, {0,%rs9};}

	// end inline asm
	mov.b32 	%r96, %f3;
	and.b32  	%r97, %r96, 2147483647;
	setp.gt.u32 	%p48, %r97, 2139095040;
	cvt.f64.f32 	%fd3, %f3;
	mov.b64 	%rd87, %fd3;
	selp.b64 	%rd31, 9223372036317904896, %rd87, %p48;
	shr.u64 	%rd88, %rd31, 52;
	cvt.u32.u64 	%r98, %rd88;
	add.s32 	%r32, %r98, 8;
	shr.u64 	%rd89, %rd31, 49;
	cvt.u32.u64 	%r33, %rd89;
	and.b32  	%r34, %r33, 7;
	and.b64  	%rd32, %rd31, 9223372036854775807;
	setp.lt.u64 	%p49, %rd32, 4562146422526312449;
	mov.u32 	%r124, 0;
	@%p49 bra 	$L__BB8_41;

	setp.gt.u64 	%p50, %rd32, 9218868437227405312;
	mov.u32 	%r124, 127;
	@%p50 bra 	$L__BB8_41;

	setp.gt.u64 	%p51, %rd32, 4646870390516219904;
	mov.u32 	%r124, 126;
	@%p51 bra 	$L__BB8_41;

	setp.lt.u64 	%p52, %rd32, 4580160821035794432;
	@%p52 bra 	$L__BB8_38;
	bra.uni 	$L__BB8_37;

$L__BB8_38:
	cvt.u16.u32 	%rs10, %r32;
	mov.u16 	%rs11, 1;
	sub.s16 	%rs12, %rs11, %rs10;
	cvt.u32.u16 	%r107, %rs12;
	and.b32  	%r108, %r107, 255;
	or.b32  	%r109, %r34, 8;
	shr.u32 	%r124, %r109, %r108;
	mov.u64 	%rd91, 562949953421312;
	shl.b64 	%rd92, %rd91, %r108;
	add.s64 	%rd93, %rd92, -1;
	or.b64  	%rd94, %rd31, 4503599627370496;
	and.b64  	%rd33, %rd93, %rd94;
	mov.u64 	%rd95, 281474976710656;
	shl.b64 	%rd34, %rd95, %r108;
	setp.gt.u64 	%p58, %rd33, %rd34;
	@%p58 bra 	$L__BB8_40;

	setp.ne.s64 	%p59, %rd33, %rd34;
	and.b32  	%r110, %r124, 1;
	setp.eq.b32 	%p60, %r110, 1;
	not.pred 	%p61, %p60;
	or.pred  	%p62, %p61, %p59;
	@%p62 bra 	$L__BB8_41;

$L__BB8_40:
	and.b32  	%r111, %r124, 255;
	add.s32 	%r124, %r111, 1;
	bra.uni 	$L__BB8_41;

$L__BB8_37:
	shl.b32 	%r101, %r32, 3;
	and.b32  	%r102, %r101, 2040;
	or.b32  	%r103, %r102, %r34;
	and.b64  	%rd90, %rd31, 562949953421311;
	setp.gt.u64 	%p53, %rd90, 281474976710656;
	setp.eq.s64 	%p54, %rd90, 281474976710656;
	and.b32  	%r104, %r33, 1;
	setp.eq.b32 	%p55, %r104, 1;
	and.pred  	%p56, %p54, %p55;
	or.pred  	%p57, %p53, %p56;
	and.b32  	%r105, %r103, 255;
	add.s32 	%r106, %r105, 1;
	selp.b32 	%r124, %r106, %r103, %p57;

$L__BB8_41:
	shr.u64 	%rd96, %rd31, 63;
	cvt.u32.u64 	%r112, %rd96;
	shl.b32 	%r113, %r112, 7;
	or.b32  	%r114, %r124, %r113;
	add.s64 	%rd97, %rd1, %rd99;
	st.global.u8 	[%rd97], %r114;
	add.s32 	%r116, %r116, %r30;
	cvt.u64.u32 	%rd99, %r116;
	setp.lt.u64 	%p63, %rd99, %rd36;
	@%p63 bra 	$L__BB8_33;
	bra.uni 	$L__BB8_42;

$L__BB8_6:
	setp.ge.u64 	%p11, %rd99, %rd36;
	@%p11 bra 	$L__BB8_42;

	mov.u32 	%r44, %nctaid.x;
	mul.lo.s32 	%r5, %r3, %r44;
	@%p3 bra 	$L__BB8_22;

$L__BB8_8:
	mov.u32 	%r117, 0;
	mov.u32 	%r118, %r116;
	mov.u32 	%r119, %r117;

$L__BB8_9:
	not.b32 	%r47, %r117;
	cvt.u64.u32 	%rd50, %r47;
	add.s64 	%rd51, %rd50, %rd37;
	cvt.u64.u32 	%rd10, %r118;
	shl.b64 	%rd52, %rd51, 3;
	and.b64  	%rd53, %rd52, 34359738360;
	add.s64 	%rd11, %rd3, %rd53;
	ld.global.u64 	%rd12, [%rd11];
	and.b64  	%rd54, %rd12, -4294967296;
	setp.eq.s64 	%p13, %rd54, 0;
	@%p13 bra 	$L__BB8_11;

	div.u64 	%rd100, %rd10, %rd12;
	mul.lo.s64 	%rd55, %rd100, %rd12;
	sub.s64 	%rd101, %rd10, %rd55;
	bra.uni 	$L__BB8_12;

$L__BB8_11:
	cvt.u32.u64 	%r48, %rd12;
	cvt.u32.u64 	%r49, %rd10;
	div.u32 	%r50, %r49, %r48;
	mul.lo.s32 	%r51, %r50, %r48;
	sub.s32 	%r52, %r49, %r51;
	cvt.u64.u32 	%rd100, %r50;
	cvt.u64.u32 	%rd101, %r52;

$L__BB8_12:
	shl.b64 	%rd56, %rd37, 3;
	add.s64 	%rd57, %rd11, %rd56;
	ld.global.u64 	%rd58, [%rd57];
	mul.lo.s64 	%rd59, %rd58, %rd101;
	cvt.u32.u64 	%r53, %rd59;
	add.s32 	%r119, %r119, %r53;
	cvt.u32.u64 	%r118, %rd100;
	add.s32 	%r117, %r117, 1;
	cvt.u64.u32 	%rd60, %r117;
	setp.lt.u64 	%p14, %rd60, %rd37;
	@%p14 bra 	$L__BB8_9;

	mul.wide.u32 	%rd61, %r119, 2;
	add.s64 	%rd62, %rd2, %rd61;
	ld.global.u16 	%rs1, [%rd62];
	// begin inline asm
	{ mov.b32 %f1, {0,%rs1};}

	// end inline asm
	mov.b32 	%r55, %f1;
	and.b32  	%r56, %r55, 2147483647;
	setp.gt.u32 	%p15, %r56, 2139095040;
	cvt.f64.f32 	%fd1, %f1;
	mov.b64 	%rd63, %fd1;
	selp.b64 	%rd19, 9223372036317904896, %rd63, %p15;
	shr.u64 	%rd64, %rd19, 52;
	cvt.u32.u64 	%r57, %rd64;
	add.s32 	%r13, %r57, 8;
	shr.u64 	%rd65, %rd19, 49;
	cvt.u32.u64 	%r14, %rd65;
	and.b32  	%r15, %r14, 7;
	and.b64  	%rd20, %rd19, 9223372036854775807;
	setp.lt.u64 	%p16, %rd20, 4562146422526312449;
	mov.u32 	%r120, 0;
	@%p16 bra 	$L__BB8_21;

	setp.gt.u64 	%p17, %rd20, 9218868437227405312;
	mov.u32 	%r120, 127;
	@%p17 bra 	$L__BB8_21;

	setp.gt.u64 	%p18, %rd20, 4646870390516219904;
	mov.u32 	%r120, 126;
	@%p18 bra 	$L__BB8_21;

	setp.lt.u64 	%p19, %rd20, 4580160821035794432;
	@%p19 bra 	$L__BB8_18;
	bra.uni 	$L__BB8_17;

$L__BB8_18:
	cvt.u16.u32 	%rs2, %r13;
	mov.u16 	%rs3, 1;
	sub.s16 	%rs4, %rs3, %rs2;
	cvt.u32.u16 	%r66, %rs4;
	and.b32  	%r67, %r66, 255;
	or.b32  	%r68, %r15, 8;
	shr.u32 	%r120, %r68, %r67;
	mov.u64 	%rd67, 562949953421312;
	shl.b64 	%rd68, %rd67, %r67;
	add.s64 	%rd69, %rd68, -1;
	or.b64  	%rd70, %rd19, 4503599627370496;
	and.b64  	%rd21, %rd69, %rd70;
	mov.u64 	%rd71, 281474976710656;
	shl.b64 	%rd22, %rd71, %r67;
	setp.gt.u64 	%p25, %rd21, %rd22;
	@%p25 bra 	$L__BB8_20;

	setp.ne.s64 	%p26, %rd21, %rd22;
	and.b32  	%r69, %r120, 1;
	setp.eq.b32 	%p27, %r69, 1;
	not.pred 	%p28, %p27;
	or.pred  	%p29, %p28, %p26;
	@%p29 bra 	$L__BB8_21;

$L__BB8_20:
	and.b32  	%r70, %r120, 255;
	add.s32 	%r120, %r70, 1;
	bra.uni 	$L__BB8_21;

$L__BB8_17:
	shl.b32 	%r60, %r13, 3;
	and.b32  	%r61, %r60, 2040;
	or.b32  	%r62, %r61, %r15;
	and.b64  	%rd66, %rd19, 562949953421311;
	setp.gt.u64 	%p20, %rd66, 281474976710656;
	setp.eq.s64 	%p21, %rd66, 281474976710656;
	and.b32  	%r63, %r14, 1;
	setp.eq.b32 	%p22, %r63, 1;
	and.pred  	%p23, %p21, %p22;
	or.pred  	%p24, %p20, %p23;
	and.b32  	%r64, %r62, 255;
	add.s32 	%r65, %r64, 1;
	selp.b32 	%r120, %r65, %r62, %p24;

$L__BB8_21:
	shr.u64 	%rd72, %rd19, 63;
	cvt.u32.u64 	%r71, %rd72;
	shl.b32 	%r72, %r71, 7;
	or.b32  	%r73, %r120, %r72;
	add.s64 	%rd73, %rd1, %rd99;
	st.global.u8 	[%rd73], %r73;
	add.s32 	%r116, %r116, %r5;
	cvt.u64.u32 	%rd99, %r116;
	setp.lt.u64 	%p30, %rd99, %rd36;
	@%p30 bra 	$L__BB8_8;
	bra.uni 	$L__BB8_42;

$L__BB8_22:
	ld.global.u16 	%rs5, [%rd2];
	// begin inline asm
	{ mov.b32 %f2, {0,%rs5};}

	// end inline asm
	mov.b32 	%r75, %f2;
	and.b32  	%r76, %r75, 2147483647;
	setp.gt.u32 	%p31, %r76, 2139095040;
	cvt.f64.f32 	%fd2, %f2;
	mov.b64 	%rd74, %fd2;
	selp.b64 	%rd25, 9223372036317904896, %rd74, %p31;
	shr.u64 	%rd75, %rd25, 52;
	cvt.u32.u64 	%r77, %rd75;
	add.s32 	%r22, %r77, 8;
	shr.u64 	%rd76, %rd25, 49;
	cvt.u32.u64 	%r23, %rd76;
	and.b32  	%r24, %r23, 7;
	and.b64  	%rd26, %rd25, 9223372036854775807;
	setp.lt.u64 	%p32, %rd26, 4562146422526312449;
	mov.u32 	%r122, 0;
	@%p32 bra 	$L__BB8_30;

	setp.gt.u64 	%p33, %rd26, 9218868437227405312;
	mov.u32 	%r122, 127;
	@%p33 bra 	$L__BB8_30;

	setp.gt.u64 	%p34, %rd26, 4646870390516219904;
	mov.u32 	%r122, 126;
	@%p34 bra 	$L__BB8_30;

	setp.lt.u64 	%p35, %rd26, 4580160821035794432;
	@%p35 bra 	$L__BB8_27;
	bra.uni 	$L__BB8_26;

$L__BB8_27:
	cvt.u16.u32 	%rs6, %r22;
	mov.u16 	%rs7, 1;
	sub.s16 	%rs8, %rs7, %rs6;
	cvt.u32.u16 	%r86, %rs8;
	and.b32  	%r87, %r86, 255;
	or.b32  	%r88, %r24, 8;
	shr.u32 	%r122, %r88, %r87;
	mov.u64 	%rd78, 562949953421312;
	shl.b64 	%rd79, %rd78, %r87;
	add.s64 	%rd80, %rd79, -1;
	or.b64  	%rd81, %rd25, 4503599627370496;
	and.b64  	%rd27, %rd80, %rd81;
	mov.u64 	%rd82, 281474976710656;
	shl.b64 	%rd28, %rd82, %r87;
	setp.gt.u64 	%p41, %rd27, %rd28;
	@%p41 bra 	$L__BB8_29;

	setp.ne.s64 	%p42, %rd27, %rd28;
	and.b32  	%r89, %r122, 1;
	setp.eq.b32 	%p43, %r89, 1;
	not.pred 	%p44, %p43;
	or.pred  	%p45, %p44, %p42;
	@%p45 bra 	$L__BB8_30;

$L__BB8_29:
	and.b32  	%r90, %r122, 255;
	add.s32 	%r122, %r90, 1;
	bra.uni 	$L__BB8_30;

$L__BB8_26:
	shl.b32 	%r80, %r22, 3;
	and.b32  	%r81, %r80, 2040;
	or.b32  	%r82, %r81, %r24;
	and.b64  	%rd77, %rd25, 562949953421311;
	setp.gt.u64 	%p36, %rd77, 281474976710656;
	setp.eq.s64 	%p37, %rd77, 281474976710656;
	and.b32  	%r83, %r23, 1;
	setp.eq.b32 	%p38, %r83, 1;
	and.pred  	%p39, %p37, %p38;
	or.pred  	%p40, %p36, %p39;
	and.b32  	%r84, %r82, 255;
	add.s32 	%r85, %r84, 1;
	selp.b32 	%r122, %r85, %r82, %p40;

$L__BB8_30:
	shr.u64 	%rd83, %rd25, 63;
	cvt.u32.u64 	%r91, %rd83;
	shl.b32 	%r92, %r91, 7;
	or.b32  	%r93, %r122, %r92;
	add.s64 	%rd84, %rd1, %rd99;
	st.global.u8 	[%rd84], %r93;
	add.s32 	%r116, %r116, %r5;
	cvt.u64.u32 	%rd99, %r116;
	setp.lt.u64 	%p46, %rd99, %rd36;
	@%p46 bra 	$L__BB8_22;

$L__BB8_42:
	ret;

}
	// .globl	cast_u32_u32
.visible .entry cast_u32_u32(
	.param .u64 cast_u32_u32_param_0,
	.param .u64 cast_u32_u32_param_1,
	.param .u64 cast_u32_u32_param_2,
	.param .u64 cast_u32_u32_param_3,
	.param .u64 cast_u32_u32_param_4
)
{
	.reg .pred 	%p<20>;
	.reg .b32 	%r<44>;
	.reg .b64 	%rd<64>;


	ld.param.u64 	%rd24, [cast_u32_u32_param_0];
	ld.param.u64 	%rd25, [cast_u32_u32_param_1];
	ld.param.u64 	%rd26, [cast_u32_u32_param_2];
	ld.param.u64 	%rd27, [cast_u32_u32_param_3];
	ld.param.u64 	%rd28, [cast_u32_u32_param_4];
	cvta.to.global.u64 	%rd1, %rd28;
	cvta.to.global.u64 	%rd2, %rd27;
	cvta.to.global.u64 	%rd3, %rd26;
	setp.eq.s64 	%p3, %rd25, 0;
	setp.eq.s64 	%p4, %rd26, 0;
	or.pred  	%p5, %p4, %p3;
	mov.pred 	%p2, -1;
	mov.pred 	%p19, %p2;
	@%p5 bra 	$L__BB9_5;

	mov.u64 	%rd58, 1;
	mov.u32 	%r37, 0;

$L__BB9_2:
	not.b32 	%r20, %r37;
	cvt.u64.u32 	%rd30, %r20;
	add.s64 	%rd31, %rd30, %rd25;
	shl.b64 	%rd32, %rd31, 3;
	and.b64  	%rd33, %rd32, 34359738360;
	add.s64 	%rd5, %rd3, %rd33;
	ld.global.u64 	%rd6, [%rd5];
	setp.lt.u64 	%p6, %rd6, 2;
	@%p6 bra 	$L__BB9_4;

	shl.b64 	%rd34, %rd25, 3;
	add.s64 	%rd35, %rd5, %rd34;
	ld.global.u64 	%rd36, [%rd35];
	setp.ne.s64 	%p8, %rd58, %rd36;
	mov.pred 	%p19, 0;
	@%p8 bra 	$L__BB9_5;

$L__BB9_4:
	mul.lo.s64 	%rd58, %rd6, %rd58;
	add.s32 	%r37, %r37, 1;
	cvt.u64.u32 	%rd37, %r37;
	setp.lt.u64 	%p10, %rd37, %rd25;
	mov.pred 	%p19, %p2;
	@%p10 bra 	$L__BB9_2;

$L__BB9_5:
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r21, %ctaid.x;
	mov.u32 	%r22, %tid.x;
	mad.lo.s32 	%r38, %r21, %r3, %r22;
	cvt.u64.u32 	%rd59, %r38;
	@%p19 bra 	$L__BB9_15;
	bra.uni 	$L__BB9_6;

$L__BB9_15:
	setp.ge.u64 	%p17, %rd59, %rd24;
	@%p17 bra 	$L__BB9_18;

	mov.u32 	%r35, %nctaid.x;
	mul.lo.s32 	%r16, %r3, %r35;

$L__BB9_17:
	shl.b64 	%rd55, %rd59, 2;
	add.s64 	%rd56, %rd2, %rd55;
	ld.global.u32 	%r36, [%rd56];
	add.s64 	%rd57, %rd1, %rd55;
	st.global.u32 	[%rd57], %r36;
	add.s32 	%r38, %r38, %r16;
	cvt.u64.u32 	%rd59, %r38;
	setp.lt.u64 	%p18, %rd59, %rd24;
	@%p18 bra 	$L__BB9_17;
	bra.uni 	$L__BB9_18;

$L__BB9_6:
	setp.ge.u64 	%p11, %rd59, %rd24;
	@%p11 bra 	$L__BB9_18;

	mov.u32 	%r23, %nctaid.x;
	mul.lo.s32 	%r5, %r3, %r23;
	@%p3 bra 	$L__BB9_14;

$L__BB9_8:
	mov.u32 	%r39, 0;
	mov.u32 	%r40, %r38;
	mov.u32 	%r41, %r39;

$L__BB9_9:
	not.b32 	%r26, %r39;
	cvt.u64.u32 	%rd38, %r26;
	add.s64 	%rd39, %rd38, %rd25;
	cvt.u64.u32 	%rd10, %r40;
	shl.b64 	%rd40, %rd39, 3;
	and.b64  	%rd41, %rd40, 34359738360;
	add.s64 	%rd11, %rd3, %rd41;
	ld.global.u64 	%rd12, [%rd11];
	and.b64  	%rd42, %rd12, -4294967296;
	setp.eq.s64 	%p13, %rd42, 0;
	@%p13 bra 	$L__BB9_11;

	div.u64 	%rd60, %rd10, %rd12;
	mul.lo.s64 	%rd43, %rd60, %rd12;
	sub.s64 	%rd61, %rd10, %rd43;
	bra.uni 	$L__BB9_12;

$L__BB9_11:
	cvt.u32.u64 	%r27, %rd12;
	cvt.u32.u64 	%r28, %rd10;
	div.u32 	%r29, %r28, %r27;
	mul.lo.s32 	%r30, %r29, %r27;
	sub.s32 	%r31, %r28, %r30;
	cvt.u64.u32 	%rd60, %r29;
	cvt.u64.u32 	%rd61, %r31;

$L__BB9_12:
	shl.b64 	%rd44, %rd25, 3;
	add.s64 	%rd45, %rd11, %rd44;
	ld.global.u64 	%rd46, [%rd45];
	mul.lo.s64 	%rd47, %rd46, %rd61;
	cvt.u32.u64 	%r32, %rd47;
	add.s32 	%r41, %r41, %r32;
	cvt.u32.u64 	%r40, %rd60;
	add.s32 	%r39, %r39, 1;
	cvt.u64.u32 	%rd48, %r39;
	setp.lt.u64 	%p14, %rd48, %rd25;
	@%p14 bra 	$L__BB9_9;

	mul.wide.u32 	%rd49, %r41, 4;
	add.s64 	%rd50, %rd2, %rd49;
	ld.global.u32 	%r33, [%rd50];
	shl.b64 	%rd51, %rd59, 2;
	add.s64 	%rd52, %rd1, %rd51;
	st.global.u32 	[%rd52], %r33;
	add.s32 	%r38, %r38, %r5;
	cvt.u64.u32 	%rd59, %r38;
	setp.lt.u64 	%p15, %rd59, %rd24;
	@%p15 bra 	$L__BB9_8;
	bra.uni 	$L__BB9_18;

$L__BB9_14:
	ld.global.u32 	%r34, [%rd2];
	shl.b64 	%rd53, %rd59, 2;
	add.s64 	%rd54, %rd1, %rd53;
	st.global.u32 	[%rd54], %r34;
	add.s32 	%r38, %r38, %r5;
	cvt.u64.u32 	%rd59, %r38;
	setp.lt.u64 	%p16, %rd59, %rd24;
	@%p16 bra 	$L__BB9_14;

$L__BB9_18:
	ret;

}
	// .globl	cast_u32_u8
.visible .entry cast_u32_u8(
	.param .u64 cast_u32_u8_param_0,
	.param .u64 cast_u32_u8_param_1,
	.param .u64 cast_u32_u8_param_2,
	.param .u64 cast_u32_u8_param_3,
	.param .u64 cast_u32_u8_param_4
)
{
	.reg .pred 	%p<20>;
	.reg .b32 	%r<44>;
	.reg .b64 	%rd<62>;


	ld.param.u64 	%rd24, [cast_u32_u8_param_0];
	ld.param.u64 	%rd25, [cast_u32_u8_param_1];
	ld.param.u64 	%rd26, [cast_u32_u8_param_2];
	ld.param.u64 	%rd27, [cast_u32_u8_param_3];
	ld.param.u64 	%rd28, [cast_u32_u8_param_4];
	cvta.to.global.u64 	%rd1, %rd28;
	cvta.to.global.u64 	%rd2, %rd27;
	cvta.to.global.u64 	%rd3, %rd26;
	setp.eq.s64 	%p3, %rd25, 0;
	setp.eq.s64 	%p4, %rd26, 0;
	or.pred  	%p5, %p4, %p3;
	mov.pred 	%p2, -1;
	mov.pred 	%p19, %p2;
	@%p5 bra 	$L__BB10_5;

	mov.u64 	%rd56, 1;
	mov.u32 	%r37, 0;

$L__BB10_2:
	not.b32 	%r20, %r37;
	cvt.u64.u32 	%rd30, %r20;
	add.s64 	%rd31, %rd30, %rd25;
	shl.b64 	%rd32, %rd31, 3;
	and.b64  	%rd33, %rd32, 34359738360;
	add.s64 	%rd5, %rd3, %rd33;
	ld.global.u64 	%rd6, [%rd5];
	setp.lt.u64 	%p6, %rd6, 2;
	@%p6 bra 	$L__BB10_4;

	shl.b64 	%rd34, %rd25, 3;
	add.s64 	%rd35, %rd5, %rd34;
	ld.global.u64 	%rd36, [%rd35];
	setp.ne.s64 	%p8, %rd56, %rd36;
	mov.pred 	%p19, 0;
	@%p8 bra 	$L__BB10_5;

$L__BB10_4:
	mul.lo.s64 	%rd56, %rd6, %rd56;
	add.s32 	%r37, %r37, 1;
	cvt.u64.u32 	%rd37, %r37;
	setp.lt.u64 	%p10, %rd37, %rd25;
	mov.pred 	%p19, %p2;
	@%p10 bra 	$L__BB10_2;

$L__BB10_5:
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r21, %ctaid.x;
	mov.u32 	%r22, %tid.x;
	mad.lo.s32 	%r38, %r21, %r3, %r22;
	cvt.u64.u32 	%rd57, %r38;
	@%p19 bra 	$L__BB10_15;
	bra.uni 	$L__BB10_6;

$L__BB10_15:
	setp.ge.u64 	%p17, %rd57, %rd24;
	@%p17 bra 	$L__BB10_18;

	mov.u32 	%r35, %nctaid.x;
	mul.lo.s32 	%r16, %r3, %r35;

$L__BB10_17:
	shl.b64 	%rd53, %rd57, 2;
	add.s64 	%rd54, %rd2, %rd53;
	ld.global.u32 	%r36, [%rd54];
	add.s64 	%rd55, %rd1, %rd57;
	st.global.u8 	[%rd55], %r36;
	add.s32 	%r38, %r38, %r16;
	cvt.u64.u32 	%rd57, %r38;
	setp.lt.u64 	%p18, %rd57, %rd24;
	@%p18 bra 	$L__BB10_17;
	bra.uni 	$L__BB10_18;

$L__BB10_6:
	setp.ge.u64 	%p11, %rd57, %rd24;
	@%p11 bra 	$L__BB10_18;

	mov.u32 	%r23, %nctaid.x;
	mul.lo.s32 	%r5, %r3, %r23;
	@%p3 bra 	$L__BB10_14;

$L__BB10_8:
	mov.u32 	%r39, 0;
	mov.u32 	%r40, %r38;
	mov.u32 	%r41, %r39;

$L__BB10_9:
	not.b32 	%r26, %r39;
	cvt.u64.u32 	%rd38, %r26;
	add.s64 	%rd39, %rd38, %rd25;
	cvt.u64.u32 	%rd10, %r40;
	shl.b64 	%rd40, %rd39, 3;
	and.b64  	%rd41, %rd40, 34359738360;
	add.s64 	%rd11, %rd3, %rd41;
	ld.global.u64 	%rd12, [%rd11];
	and.b64  	%rd42, %rd12, -4294967296;
	setp.eq.s64 	%p13, %rd42, 0;
	@%p13 bra 	$L__BB10_11;

	div.u64 	%rd58, %rd10, %rd12;
	mul.lo.s64 	%rd43, %rd58, %rd12;
	sub.s64 	%rd59, %rd10, %rd43;
	bra.uni 	$L__BB10_12;

$L__BB10_11:
	cvt.u32.u64 	%r27, %rd12;
	cvt.u32.u64 	%r28, %rd10;
	div.u32 	%r29, %r28, %r27;
	mul.lo.s32 	%r30, %r29, %r27;
	sub.s32 	%r31, %r28, %r30;
	cvt.u64.u32 	%rd58, %r29;
	cvt.u64.u32 	%rd59, %r31;

$L__BB10_12:
	shl.b64 	%rd44, %rd25, 3;
	add.s64 	%rd45, %rd11, %rd44;
	ld.global.u64 	%rd46, [%rd45];
	mul.lo.s64 	%rd47, %rd46, %rd59;
	cvt.u32.u64 	%r32, %rd47;
	add.s32 	%r41, %r41, %r32;
	cvt.u32.u64 	%r40, %rd58;
	add.s32 	%r39, %r39, 1;
	cvt.u64.u32 	%rd48, %r39;
	setp.lt.u64 	%p14, %rd48, %rd25;
	@%p14 bra 	$L__BB10_9;

	mul.wide.u32 	%rd49, %r41, 4;
	add.s64 	%rd50, %rd2, %rd49;
	ld.global.u32 	%r33, [%rd50];
	add.s64 	%rd51, %rd1, %rd57;
	st.global.u8 	[%rd51], %r33;
	add.s32 	%r38, %r38, %r5;
	cvt.u64.u32 	%rd57, %r38;
	setp.lt.u64 	%p15, %rd57, %rd24;
	@%p15 bra 	$L__BB10_8;
	bra.uni 	$L__BB10_18;

$L__BB10_14:
	ld.global.u32 	%r34, [%rd2];
	add.s64 	%rd52, %rd1, %rd57;
	st.global.u8 	[%rd52], %r34;
	add.s32 	%r38, %r38, %r5;
	cvt.u64.u32 	%rd57, %r38;
	setp.lt.u64 	%p16, %rd57, %rd24;
	@%p16 bra 	$L__BB10_14;

$L__BB10_18:
	ret;

}
	// .globl	cast_u32_i64
.visible .entry cast_u32_i64(
	.param .u64 cast_u32_i64_param_0,
	.param .u64 cast_u32_i64_param_1,
	.param .u64 cast_u32_i64_param_2,
	.param .u64 cast_u32_i64_param_3,
	.param .u64 cast_u32_i64_param_4
)
{
	.reg .pred 	%p<20>;
	.reg .b32 	%r<41>;
	.reg .b64 	%rd<68>;


	ld.param.u64 	%rd24, [cast_u32_i64_param_0];
	ld.param.u64 	%rd25, [cast_u32_i64_param_1];
	ld.param.u64 	%rd26, [cast_u32_i64_param_2];
	ld.param.u64 	%rd27, [cast_u32_i64_param_3];
	ld.param.u64 	%rd28, [cast_u32_i64_param_4];
	cvta.to.global.u64 	%rd1, %rd28;
	cvta.to.global.u64 	%rd2, %rd27;
	cvta.to.global.u64 	%rd3, %rd26;
	setp.eq.s64 	%p3, %rd25, 0;
	setp.eq.s64 	%p4, %rd26, 0;
	or.pred  	%p5, %p4, %p3;
	mov.pred 	%p2, -1;
	mov.pred 	%p19, %p2;
	@%p5 bra 	$L__BB11_5;

	mov.u64 	%rd62, 1;
	mov.u32 	%r34, 0;

$L__BB11_2:
	not.b32 	%r20, %r34;
	cvt.u64.u32 	%rd30, %r20;
	add.s64 	%rd31, %rd30, %rd25;
	shl.b64 	%rd32, %rd31, 3;
	and.b64  	%rd33, %rd32, 34359738360;
	add.s64 	%rd5, %rd3, %rd33;
	ld.global.u64 	%rd6, [%rd5];
	setp.lt.u64 	%p6, %rd6, 2;
	@%p6 bra 	$L__BB11_4;

	shl.b64 	%rd34, %rd25, 3;
	add.s64 	%rd35, %rd5, %rd34;
	ld.global.u64 	%rd36, [%rd35];
	setp.ne.s64 	%p8, %rd62, %rd36;
	mov.pred 	%p19, 0;
	@%p8 bra 	$L__BB11_5;

$L__BB11_4:
	mul.lo.s64 	%rd62, %rd6, %rd62;
	add.s32 	%r34, %r34, 1;
	cvt.u64.u32 	%rd37, %r34;
	setp.lt.u64 	%p10, %rd37, %rd25;
	mov.pred 	%p19, %p2;
	@%p10 bra 	$L__BB11_2;

$L__BB11_5:
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r21, %ctaid.x;
	mov.u32 	%r22, %tid.x;
	mad.lo.s32 	%r35, %r21, %r3, %r22;
	cvt.u64.u32 	%rd63, %r35;
	@%p19 bra 	$L__BB11_15;
	bra.uni 	$L__BB11_6;

$L__BB11_15:
	setp.ge.u64 	%p17, %rd63, %rd24;
	@%p17 bra 	$L__BB11_18;

	mov.u32 	%r33, %nctaid.x;
	mul.lo.s32 	%r16, %r3, %r33;

$L__BB11_17:
	shl.b64 	%rd57, %rd63, 2;
	add.s64 	%rd58, %rd2, %rd57;
	ld.global.u32 	%rd59, [%rd58];
	shl.b64 	%rd60, %rd63, 3;
	add.s64 	%rd61, %rd1, %rd60;
	st.global.u64 	[%rd61], %rd59;
	add.s32 	%r35, %r35, %r16;
	cvt.u64.u32 	%rd63, %r35;
	setp.lt.u64 	%p18, %rd63, %rd24;
	@%p18 bra 	$L__BB11_17;
	bra.uni 	$L__BB11_18;

$L__BB11_6:
	setp.ge.u64 	%p11, %rd63, %rd24;
	@%p11 bra 	$L__BB11_18;

	mov.u32 	%r23, %nctaid.x;
	mul.lo.s32 	%r5, %r3, %r23;
	@%p3 bra 	$L__BB11_14;

$L__BB11_8:
	mov.u32 	%r36, 0;
	mov.u32 	%r37, %r35;
	mov.u32 	%r38, %r36;

$L__BB11_9:
	not.b32 	%r26, %r36;
	cvt.u64.u32 	%rd38, %r26;
	add.s64 	%rd39, %rd38, %rd25;
	cvt.u64.u32 	%rd10, %r37;
	shl.b64 	%rd40, %rd39, 3;
	and.b64  	%rd41, %rd40, 34359738360;
	add.s64 	%rd11, %rd3, %rd41;
	ld.global.u64 	%rd12, [%rd11];
	and.b64  	%rd42, %rd12, -4294967296;
	setp.eq.s64 	%p13, %rd42, 0;
	@%p13 bra 	$L__BB11_11;

	div.u64 	%rd64, %rd10, %rd12;
	mul.lo.s64 	%rd43, %rd64, %rd12;
	sub.s64 	%rd65, %rd10, %rd43;
	bra.uni 	$L__BB11_12;

$L__BB11_11:
	cvt.u32.u64 	%r27, %rd12;
	cvt.u32.u64 	%r28, %rd10;
	div.u32 	%r29, %r28, %r27;
	mul.lo.s32 	%r30, %r29, %r27;
	sub.s32 	%r31, %r28, %r30;
	cvt.u64.u32 	%rd64, %r29;
	cvt.u64.u32 	%rd65, %r31;

$L__BB11_12:
	shl.b64 	%rd44, %rd25, 3;
	add.s64 	%rd45, %rd11, %rd44;
	ld.global.u64 	%rd46, [%rd45];
	mul.lo.s64 	%rd47, %rd46, %rd65;
	cvt.u32.u64 	%r32, %rd47;
	add.s32 	%r38, %r38, %r32;
	cvt.u32.u64 	%r37, %rd64;
	add.s32 	%r36, %r36, 1;
	cvt.u64.u32 	%rd48, %r36;
	setp.lt.u64 	%p14, %rd48, %rd25;
	@%p14 bra 	$L__BB11_9;

	mul.wide.u32 	%rd49, %r38, 4;
	add.s64 	%rd50, %rd2, %rd49;
	ld.global.u32 	%rd51, [%rd50];
	shl.b64 	%rd52, %rd63, 3;
	add.s64 	%rd53, %rd1, %rd52;
	st.global.u64 	[%rd53], %rd51;
	add.s32 	%r35, %r35, %r5;
	cvt.u64.u32 	%rd63, %r35;
	setp.lt.u64 	%p15, %rd63, %rd24;
	@%p15 bra 	$L__BB11_8;
	bra.uni 	$L__BB11_18;

$L__BB11_14:
	ld.global.u32 	%rd54, [%rd2];
	shl.b64 	%rd55, %rd63, 3;
	add.s64 	%rd56, %rd1, %rd55;
	st.global.u64 	[%rd56], %rd54;
	add.s32 	%r35, %r35, %r5;
	cvt.u64.u32 	%rd63, %r35;
	setp.lt.u64 	%p16, %rd63, %rd24;
	@%p16 bra 	$L__BB11_14;

$L__BB11_18:
	ret;

}
	// .globl	cast_u32_f32
.visible .entry cast_u32_f32(
	.param .u64 cast_u32_f32_param_0,
	.param .u64 cast_u32_f32_param_1,
	.param .u64 cast_u32_f32_param_2,
	.param .u64 cast_u32_f32_param_3,
	.param .u64 cast_u32_f32_param_4
)
{
	.reg .pred 	%p<20>;
	.reg .f32 	%f<4>;
	.reg .b32 	%r<44>;
	.reg .b64 	%rd<64>;


	ld.param.u64 	%rd24, [cast_u32_f32_param_0];
	ld.param.u64 	%rd25, [cast_u32_f32_param_1];
	ld.param.u64 	%rd26, [cast_u32_f32_param_2];
	ld.param.u64 	%rd27, [cast_u32_f32_param_3];
	ld.param.u64 	%rd28, [cast_u32_f32_param_4];
	cvta.to.global.u64 	%rd1, %rd28;
	cvta.to.global.u64 	%rd2, %rd27;
	cvta.to.global.u64 	%rd3, %rd26;
	setp.eq.s64 	%p3, %rd25, 0;
	setp.eq.s64 	%p4, %rd26, 0;
	or.pred  	%p5, %p4, %p3;
	mov.pred 	%p2, -1;
	mov.pred 	%p19, %p2;
	@%p5 bra 	$L__BB12_5;

	mov.u64 	%rd58, 1;
	mov.u32 	%r37, 0;

$L__BB12_2:
	not.b32 	%r20, %r37;
	cvt.u64.u32 	%rd30, %r20;
	add.s64 	%rd31, %rd30, %rd25;
	shl.b64 	%rd32, %rd31, 3;
	and.b64  	%rd33, %rd32, 34359738360;
	add.s64 	%rd5, %rd3, %rd33;
	ld.global.u64 	%rd6, [%rd5];
	setp.lt.u64 	%p6, %rd6, 2;
	@%p6 bra 	$L__BB12_4;

	shl.b64 	%rd34, %rd25, 3;
	add.s64 	%rd35, %rd5, %rd34;
	ld.global.u64 	%rd36, [%rd35];
	setp.ne.s64 	%p8, %rd58, %rd36;
	mov.pred 	%p19, 0;
	@%p8 bra 	$L__BB12_5;

$L__BB12_4:
	mul.lo.s64 	%rd58, %rd6, %rd58;
	add.s32 	%r37, %r37, 1;
	cvt.u64.u32 	%rd37, %r37;
	setp.lt.u64 	%p10, %rd37, %rd25;
	mov.pred 	%p19, %p2;
	@%p10 bra 	$L__BB12_2;

$L__BB12_5:
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r21, %ctaid.x;
	mov.u32 	%r22, %tid.x;
	mad.lo.s32 	%r38, %r21, %r3, %r22;
	cvt.u64.u32 	%rd59, %r38;
	@%p19 bra 	$L__BB12_15;
	bra.uni 	$L__BB12_6;

$L__BB12_15:
	setp.ge.u64 	%p17, %rd59, %rd24;
	@%p17 bra 	$L__BB12_18;

	mov.u32 	%r35, %nctaid.x;
	mul.lo.s32 	%r16, %r3, %r35;

$L__BB12_17:
	shl.b64 	%rd55, %rd59, 2;
	add.s64 	%rd56, %rd2, %rd55;
	ld.global.u32 	%r36, [%rd56];
	cvt.rn.f32.u32 	%f3, %r36;
	add.s64 	%rd57, %rd1, %rd55;
	st.global.f32 	[%rd57], %f3;
	add.s32 	%r38, %r38, %r16;
	cvt.u64.u32 	%rd59, %r38;
	setp.lt.u64 	%p18, %rd59, %rd24;
	@%p18 bra 	$L__BB12_17;
	bra.uni 	$L__BB12_18;

$L__BB12_6:
	setp.ge.u64 	%p11, %rd59, %rd24;
	@%p11 bra 	$L__BB12_18;

	mov.u32 	%r23, %nctaid.x;
	mul.lo.s32 	%r5, %r3, %r23;
	@%p3 bra 	$L__BB12_14;

$L__BB12_8:
	mov.u32 	%r39, 0;
	mov.u32 	%r40, %r38;
	mov.u32 	%r41, %r39;

$L__BB12_9:
	not.b32 	%r26, %r39;
	cvt.u64.u32 	%rd38, %r26;
	add.s64 	%rd39, %rd38, %rd25;
	cvt.u64.u32 	%rd10, %r40;
	shl.b64 	%rd40, %rd39, 3;
	and.b64  	%rd41, %rd40, 34359738360;
	add.s64 	%rd11, %rd3, %rd41;
	ld.global.u64 	%rd12, [%rd11];
	and.b64  	%rd42, %rd12, -4294967296;
	setp.eq.s64 	%p13, %rd42, 0;
	@%p13 bra 	$L__BB12_11;

	div.u64 	%rd60, %rd10, %rd12;
	mul.lo.s64 	%rd43, %rd60, %rd12;
	sub.s64 	%rd61, %rd10, %rd43;
	bra.uni 	$L__BB12_12;

$L__BB12_11:
	cvt.u32.u64 	%r27, %rd12;
	cvt.u32.u64 	%r28, %rd10;
	div.u32 	%r29, %r28, %r27;
	mul.lo.s32 	%r30, %r29, %r27;
	sub.s32 	%r31, %r28, %r30;
	cvt.u64.u32 	%rd60, %r29;
	cvt.u64.u32 	%rd61, %r31;

$L__BB12_12:
	shl.b64 	%rd44, %rd25, 3;
	add.s64 	%rd45, %rd11, %rd44;
	ld.global.u64 	%rd46, [%rd45];
	mul.lo.s64 	%rd47, %rd46, %rd61;
	cvt.u32.u64 	%r32, %rd47;
	add.s32 	%r41, %r41, %r32;
	cvt.u32.u64 	%r40, %rd60;
	add.s32 	%r39, %r39, 1;
	cvt.u64.u32 	%rd48, %r39;
	setp.lt.u64 	%p14, %rd48, %rd25;
	@%p14 bra 	$L__BB12_9;

	mul.wide.u32 	%rd49, %r41, 4;
	add.s64 	%rd50, %rd2, %rd49;
	ld.global.u32 	%r33, [%rd50];
	cvt.rn.f32.u32 	%f1, %r33;
	shl.b64 	%rd51, %rd59, 2;
	add.s64 	%rd52, %rd1, %rd51;
	st.global.f32 	[%rd52], %f1;
	add.s32 	%r38, %r38, %r5;
	cvt.u64.u32 	%rd59, %r38;
	setp.lt.u64 	%p15, %rd59, %rd24;
	@%p15 bra 	$L__BB12_8;
	bra.uni 	$L__BB12_18;

$L__BB12_14:
	ld.global.u32 	%r34, [%rd2];
	cvt.rn.f32.u32 	%f2, %r34;
	shl.b64 	%rd53, %rd59, 2;
	add.s64 	%rd54, %rd1, %rd53;
	st.global.f32 	[%rd54], %f2;
	add.s32 	%r38, %r38, %r5;
	cvt.u64.u32 	%rd59, %r38;
	setp.lt.u64 	%p16, %rd59, %rd24;
	@%p16 bra 	$L__BB12_14;

$L__BB12_18:
	ret;

}
	// .globl	cast_u32_f64
.visible .entry cast_u32_f64(
	.param .u64 cast_u32_f64_param_0,
	.param .u64 cast_u32_f64_param_1,
	.param .u64 cast_u32_f64_param_2,
	.param .u64 cast_u32_f64_param_3,
	.param .u64 cast_u32_f64_param_4
)
{
	.reg .pred 	%p<20>;
	.reg .b32 	%r<44>;
	.reg .f64 	%fd<4>;
	.reg .b64 	%rd<65>;


	ld.param.u64 	%rd24, [cast_u32_f64_param_0];
	ld.param.u64 	%rd25, [cast_u32_f64_param_1];
	ld.param.u64 	%rd26, [cast_u32_f64_param_2];
	ld.param.u64 	%rd27, [cast_u32_f64_param_3];
	ld.param.u64 	%rd28, [cast_u32_f64_param_4];
	cvta.to.global.u64 	%rd1, %rd28;
	cvta.to.global.u64 	%rd2, %rd27;
	cvta.to.global.u64 	%rd3, %rd26;
	setp.eq.s64 	%p3, %rd25, 0;
	setp.eq.s64 	%p4, %rd26, 0;
	or.pred  	%p5, %p4, %p3;
	mov.pred 	%p2, -1;
	mov.pred 	%p19, %p2;
	@%p5 bra 	$L__BB13_5;

	mov.u64 	%rd59, 1;
	mov.u32 	%r37, 0;

$L__BB13_2:
	not.b32 	%r20, %r37;
	cvt.u64.u32 	%rd30, %r20;
	add.s64 	%rd31, %rd30, %rd25;
	shl.b64 	%rd32, %rd31, 3;
	and.b64  	%rd33, %rd32, 34359738360;
	add.s64 	%rd5, %rd3, %rd33;
	ld.global.u64 	%rd6, [%rd5];
	setp.lt.u64 	%p6, %rd6, 2;
	@%p6 bra 	$L__BB13_4;

	shl.b64 	%rd34, %rd25, 3;
	add.s64 	%rd35, %rd5, %rd34;
	ld.global.u64 	%rd36, [%rd35];
	setp.ne.s64 	%p8, %rd59, %rd36;
	mov.pred 	%p19, 0;
	@%p8 bra 	$L__BB13_5;

$L__BB13_4:
	mul.lo.s64 	%rd59, %rd6, %rd59;
	add.s32 	%r37, %r37, 1;
	cvt.u64.u32 	%rd37, %r37;
	setp.lt.u64 	%p10, %rd37, %rd25;
	mov.pred 	%p19, %p2;
	@%p10 bra 	$L__BB13_2;

$L__BB13_5:
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r21, %ctaid.x;
	mov.u32 	%r22, %tid.x;
	mad.lo.s32 	%r38, %r21, %r3, %r22;
	cvt.u64.u32 	%rd60, %r38;
	@%p19 bra 	$L__BB13_15;
	bra.uni 	$L__BB13_6;

$L__BB13_15:
	setp.ge.u64 	%p17, %rd60, %rd24;
	@%p17 bra 	$L__BB13_18;

	mov.u32 	%r35, %nctaid.x;
	mul.lo.s32 	%r16, %r3, %r35;

$L__BB13_17:
	shl.b64 	%rd55, %rd60, 2;
	add.s64 	%rd56, %rd2, %rd55;
	ld.global.u32 	%r36, [%rd56];
	cvt.rn.f64.u32 	%fd3, %r36;
	shl.b64 	%rd57, %rd60, 3;
	add.s64 	%rd58, %rd1, %rd57;
	st.global.f64 	[%rd58], %fd3;
	add.s32 	%r38, %r38, %r16;
	cvt.u64.u32 	%rd60, %r38;
	setp.lt.u64 	%p18, %rd60, %rd24;
	@%p18 bra 	$L__BB13_17;
	bra.uni 	$L__BB13_18;

$L__BB13_6:
	setp.ge.u64 	%p11, %rd60, %rd24;
	@%p11 bra 	$L__BB13_18;

	mov.u32 	%r23, %nctaid.x;
	mul.lo.s32 	%r5, %r3, %r23;
	@%p3 bra 	$L__BB13_14;

$L__BB13_8:
	mov.u32 	%r39, 0;
	mov.u32 	%r40, %r38;
	mov.u32 	%r41, %r39;

$L__BB13_9:
	not.b32 	%r26, %r39;
	cvt.u64.u32 	%rd38, %r26;
	add.s64 	%rd39, %rd38, %rd25;
	cvt.u64.u32 	%rd10, %r40;
	shl.b64 	%rd40, %rd39, 3;
	and.b64  	%rd41, %rd40, 34359738360;
	add.s64 	%rd11, %rd3, %rd41;
	ld.global.u64 	%rd12, [%rd11];
	and.b64  	%rd42, %rd12, -4294967296;
	setp.eq.s64 	%p13, %rd42, 0;
	@%p13 bra 	$L__BB13_11;

	div.u64 	%rd61, %rd10, %rd12;
	mul.lo.s64 	%rd43, %rd61, %rd12;
	sub.s64 	%rd62, %rd10, %rd43;
	bra.uni 	$L__BB13_12;

$L__BB13_11:
	cvt.u32.u64 	%r27, %rd12;
	cvt.u32.u64 	%r28, %rd10;
	div.u32 	%r29, %r28, %r27;
	mul.lo.s32 	%r30, %r29, %r27;
	sub.s32 	%r31, %r28, %r30;
	cvt.u64.u32 	%rd61, %r29;
	cvt.u64.u32 	%rd62, %r31;

$L__BB13_12:
	shl.b64 	%rd44, %rd25, 3;
	add.s64 	%rd45, %rd11, %rd44;
	ld.global.u64 	%rd46, [%rd45];
	mul.lo.s64 	%rd47, %rd46, %rd62;
	cvt.u32.u64 	%r32, %rd47;
	add.s32 	%r41, %r41, %r32;
	cvt.u32.u64 	%r40, %rd61;
	add.s32 	%r39, %r39, 1;
	cvt.u64.u32 	%rd48, %r39;
	setp.lt.u64 	%p14, %rd48, %rd25;
	@%p14 bra 	$L__BB13_9;

	mul.wide.u32 	%rd49, %r41, 4;
	add.s64 	%rd50, %rd2, %rd49;
	ld.global.u32 	%r33, [%rd50];
	cvt.rn.f64.u32 	%fd1, %r33;
	shl.b64 	%rd51, %rd60, 3;
	add.s64 	%rd52, %rd1, %rd51;
	st.global.f64 	[%rd52], %fd1;
	add.s32 	%r38, %r38, %r5;
	cvt.u64.u32 	%rd60, %r38;
	setp.lt.u64 	%p15, %rd60, %rd24;
	@%p15 bra 	$L__BB13_8;
	bra.uni 	$L__BB13_18;

$L__BB13_14:
	ld.global.u32 	%r34, [%rd2];
	cvt.rn.f64.u32 	%fd2, %r34;
	shl.b64 	%rd53, %rd60, 3;
	add.s64 	%rd54, %rd1, %rd53;
	st.global.f64 	[%rd54], %fd2;
	add.s32 	%r38, %r38, %r5;
	cvt.u64.u32 	%rd60, %r38;
	setp.lt.u64 	%p16, %rd60, %rd24;
	@%p16 bra 	$L__BB13_14;

$L__BB13_18:
	ret;

}
	// .globl	cast_u8_u32
.visible .entry cast_u8_u32(
	.param .u64 cast_u8_u32_param_0,
	.param .u64 cast_u8_u32_param_1,
	.param .u64 cast_u8_u32_param_2,
	.param .u64 cast_u8_u32_param_3,
	.param .u64 cast_u8_u32_param_4
)
{
	.reg .pred 	%p<20>;
	.reg .b32 	%r<44>;
	.reg .b64 	%rd<64>;


	ld.param.u64 	%rd24, [cast_u8_u32_param_0];
	ld.param.u64 	%rd25, [cast_u8_u32_param_1];
	ld.param.u64 	%rd26, [cast_u8_u32_param_2];
	ld.param.u64 	%rd27, [cast_u8_u32_param_3];
	ld.param.u64 	%rd28, [cast_u8_u32_param_4];
	cvta.to.global.u64 	%rd1, %rd28;
	cvta.to.global.u64 	%rd2, %rd27;
	cvta.to.global.u64 	%rd3, %rd26;
	setp.eq.s64 	%p3, %rd25, 0;
	setp.eq.s64 	%p4, %rd26, 0;
	or.pred  	%p5, %p4, %p3;
	mov.pred 	%p2, -1;
	mov.pred 	%p19, %p2;
	@%p5 bra 	$L__BB14_5;

	mov.u64 	%rd58, 1;
	mov.u32 	%r37, 0;

$L__BB14_2:
	not.b32 	%r20, %r37;
	cvt.u64.u32 	%rd30, %r20;
	add.s64 	%rd31, %rd30, %rd25;
	shl.b64 	%rd32, %rd31, 3;
	and.b64  	%rd33, %rd32, 34359738360;
	add.s64 	%rd5, %rd3, %rd33;
	ld.global.u64 	%rd6, [%rd5];
	setp.lt.u64 	%p6, %rd6, 2;
	@%p6 bra 	$L__BB14_4;

	shl.b64 	%rd34, %rd25, 3;
	add.s64 	%rd35, %rd5, %rd34;
	ld.global.u64 	%rd36, [%rd35];
	setp.ne.s64 	%p8, %rd58, %rd36;
	mov.pred 	%p19, 0;
	@%p8 bra 	$L__BB14_5;

$L__BB14_4:
	mul.lo.s64 	%rd58, %rd6, %rd58;
	add.s32 	%r37, %r37, 1;
	cvt.u64.u32 	%rd37, %r37;
	setp.lt.u64 	%p10, %rd37, %rd25;
	mov.pred 	%p19, %p2;
	@%p10 bra 	$L__BB14_2;

$L__BB14_5:
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r21, %ctaid.x;
	mov.u32 	%r22, %tid.x;
	mad.lo.s32 	%r38, %r21, %r3, %r22;
	cvt.u64.u32 	%rd59, %r38;
	@%p19 bra 	$L__BB14_15;
	bra.uni 	$L__BB14_6;

$L__BB14_15:
	setp.ge.u64 	%p17, %rd59, %rd24;
	@%p17 bra 	$L__BB14_18;

	mov.u32 	%r35, %nctaid.x;
	mul.lo.s32 	%r16, %r3, %r35;

$L__BB14_17:
	add.s64 	%rd55, %rd2, %rd59;
	ld.global.u8 	%r36, [%rd55];
	shl.b64 	%rd56, %rd59, 2;
	add.s64 	%rd57, %rd1, %rd56;
	st.global.u32 	[%rd57], %r36;
	add.s32 	%r38, %r38, %r16;
	cvt.u64.u32 	%rd59, %r38;
	setp.lt.u64 	%p18, %rd59, %rd24;
	@%p18 bra 	$L__BB14_17;
	bra.uni 	$L__BB14_18;

$L__BB14_6:
	setp.ge.u64 	%p11, %rd59, %rd24;
	@%p11 bra 	$L__BB14_18;

	mov.u32 	%r23, %nctaid.x;
	mul.lo.s32 	%r5, %r3, %r23;
	@%p3 bra 	$L__BB14_14;

$L__BB14_8:
	mov.u32 	%r39, 0;
	mov.u32 	%r40, %r38;
	mov.u32 	%r41, %r39;

$L__BB14_9:
	not.b32 	%r26, %r39;
	cvt.u64.u32 	%rd38, %r26;
	add.s64 	%rd39, %rd38, %rd25;
	cvt.u64.u32 	%rd10, %r40;
	shl.b64 	%rd40, %rd39, 3;
	and.b64  	%rd41, %rd40, 34359738360;
	add.s64 	%rd11, %rd3, %rd41;
	ld.global.u64 	%rd12, [%rd11];
	and.b64  	%rd42, %rd12, -4294967296;
	setp.eq.s64 	%p13, %rd42, 0;
	@%p13 bra 	$L__BB14_11;

	div.u64 	%rd60, %rd10, %rd12;
	mul.lo.s64 	%rd43, %rd60, %rd12;
	sub.s64 	%rd61, %rd10, %rd43;
	bra.uni 	$L__BB14_12;

$L__BB14_11:
	cvt.u32.u64 	%r27, %rd12;
	cvt.u32.u64 	%r28, %rd10;
	div.u32 	%r29, %r28, %r27;
	mul.lo.s32 	%r30, %r29, %r27;
	sub.s32 	%r31, %r28, %r30;
	cvt.u64.u32 	%rd60, %r29;
	cvt.u64.u32 	%rd61, %r31;

$L__BB14_12:
	shl.b64 	%rd44, %rd25, 3;
	add.s64 	%rd45, %rd11, %rd44;
	ld.global.u64 	%rd46, [%rd45];
	mul.lo.s64 	%rd47, %rd46, %rd61;
	cvt.u32.u64 	%r32, %rd47;
	add.s32 	%r41, %r41, %r32;
	cvt.u32.u64 	%r40, %rd60;
	add.s32 	%r39, %r39, 1;
	cvt.u64.u32 	%rd48, %r39;
	setp.lt.u64 	%p14, %rd48, %rd25;
	@%p14 bra 	$L__BB14_9;

	cvt.u64.u32 	%rd49, %r41;
	add.s64 	%rd50, %rd2, %rd49;
	ld.global.u8 	%r33, [%rd50];
	shl.b64 	%rd51, %rd59, 2;
	add.s64 	%rd52, %rd1, %rd51;
	st.global.u32 	[%rd52], %r33;
	add.s32 	%r38, %r38, %r5;
	cvt.u64.u32 	%rd59, %r38;
	setp.lt.u64 	%p15, %rd59, %rd24;
	@%p15 bra 	$L__BB14_8;
	bra.uni 	$L__BB14_18;

$L__BB14_14:
	ld.global.u8 	%r34, [%rd2];
	shl.b64 	%rd53, %rd59, 2;
	add.s64 	%rd54, %rd1, %rd53;
	st.global.u32 	[%rd54], %r34;
	add.s32 	%r38, %r38, %r5;
	cvt.u64.u32 	%rd59, %r38;
	setp.lt.u64 	%p16, %rd59, %rd24;
	@%p16 bra 	$L__BB14_14;

$L__BB14_18:
	ret;

}
	// .globl	cast_u8_u8
.visible .entry cast_u8_u8(
	.param .u64 cast_u8_u8_param_0,
	.param .u64 cast_u8_u8_param_1,
	.param .u64 cast_u8_u8_param_2,
	.param .u64 cast_u8_u8_param_3,
	.param .u64 cast_u8_u8_param_4
)
{
	.reg .pred 	%p<20>;
	.reg .b16 	%rs<4>;
	.reg .b32 	%r<41>;
	.reg .b64 	%rd<61>;


	ld.param.u64 	%rd24, [cast_u8_u8_param_0];
	ld.param.u64 	%rd25, [cast_u8_u8_param_1];
	ld.param.u64 	%rd26, [cast_u8_u8_param_2];
	ld.param.u64 	%rd27, [cast_u8_u8_param_3];
	ld.param.u64 	%rd28, [cast_u8_u8_param_4];
	cvta.to.global.u64 	%rd1, %rd28;
	cvta.to.global.u64 	%rd2, %rd27;
	cvta.to.global.u64 	%rd3, %rd26;
	setp.eq.s64 	%p3, %rd25, 0;
	setp.eq.s64 	%p4, %rd26, 0;
	or.pred  	%p5, %p4, %p3;
	mov.pred 	%p2, -1;
	mov.pred 	%p19, %p2;
	@%p5 bra 	$L__BB15_5;

	mov.u64 	%rd55, 1;
	mov.u32 	%r34, 0;

$L__BB15_2:
	not.b32 	%r20, %r34;
	cvt.u64.u32 	%rd30, %r20;
	add.s64 	%rd31, %rd30, %rd25;
	shl.b64 	%rd32, %rd31, 3;
	and.b64  	%rd33, %rd32, 34359738360;
	add.s64 	%rd5, %rd3, %rd33;
	ld.global.u64 	%rd6, [%rd5];
	setp.lt.u64 	%p6, %rd6, 2;
	@%p6 bra 	$L__BB15_4;

	shl.b64 	%rd34, %rd25, 3;
	add.s64 	%rd35, %rd5, %rd34;
	ld.global.u64 	%rd36, [%rd35];
	setp.ne.s64 	%p8, %rd55, %rd36;
	mov.pred 	%p19, 0;
	@%p8 bra 	$L__BB15_5;

$L__BB15_4:
	mul.lo.s64 	%rd55, %rd6, %rd55;
	add.s32 	%r34, %r34, 1;
	cvt.u64.u32 	%rd37, %r34;
	setp.lt.u64 	%p10, %rd37, %rd25;
	mov.pred 	%p19, %p2;
	@%p10 bra 	$L__BB15_2;

$L__BB15_5:
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r21, %ctaid.x;
	mov.u32 	%r22, %tid.x;
	mad.lo.s32 	%r35, %r21, %r3, %r22;
	cvt.u64.u32 	%rd56, %r35;
	@%p19 bra 	$L__BB15_15;
	bra.uni 	$L__BB15_6;

$L__BB15_15:
	setp.ge.u64 	%p17, %rd56, %rd24;
	@%p17 bra 	$L__BB15_18;

	mov.u32 	%r33, %nctaid.x;
	mul.lo.s32 	%r16, %r3, %r33;

$L__BB15_17:
	add.s64 	%rd53, %rd2, %rd56;
	ld.global.u8 	%rs3, [%rd53];
	add.s64 	%rd54, %rd1, %rd56;
	st.global.u8 	[%rd54], %rs3;
	add.s32 	%r35, %r35, %r16;
	cvt.u64.u32 	%rd56, %r35;
	setp.lt.u64 	%p18, %rd56, %rd24;
	@%p18 bra 	$L__BB15_17;
	bra.uni 	$L__BB15_18;

$L__BB15_6:
	setp.ge.u64 	%p11, %rd56, %rd24;
	@%p11 bra 	$L__BB15_18;

	mov.u32 	%r23, %nctaid.x;
	mul.lo.s32 	%r5, %r3, %r23;
	@%p3 bra 	$L__BB15_14;

$L__BB15_8:
	mov.u32 	%r36, 0;
	mov.u32 	%r37, %r35;
	mov.u32 	%r38, %r36;

$L__BB15_9:
	not.b32 	%r26, %r36;
	cvt.u64.u32 	%rd38, %r26;
	add.s64 	%rd39, %rd38, %rd25;
	cvt.u64.u32 	%rd10, %r37;
	shl.b64 	%rd40, %rd39, 3;
	and.b64  	%rd41, %rd40, 34359738360;
	add.s64 	%rd11, %rd3, %rd41;
	ld.global.u64 	%rd12, [%rd11];
	and.b64  	%rd42, %rd12, -4294967296;
	setp.eq.s64 	%p13, %rd42, 0;
	@%p13 bra 	$L__BB15_11;

	div.u64 	%rd57, %rd10, %rd12;
	mul.lo.s64 	%rd43, %rd57, %rd12;
	sub.s64 	%rd58, %rd10, %rd43;
	bra.uni 	$L__BB15_12;

$L__BB15_11:
	cvt.u32.u64 	%r27, %rd12;
	cvt.u32.u64 	%r28, %rd10;
	div.u32 	%r29, %r28, %r27;
	mul.lo.s32 	%r30, %r29, %r27;
	sub.s32 	%r31, %r28, %r30;
	cvt.u64.u32 	%rd57, %r29;
	cvt.u64.u32 	%rd58, %r31;

$L__BB15_12:
	shl.b64 	%rd44, %rd25, 3;
	add.s64 	%rd45, %rd11, %rd44;
	ld.global.u64 	%rd46, [%rd45];
	mul.lo.s64 	%rd47, %rd46, %rd58;
	cvt.u32.u64 	%r32, %rd47;
	add.s32 	%r38, %r38, %r32;
	cvt.u32.u64 	%r37, %rd57;
	add.s32 	%r36, %r36, 1;
	cvt.u64.u32 	%rd48, %r36;
	setp.lt.u64 	%p14, %rd48, %rd25;
	@%p14 bra 	$L__BB15_9;

	cvt.u64.u32 	%rd49, %r38;
	add.s64 	%rd50, %rd2, %rd49;
	ld.global.u8 	%rs1, [%rd50];
	add.s64 	%rd51, %rd1, %rd56;
	st.global.u8 	[%rd51], %rs1;
	add.s32 	%r35, %r35, %r5;
	cvt.u64.u32 	%rd56, %r35;
	setp.lt.u64 	%p15, %rd56, %rd24;
	@%p15 bra 	$L__BB15_8;
	bra.uni 	$L__BB15_18;

$L__BB15_14:
	ld.global.u8 	%rs2, [%rd2];
	add.s64 	%rd52, %rd1, %rd56;
	st.global.u8 	[%rd52], %rs2;
	add.s32 	%r35, %r35, %r5;
	cvt.u64.u32 	%rd56, %r35;
	setp.lt.u64 	%p16, %rd56, %rd24;
	@%p16 bra 	$L__BB15_14;

$L__BB15_18:
	ret;

}
	// .globl	cast_u8_i64
.visible .entry cast_u8_i64(
	.param .u64 cast_u8_i64_param_0,
	.param .u64 cast_u8_i64_param_1,
	.param .u64 cast_u8_i64_param_2,
	.param .u64 cast_u8_i64_param_3,
	.param .u64 cast_u8_i64_param_4
)
{
	.reg .pred 	%p<20>;
	.reg .b32 	%r<41>;
	.reg .b64 	%rd<67>;


	ld.param.u64 	%rd24, [cast_u8_i64_param_0];
	ld.param.u64 	%rd25, [cast_u8_i64_param_1];
	ld.param.u64 	%rd26, [cast_u8_i64_param_2];
	ld.param.u64 	%rd27, [cast_u8_i64_param_3];
	ld.param.u64 	%rd28, [cast_u8_i64_param_4];
	cvta.to.global.u64 	%rd1, %rd28;
	cvta.to.global.u64 	%rd2, %rd27;
	cvta.to.global.u64 	%rd3, %rd26;
	setp.eq.s64 	%p3, %rd25, 0;
	setp.eq.s64 	%p4, %rd26, 0;
	or.pred  	%p5, %p4, %p3;
	mov.pred 	%p2, -1;
	mov.pred 	%p19, %p2;
	@%p5 bra 	$L__BB16_5;

	mov.u64 	%rd61, 1;
	mov.u32 	%r34, 0;

$L__BB16_2:
	not.b32 	%r20, %r34;
	cvt.u64.u32 	%rd30, %r20;
	add.s64 	%rd31, %rd30, %rd25;
	shl.b64 	%rd32, %rd31, 3;
	and.b64  	%rd33, %rd32, 34359738360;
	add.s64 	%rd5, %rd3, %rd33;
	ld.global.u64 	%rd6, [%rd5];
	setp.lt.u64 	%p6, %rd6, 2;
	@%p6 bra 	$L__BB16_4;

	shl.b64 	%rd34, %rd25, 3;
	add.s64 	%rd35, %rd5, %rd34;
	ld.global.u64 	%rd36, [%rd35];
	setp.ne.s64 	%p8, %rd61, %rd36;
	mov.pred 	%p19, 0;
	@%p8 bra 	$L__BB16_5;

$L__BB16_4:
	mul.lo.s64 	%rd61, %rd6, %rd61;
	add.s32 	%r34, %r34, 1;
	cvt.u64.u32 	%rd37, %r34;
	setp.lt.u64 	%p10, %rd37, %rd25;
	mov.pred 	%p19, %p2;
	@%p10 bra 	$L__BB16_2;

$L__BB16_5:
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r21, %ctaid.x;
	mov.u32 	%r22, %tid.x;
	mad.lo.s32 	%r35, %r21, %r3, %r22;
	cvt.u64.u32 	%rd62, %r35;
	@%p19 bra 	$L__BB16_15;
	bra.uni 	$L__BB16_6;

$L__BB16_15:
	setp.ge.u64 	%p17, %rd62, %rd24;
	@%p17 bra 	$L__BB16_18;

	mov.u32 	%r33, %nctaid.x;
	mul.lo.s32 	%r16, %r3, %r33;

$L__BB16_17:
	add.s64 	%rd57, %rd2, %rd62;
	ld.global.u8 	%rd58, [%rd57];
	shl.b64 	%rd59, %rd62, 3;
	add.s64 	%rd60, %rd1, %rd59;
	st.global.u64 	[%rd60], %rd58;
	add.s32 	%r35, %r35, %r16;
	cvt.u64.u32 	%rd62, %r35;
	setp.lt.u64 	%p18, %rd62, %rd24;
	@%p18 bra 	$L__BB16_17;
	bra.uni 	$L__BB16_18;

$L__BB16_6:
	setp.ge.u64 	%p11, %rd62, %rd24;
	@%p11 bra 	$L__BB16_18;

	mov.u32 	%r23, %nctaid.x;
	mul.lo.s32 	%r5, %r3, %r23;
	@%p3 bra 	$L__BB16_14;

$L__BB16_8:
	mov.u32 	%r36, 0;
	mov.u32 	%r37, %r35;
	mov.u32 	%r38, %r36;

$L__BB16_9:
	not.b32 	%r26, %r36;
	cvt.u64.u32 	%rd38, %r26;
	add.s64 	%rd39, %rd38, %rd25;
	cvt.u64.u32 	%rd10, %r37;
	shl.b64 	%rd40, %rd39, 3;
	and.b64  	%rd41, %rd40, 34359738360;
	add.s64 	%rd11, %rd3, %rd41;
	ld.global.u64 	%rd12, [%rd11];
	and.b64  	%rd42, %rd12, -4294967296;
	setp.eq.s64 	%p13, %rd42, 0;
	@%p13 bra 	$L__BB16_11;

	div.u64 	%rd63, %rd10, %rd12;
	mul.lo.s64 	%rd43, %rd63, %rd12;
	sub.s64 	%rd64, %rd10, %rd43;
	bra.uni 	$L__BB16_12;

$L__BB16_11:
	cvt.u32.u64 	%r27, %rd12;
	cvt.u32.u64 	%r28, %rd10;
	div.u32 	%r29, %r28, %r27;
	mul.lo.s32 	%r30, %r29, %r27;
	sub.s32 	%r31, %r28, %r30;
	cvt.u64.u32 	%rd63, %r29;
	cvt.u64.u32 	%rd64, %r31;

$L__BB16_12:
	shl.b64 	%rd44, %rd25, 3;
	add.s64 	%rd45, %rd11, %rd44;
	ld.global.u64 	%rd46, [%rd45];
	mul.lo.s64 	%rd47, %rd46, %rd64;
	cvt.u32.u64 	%r32, %rd47;
	add.s32 	%r38, %r38, %r32;
	cvt.u32.u64 	%r37, %rd63;
	add.s32 	%r36, %r36, 1;
	cvt.u64.u32 	%rd48, %r36;
	setp.lt.u64 	%p14, %rd48, %rd25;
	@%p14 bra 	$L__BB16_9;

	cvt.u64.u32 	%rd49, %r38;
	add.s64 	%rd50, %rd2, %rd49;
	ld.global.u8 	%rd51, [%rd50];
	shl.b64 	%rd52, %rd62, 3;
	add.s64 	%rd53, %rd1, %rd52;
	st.global.u64 	[%rd53], %rd51;
	add.s32 	%r35, %r35, %r5;
	cvt.u64.u32 	%rd62, %r35;
	setp.lt.u64 	%p15, %rd62, %rd24;
	@%p15 bra 	$L__BB16_8;
	bra.uni 	$L__BB16_18;

$L__BB16_14:
	ld.global.u8 	%rd54, [%rd2];
	shl.b64 	%rd55, %rd62, 3;
	add.s64 	%rd56, %rd1, %rd55;
	st.global.u64 	[%rd56], %rd54;
	add.s32 	%r35, %r35, %r5;
	cvt.u64.u32 	%rd62, %r35;
	setp.lt.u64 	%p16, %rd62, %rd24;
	@%p16 bra 	$L__BB16_14;

$L__BB16_18:
	ret;

}
	// .globl	cast_u8_f32
.visible .entry cast_u8_f32(
	.param .u64 cast_u8_f32_param_0,
	.param .u64 cast_u8_f32_param_1,
	.param .u64 cast_u8_f32_param_2,
	.param .u64 cast_u8_f32_param_3,
	.param .u64 cast_u8_f32_param_4
)
{
	.reg .pred 	%p<20>;
	.reg .b16 	%rs<4>;
	.reg .f32 	%f<4>;
	.reg .b32 	%r<41>;
	.reg .b64 	%rd<64>;


	ld.param.u64 	%rd24, [cast_u8_f32_param_0];
	ld.param.u64 	%rd25, [cast_u8_f32_param_1];
	ld.param.u64 	%rd26, [cast_u8_f32_param_2];
	ld.param.u64 	%rd27, [cast_u8_f32_param_3];
	ld.param.u64 	%rd28, [cast_u8_f32_param_4];
	cvta.to.global.u64 	%rd1, %rd28;
	cvta.to.global.u64 	%rd2, %rd27;
	cvta.to.global.u64 	%rd3, %rd26;
	setp.eq.s64 	%p3, %rd25, 0;
	setp.eq.s64 	%p4, %rd26, 0;
	or.pred  	%p5, %p4, %p3;
	mov.pred 	%p2, -1;
	mov.pred 	%p19, %p2;
	@%p5 bra 	$L__BB17_5;

	mov.u64 	%rd58, 1;
	mov.u32 	%r34, 0;

$L__BB17_2:
	not.b32 	%r20, %r34;
	cvt.u64.u32 	%rd30, %r20;
	add.s64 	%rd31, %rd30, %rd25;
	shl.b64 	%rd32, %rd31, 3;
	and.b64  	%rd33, %rd32, 34359738360;
	add.s64 	%rd5, %rd3, %rd33;
	ld.global.u64 	%rd6, [%rd5];
	setp.lt.u64 	%p6, %rd6, 2;
	@%p6 bra 	$L__BB17_4;

	shl.b64 	%rd34, %rd25, 3;
	add.s64 	%rd35, %rd5, %rd34;
	ld.global.u64 	%rd36, [%rd35];
	setp.ne.s64 	%p8, %rd58, %rd36;
	mov.pred 	%p19, 0;
	@%p8 bra 	$L__BB17_5;

$L__BB17_4:
	mul.lo.s64 	%rd58, %rd6, %rd58;
	add.s32 	%r34, %r34, 1;
	cvt.u64.u32 	%rd37, %r34;
	setp.lt.u64 	%p10, %rd37, %rd25;
	mov.pred 	%p19, %p2;
	@%p10 bra 	$L__BB17_2;

$L__BB17_5:
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r21, %ctaid.x;
	mov.u32 	%r22, %tid.x;
	mad.lo.s32 	%r35, %r21, %r3, %r22;
	cvt.u64.u32 	%rd59, %r35;
	@%p19 bra 	$L__BB17_15;
	bra.uni 	$L__BB17_6;

$L__BB17_15:
	setp.ge.u64 	%p17, %rd59, %rd24;
	@%p17 bra 	$L__BB17_18;

	mov.u32 	%r33, %nctaid.x;
	mul.lo.s32 	%r16, %r3, %r33;

$L__BB17_17:
	add.s64 	%rd55, %rd2, %rd59;
	ld.global.u8 	%rs3, [%rd55];
	cvt.rn.f32.u16 	%f3, %rs3;
	shl.b64 	%rd56, %rd59, 2;
	add.s64 	%rd57, %rd1, %rd56;
	st.global.f32 	[%rd57], %f3;
	add.s32 	%r35, %r35, %r16;
	cvt.u64.u32 	%rd59, %r35;
	setp.lt.u64 	%p18, %rd59, %rd24;
	@%p18 bra 	$L__BB17_17;
	bra.uni 	$L__BB17_18;

$L__BB17_6:
	setp.ge.u64 	%p11, %rd59, %rd24;
	@%p11 bra 	$L__BB17_18;

	mov.u32 	%r23, %nctaid.x;
	mul.lo.s32 	%r5, %r3, %r23;
	@%p3 bra 	$L__BB17_14;

$L__BB17_8:
	mov.u32 	%r36, 0;
	mov.u32 	%r37, %r35;
	mov.u32 	%r38, %r36;

$L__BB17_9:
	not.b32 	%r26, %r36;
	cvt.u64.u32 	%rd38, %r26;
	add.s64 	%rd39, %rd38, %rd25;
	cvt.u64.u32 	%rd10, %r37;
	shl.b64 	%rd40, %rd39, 3;
	and.b64  	%rd41, %rd40, 34359738360;
	add.s64 	%rd11, %rd3, %rd41;
	ld.global.u64 	%rd12, [%rd11];
	and.b64  	%rd42, %rd12, -4294967296;
	setp.eq.s64 	%p13, %rd42, 0;
	@%p13 bra 	$L__BB17_11;

	div.u64 	%rd60, %rd10, %rd12;
	mul.lo.s64 	%rd43, %rd60, %rd12;
	sub.s64 	%rd61, %rd10, %rd43;
	bra.uni 	$L__BB17_12;

$L__BB17_11:
	cvt.u32.u64 	%r27, %rd12;
	cvt.u32.u64 	%r28, %rd10;
	div.u32 	%r29, %r28, %r27;
	mul.lo.s32 	%r30, %r29, %r27;
	sub.s32 	%r31, %r28, %r30;
	cvt.u64.u32 	%rd60, %r29;
	cvt.u64.u32 	%rd61, %r31;

$L__BB17_12:
	shl.b64 	%rd44, %rd25, 3;
	add.s64 	%rd45, %rd11, %rd44;
	ld.global.u64 	%rd46, [%rd45];
	mul.lo.s64 	%rd47, %rd46, %rd61;
	cvt.u32.u64 	%r32, %rd47;
	add.s32 	%r38, %r38, %r32;
	cvt.u32.u64 	%r37, %rd60;
	add.s32 	%r36, %r36, 1;
	cvt.u64.u32 	%rd48, %r36;
	setp.lt.u64 	%p14, %rd48, %rd25;
	@%p14 bra 	$L__BB17_9;

	cvt.u64.u32 	%rd49, %r38;
	add.s64 	%rd50, %rd2, %rd49;
	ld.global.u8 	%rs1, [%rd50];
	cvt.rn.f32.u16 	%f1, %rs1;
	shl.b64 	%rd51, %rd59, 2;
	add.s64 	%rd52, %rd1, %rd51;
	st.global.f32 	[%rd52], %f1;
	add.s32 	%r35, %r35, %r5;
	cvt.u64.u32 	%rd59, %r35;
	setp.lt.u64 	%p15, %rd59, %rd24;
	@%p15 bra 	$L__BB17_8;
	bra.uni 	$L__BB17_18;

$L__BB17_14:
	ld.global.u8 	%rs2, [%rd2];
	cvt.rn.f32.u16 	%f2, %rs2;
	shl.b64 	%rd53, %rd59, 2;
	add.s64 	%rd54, %rd1, %rd53;
	st.global.f32 	[%rd54], %f2;
	add.s32 	%r35, %r35, %r5;
	cvt.u64.u32 	%rd59, %r35;
	setp.lt.u64 	%p16, %rd59, %rd24;
	@%p16 bra 	$L__BB17_14;

$L__BB17_18:
	ret;

}
	// .globl	cast_u8_f64
.visible .entry cast_u8_f64(
	.param .u64 cast_u8_f64_param_0,
	.param .u64 cast_u8_f64_param_1,
	.param .u64 cast_u8_f64_param_2,
	.param .u64 cast_u8_f64_param_3,
	.param .u64 cast_u8_f64_param_4
)
{
	.reg .pred 	%p<20>;
	.reg .b16 	%rs<4>;
	.reg .b32 	%r<41>;
	.reg .f64 	%fd<4>;
	.reg .b64 	%rd<64>;


	ld.param.u64 	%rd24, [cast_u8_f64_param_0];
	ld.param.u64 	%rd25, [cast_u8_f64_param_1];
	ld.param.u64 	%rd26, [cast_u8_f64_param_2];
	ld.param.u64 	%rd27, [cast_u8_f64_param_3];
	ld.param.u64 	%rd28, [cast_u8_f64_param_4];
	cvta.to.global.u64 	%rd1, %rd28;
	cvta.to.global.u64 	%rd2, %rd27;
	cvta.to.global.u64 	%rd3, %rd26;
	setp.eq.s64 	%p3, %rd25, 0;
	setp.eq.s64 	%p4, %rd26, 0;
	or.pred  	%p5, %p4, %p3;
	mov.pred 	%p2, -1;
	mov.pred 	%p19, %p2;
	@%p5 bra 	$L__BB18_5;

	mov.u64 	%rd58, 1;
	mov.u32 	%r34, 0;

$L__BB18_2:
	not.b32 	%r20, %r34;
	cvt.u64.u32 	%rd30, %r20;
	add.s64 	%rd31, %rd30, %rd25;
	shl.b64 	%rd32, %rd31, 3;
	and.b64  	%rd33, %rd32, 34359738360;
	add.s64 	%rd5, %rd3, %rd33;
	ld.global.u64 	%rd6, [%rd5];
	setp.lt.u64 	%p6, %rd6, 2;
	@%p6 bra 	$L__BB18_4;

	shl.b64 	%rd34, %rd25, 3;
	add.s64 	%rd35, %rd5, %rd34;
	ld.global.u64 	%rd36, [%rd35];
	setp.ne.s64 	%p8, %rd58, %rd36;
	mov.pred 	%p19, 0;
	@%p8 bra 	$L__BB18_5;

$L__BB18_4:
	mul.lo.s64 	%rd58, %rd6, %rd58;
	add.s32 	%r34, %r34, 1;
	cvt.u64.u32 	%rd37, %r34;
	setp.lt.u64 	%p10, %rd37, %rd25;
	mov.pred 	%p19, %p2;
	@%p10 bra 	$L__BB18_2;

$L__BB18_5:
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r21, %ctaid.x;
	mov.u32 	%r22, %tid.x;
	mad.lo.s32 	%r35, %r21, %r3, %r22;
	cvt.u64.u32 	%rd59, %r35;
	@%p19 bra 	$L__BB18_15;
	bra.uni 	$L__BB18_6;

$L__BB18_15:
	setp.ge.u64 	%p17, %rd59, %rd24;
	@%p17 bra 	$L__BB18_18;

	mov.u32 	%r33, %nctaid.x;
	mul.lo.s32 	%r16, %r3, %r33;

$L__BB18_17:
	add.s64 	%rd55, %rd2, %rd59;
	ld.global.u8 	%rs3, [%rd55];
	cvt.rn.f64.u16 	%fd3, %rs3;
	shl.b64 	%rd56, %rd59, 3;
	add.s64 	%rd57, %rd1, %rd56;
	st.global.f64 	[%rd57], %fd3;
	add.s32 	%r35, %r35, %r16;
	cvt.u64.u32 	%rd59, %r35;
	setp.lt.u64 	%p18, %rd59, %rd24;
	@%p18 bra 	$L__BB18_17;
	bra.uni 	$L__BB18_18;

$L__BB18_6:
	setp.ge.u64 	%p11, %rd59, %rd24;
	@%p11 bra 	$L__BB18_18;

	mov.u32 	%r23, %nctaid.x;
	mul.lo.s32 	%r5, %r3, %r23;
	@%p3 bra 	$L__BB18_14;

$L__BB18_8:
	mov.u32 	%r36, 0;
	mov.u32 	%r37, %r35;
	mov.u32 	%r38, %r36;

$L__BB18_9:
	not.b32 	%r26, %r36;
	cvt.u64.u32 	%rd38, %r26;
	add.s64 	%rd39, %rd38, %rd25;
	cvt.u64.u32 	%rd10, %r37;
	shl.b64 	%rd40, %rd39, 3;
	and.b64  	%rd41, %rd40, 34359738360;
	add.s64 	%rd11, %rd3, %rd41;
	ld.global.u64 	%rd12, [%rd11];
	and.b64  	%rd42, %rd12, -4294967296;
	setp.eq.s64 	%p13, %rd42, 0;
	@%p13 bra 	$L__BB18_11;

	div.u64 	%rd60, %rd10, %rd12;
	mul.lo.s64 	%rd43, %rd60, %rd12;
	sub.s64 	%rd61, %rd10, %rd43;
	bra.uni 	$L__BB18_12;

$L__BB18_11:
	cvt.u32.u64 	%r27, %rd12;
	cvt.u32.u64 	%r28, %rd10;
	div.u32 	%r29, %r28, %r27;
	mul.lo.s32 	%r30, %r29, %r27;
	sub.s32 	%r31, %r28, %r30;
	cvt.u64.u32 	%rd60, %r29;
	cvt.u64.u32 	%rd61, %r31;

$L__BB18_12:
	shl.b64 	%rd44, %rd25, 3;
	add.s64 	%rd45, %rd11, %rd44;
	ld.global.u64 	%rd46, [%rd45];
	mul.lo.s64 	%rd47, %rd46, %rd61;
	cvt.u32.u64 	%r32, %rd47;
	add.s32 	%r38, %r38, %r32;
	cvt.u32.u64 	%r37, %rd60;
	add.s32 	%r36, %r36, 1;
	cvt.u64.u32 	%rd48, %r36;
	setp.lt.u64 	%p14, %rd48, %rd25;
	@%p14 bra 	$L__BB18_9;

	cvt.u64.u32 	%rd49, %r38;
	add.s64 	%rd50, %rd2, %rd49;
	ld.global.u8 	%rs1, [%rd50];
	cvt.rn.f64.u16 	%fd1, %rs1;
	shl.b64 	%rd51, %rd59, 3;
	add.s64 	%rd52, %rd1, %rd51;
	st.global.f64 	[%rd52], %fd1;
	add.s32 	%r35, %r35, %r5;
	cvt.u64.u32 	%rd59, %r35;
	setp.lt.u64 	%p15, %rd59, %rd24;
	@%p15 bra 	$L__BB18_8;
	bra.uni 	$L__BB18_18;

$L__BB18_14:
	ld.global.u8 	%rs2, [%rd2];
	cvt.rn.f64.u16 	%fd2, %rs2;
	shl.b64 	%rd53, %rd59, 3;
	add.s64 	%rd54, %rd1, %rd53;
	st.global.f64 	[%rd54], %fd2;
	add.s32 	%r35, %r35, %r5;
	cvt.u64.u32 	%rd59, %r35;
	setp.lt.u64 	%p16, %rd59, %rd24;
	@%p16 bra 	$L__BB18_14;

$L__BB18_18:
	ret;

}
	// .globl	cast_i64_u32
.visible .entry cast_i64_u32(
	.param .u64 cast_i64_u32_param_0,
	.param .u64 cast_i64_u32_param_1,
	.param .u64 cast_i64_u32_param_2,
	.param .u64 cast_i64_u32_param_3,
	.param .u64 cast_i64_u32_param_4
)
{
	.reg .pred 	%p<20>;
	.reg .b32 	%r<41>;
	.reg .b64 	%rd<68>;


	ld.param.u64 	%rd24, [cast_i64_u32_param_0];
	ld.param.u64 	%rd25, [cast_i64_u32_param_1];
	ld.param.u64 	%rd26, [cast_i64_u32_param_2];
	ld.param.u64 	%rd27, [cast_i64_u32_param_3];
	ld.param.u64 	%rd28, [cast_i64_u32_param_4];
	cvta.to.global.u64 	%rd1, %rd28;
	cvta.to.global.u64 	%rd2, %rd27;
	cvta.to.global.u64 	%rd3, %rd26;
	setp.eq.s64 	%p3, %rd25, 0;
	setp.eq.s64 	%p4, %rd26, 0;
	or.pred  	%p5, %p4, %p3;
	mov.pred 	%p2, -1;
	mov.pred 	%p19, %p2;
	@%p5 bra 	$L__BB19_5;

	mov.u64 	%rd62, 1;
	mov.u32 	%r34, 0;

$L__BB19_2:
	not.b32 	%r20, %r34;
	cvt.u64.u32 	%rd30, %r20;
	add.s64 	%rd31, %rd30, %rd25;
	shl.b64 	%rd32, %rd31, 3;
	and.b64  	%rd33, %rd32, 34359738360;
	add.s64 	%rd5, %rd3, %rd33;
	ld.global.u64 	%rd6, [%rd5];
	setp.lt.u64 	%p6, %rd6, 2;
	@%p6 bra 	$L__BB19_4;

	shl.b64 	%rd34, %rd25, 3;
	add.s64 	%rd35, %rd5, %rd34;
	ld.global.u64 	%rd36, [%rd35];
	setp.ne.s64 	%p8, %rd62, %rd36;
	mov.pred 	%p19, 0;
	@%p8 bra 	$L__BB19_5;

$L__BB19_4:
	mul.lo.s64 	%rd62, %rd6, %rd62;
	add.s32 	%r34, %r34, 1;
	cvt.u64.u32 	%rd37, %r34;
	setp.lt.u64 	%p10, %rd37, %rd25;
	mov.pred 	%p19, %p2;
	@%p10 bra 	$L__BB19_2;

$L__BB19_5:
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r21, %ctaid.x;
	mov.u32 	%r22, %tid.x;
	mad.lo.s32 	%r35, %r21, %r3, %r22;
	cvt.u64.u32 	%rd63, %r35;
	@%p19 bra 	$L__BB19_15;
	bra.uni 	$L__BB19_6;

$L__BB19_15:
	setp.ge.u64 	%p17, %rd63, %rd24;
	@%p17 bra 	$L__BB19_18;

	mov.u32 	%r33, %nctaid.x;
	mul.lo.s32 	%r16, %r3, %r33;

$L__BB19_17:
	shl.b64 	%rd57, %rd63, 3;
	add.s64 	%rd58, %rd2, %rd57;
	ld.global.u64 	%rd59, [%rd58];
	shl.b64 	%rd60, %rd63, 2;
	add.s64 	%rd61, %rd1, %rd60;
	st.global.u32 	[%rd61], %rd59;
	add.s32 	%r35, %r35, %r16;
	cvt.u64.u32 	%rd63, %r35;
	setp.lt.u64 	%p18, %rd63, %rd24;
	@%p18 bra 	$L__BB19_17;
	bra.uni 	$L__BB19_18;

$L__BB19_6:
	setp.ge.u64 	%p11, %rd63, %rd24;
	@%p11 bra 	$L__BB19_18;

	mov.u32 	%r23, %nctaid.x;
	mul.lo.s32 	%r5, %r3, %r23;
	@%p3 bra 	$L__BB19_14;

$L__BB19_8:
	mov.u32 	%r36, 0;
	mov.u32 	%r37, %r35;
	mov.u32 	%r38, %r36;

$L__BB19_9:
	not.b32 	%r26, %r36;
	cvt.u64.u32 	%rd38, %r26;
	add.s64 	%rd39, %rd38, %rd25;
	cvt.u64.u32 	%rd10, %r37;
	shl.b64 	%rd40, %rd39, 3;
	and.b64  	%rd41, %rd40, 34359738360;
	add.s64 	%rd11, %rd3, %rd41;
	ld.global.u64 	%rd12, [%rd11];
	and.b64  	%rd42, %rd12, -4294967296;
	setp.eq.s64 	%p13, %rd42, 0;
	@%p13 bra 	$L__BB19_11;

	div.u64 	%rd64, %rd10, %rd12;
	mul.lo.s64 	%rd43, %rd64, %rd12;
	sub.s64 	%rd65, %rd10, %rd43;
	bra.uni 	$L__BB19_12;

$L__BB19_11:
	cvt.u32.u64 	%r27, %rd12;
	cvt.u32.u64 	%r28, %rd10;
	div.u32 	%r29, %r28, %r27;
	mul.lo.s32 	%r30, %r29, %r27;
	sub.s32 	%r31, %r28, %r30;
	cvt.u64.u32 	%rd64, %r29;
	cvt.u64.u32 	%rd65, %r31;

$L__BB19_12:
	shl.b64 	%rd44, %rd25, 3;
	add.s64 	%rd45, %rd11, %rd44;
	ld.global.u64 	%rd46, [%rd45];
	mul.lo.s64 	%rd47, %rd46, %rd65;
	cvt.u32.u64 	%r32, %rd47;
	add.s32 	%r38, %r38, %r32;
	cvt.u32.u64 	%r37, %rd64;
	add.s32 	%r36, %r36, 1;
	cvt.u64.u32 	%rd48, %r36;
	setp.lt.u64 	%p14, %rd48, %rd25;
	@%p14 bra 	$L__BB19_9;

	mul.wide.u32 	%rd49, %r38, 8;
	add.s64 	%rd50, %rd2, %rd49;
	ld.global.u64 	%rd51, [%rd50];
	shl.b64 	%rd52, %rd63, 2;
	add.s64 	%rd53, %rd1, %rd52;
	st.global.u32 	[%rd53], %rd51;
	add.s32 	%r35, %r35, %r5;
	cvt.u64.u32 	%rd63, %r35;
	setp.lt.u64 	%p15, %rd63, %rd24;
	@%p15 bra 	$L__BB19_8;
	bra.uni 	$L__BB19_18;

$L__BB19_14:
	ld.global.u64 	%rd54, [%rd2];
	shl.b64 	%rd55, %rd63, 2;
	add.s64 	%rd56, %rd1, %rd55;
	st.global.u32 	[%rd56], %rd54;
	add.s32 	%r35, %r35, %r5;
	cvt.u64.u32 	%rd63, %r35;
	setp.lt.u64 	%p16, %rd63, %rd24;
	@%p16 bra 	$L__BB19_14;

$L__BB19_18:
	ret;

}
	// .globl	cast_i64_u8
.visible .entry cast_i64_u8(
	.param .u64 cast_i64_u8_param_0,
	.param .u64 cast_i64_u8_param_1,
	.param .u64 cast_i64_u8_param_2,
	.param .u64 cast_i64_u8_param_3,
	.param .u64 cast_i64_u8_param_4
)
{
	.reg .pred 	%p<20>;
	.reg .b32 	%r<41>;
	.reg .b64 	%rd<65>;


	ld.param.u64 	%rd24, [cast_i64_u8_param_0];
	ld.param.u64 	%rd25, [cast_i64_u8_param_1];
	ld.param.u64 	%rd26, [cast_i64_u8_param_2];
	ld.param.u64 	%rd27, [cast_i64_u8_param_3];
	ld.param.u64 	%rd28, [cast_i64_u8_param_4];
	cvta.to.global.u64 	%rd1, %rd28;
	cvta.to.global.u64 	%rd2, %rd27;
	cvta.to.global.u64 	%rd3, %rd26;
	setp.eq.s64 	%p3, %rd25, 0;
	setp.eq.s64 	%p4, %rd26, 0;
	or.pred  	%p5, %p4, %p3;
	mov.pred 	%p2, -1;
	mov.pred 	%p19, %p2;
	@%p5 bra 	$L__BB20_5;

	mov.u64 	%rd59, 1;
	mov.u32 	%r34, 0;

$L__BB20_2:
	not.b32 	%r20, %r34;
	cvt.u64.u32 	%rd30, %r20;
	add.s64 	%rd31, %rd30, %rd25;
	shl.b64 	%rd32, %rd31, 3;
	and.b64  	%rd33, %rd32, 34359738360;
	add.s64 	%rd5, %rd3, %rd33;
	ld.global.u64 	%rd6, [%rd5];
	setp.lt.u64 	%p6, %rd6, 2;
	@%p6 bra 	$L__BB20_4;

	shl.b64 	%rd34, %rd25, 3;
	add.s64 	%rd35, %rd5, %rd34;
	ld.global.u64 	%rd36, [%rd35];
	setp.ne.s64 	%p8, %rd59, %rd36;
	mov.pred 	%p19, 0;
	@%p8 bra 	$L__BB20_5;

$L__BB20_4:
	mul.lo.s64 	%rd59, %rd6, %rd59;
	add.s32 	%r34, %r34, 1;
	cvt.u64.u32 	%rd37, %r34;
	setp.lt.u64 	%p10, %rd37, %rd25;
	mov.pred 	%p19, %p2;
	@%p10 bra 	$L__BB20_2;

$L__BB20_5:
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r21, %ctaid.x;
	mov.u32 	%r22, %tid.x;
	mad.lo.s32 	%r35, %r21, %r3, %r22;
	cvt.u64.u32 	%rd60, %r35;
	@%p19 bra 	$L__BB20_15;
	bra.uni 	$L__BB20_6;

$L__BB20_15:
	setp.ge.u64 	%p17, %rd60, %rd24;
	@%p17 bra 	$L__BB20_18;

	mov.u32 	%r33, %nctaid.x;
	mul.lo.s32 	%r16, %r3, %r33;

$L__BB20_17:
	shl.b64 	%rd55, %rd60, 3;
	add.s64 	%rd56, %rd2, %rd55;
	ld.global.u64 	%rd57, [%rd56];
	add.s64 	%rd58, %rd1, %rd60;
	st.global.u8 	[%rd58], %rd57;
	add.s32 	%r35, %r35, %r16;
	cvt.u64.u32 	%rd60, %r35;
	setp.lt.u64 	%p18, %rd60, %rd24;
	@%p18 bra 	$L__BB20_17;
	bra.uni 	$L__BB20_18;

$L__BB20_6:
	setp.ge.u64 	%p11, %rd60, %rd24;
	@%p11 bra 	$L__BB20_18;

	mov.u32 	%r23, %nctaid.x;
	mul.lo.s32 	%r5, %r3, %r23;
	@%p3 bra 	$L__BB20_14;

$L__BB20_8:
	mov.u32 	%r36, 0;
	mov.u32 	%r37, %r35;
	mov.u32 	%r38, %r36;

$L__BB20_9:
	not.b32 	%r26, %r36;
	cvt.u64.u32 	%rd38, %r26;
	add.s64 	%rd39, %rd38, %rd25;
	cvt.u64.u32 	%rd10, %r37;
	shl.b64 	%rd40, %rd39, 3;
	and.b64  	%rd41, %rd40, 34359738360;
	add.s64 	%rd11, %rd3, %rd41;
	ld.global.u64 	%rd12, [%rd11];
	and.b64  	%rd42, %rd12, -4294967296;
	setp.eq.s64 	%p13, %rd42, 0;
	@%p13 bra 	$L__BB20_11;

	div.u64 	%rd61, %rd10, %rd12;
	mul.lo.s64 	%rd43, %rd61, %rd12;
	sub.s64 	%rd62, %rd10, %rd43;
	bra.uni 	$L__BB20_12;

$L__BB20_11:
	cvt.u32.u64 	%r27, %rd12;
	cvt.u32.u64 	%r28, %rd10;
	div.u32 	%r29, %r28, %r27;
	mul.lo.s32 	%r30, %r29, %r27;
	sub.s32 	%r31, %r28, %r30;
	cvt.u64.u32 	%rd61, %r29;
	cvt.u64.u32 	%rd62, %r31;

$L__BB20_12:
	shl.b64 	%rd44, %rd25, 3;
	add.s64 	%rd45, %rd11, %rd44;
	ld.global.u64 	%rd46, [%rd45];
	mul.lo.s64 	%rd47, %rd46, %rd62;
	cvt.u32.u64 	%r32, %rd47;
	add.s32 	%r38, %r38, %r32;
	cvt.u32.u64 	%r37, %rd61;
	add.s32 	%r36, %r36, 1;
	cvt.u64.u32 	%rd48, %r36;
	setp.lt.u64 	%p14, %rd48, %rd25;
	@%p14 bra 	$L__BB20_9;

	mul.wide.u32 	%rd49, %r38, 8;
	add.s64 	%rd50, %rd2, %rd49;
	ld.global.u64 	%rd51, [%rd50];
	add.s64 	%rd52, %rd1, %rd60;
	st.global.u8 	[%rd52], %rd51;
	add.s32 	%r35, %r35, %r5;
	cvt.u64.u32 	%rd60, %r35;
	setp.lt.u64 	%p15, %rd60, %rd24;
	@%p15 bra 	$L__BB20_8;
	bra.uni 	$L__BB20_18;

$L__BB20_14:
	ld.global.u64 	%rd53, [%rd2];
	add.s64 	%rd54, %rd1, %rd60;
	st.global.u8 	[%rd54], %rd53;
	add.s32 	%r35, %r35, %r5;
	cvt.u64.u32 	%rd60, %r35;
	setp.lt.u64 	%p16, %rd60, %rd24;
	@%p16 bra 	$L__BB20_14;

$L__BB20_18:
	ret;

}
	// .globl	cast_i64_i64
.visible .entry cast_i64_i64(
	.param .u64 cast_i64_i64_param_0,
	.param .u64 cast_i64_i64_param_1,
	.param .u64 cast_i64_i64_param_2,
	.param .u64 cast_i64_i64_param_3,
	.param .u64 cast_i64_i64_param_4
)
{
	.reg .pred 	%p<20>;
	.reg .b32 	%r<41>;
	.reg .b64 	%rd<67>;


	ld.param.u64 	%rd24, [cast_i64_i64_param_0];
	ld.param.u64 	%rd25, [cast_i64_i64_param_1];
	ld.param.u64 	%rd26, [cast_i64_i64_param_2];
	ld.param.u64 	%rd27, [cast_i64_i64_param_3];
	ld.param.u64 	%rd28, [cast_i64_i64_param_4];
	cvta.to.global.u64 	%rd1, %rd28;
	cvta.to.global.u64 	%rd2, %rd27;
	cvta.to.global.u64 	%rd3, %rd26;
	setp.eq.s64 	%p3, %rd25, 0;
	setp.eq.s64 	%p4, %rd26, 0;
	or.pred  	%p5, %p4, %p3;
	mov.pred 	%p2, -1;
	mov.pred 	%p19, %p2;
	@%p5 bra 	$L__BB21_5;

	mov.u64 	%rd61, 1;
	mov.u32 	%r34, 0;

$L__BB21_2:
	not.b32 	%r20, %r34;
	cvt.u64.u32 	%rd30, %r20;
	add.s64 	%rd31, %rd30, %rd25;
	shl.b64 	%rd32, %rd31, 3;
	and.b64  	%rd33, %rd32, 34359738360;
	add.s64 	%rd5, %rd3, %rd33;
	ld.global.u64 	%rd6, [%rd5];
	setp.lt.u64 	%p6, %rd6, 2;
	@%p6 bra 	$L__BB21_4;

	shl.b64 	%rd34, %rd25, 3;
	add.s64 	%rd35, %rd5, %rd34;
	ld.global.u64 	%rd36, [%rd35];
	setp.ne.s64 	%p8, %rd61, %rd36;
	mov.pred 	%p19, 0;
	@%p8 bra 	$L__BB21_5;

$L__BB21_4:
	mul.lo.s64 	%rd61, %rd6, %rd61;
	add.s32 	%r34, %r34, 1;
	cvt.u64.u32 	%rd37, %r34;
	setp.lt.u64 	%p10, %rd37, %rd25;
	mov.pred 	%p19, %p2;
	@%p10 bra 	$L__BB21_2;

$L__BB21_5:
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r21, %ctaid.x;
	mov.u32 	%r22, %tid.x;
	mad.lo.s32 	%r35, %r21, %r3, %r22;
	cvt.u64.u32 	%rd62, %r35;
	@%p19 bra 	$L__BB21_15;
	bra.uni 	$L__BB21_6;

$L__BB21_15:
	setp.ge.u64 	%p17, %rd62, %rd24;
	@%p17 bra 	$L__BB21_18;

	mov.u32 	%r33, %nctaid.x;
	mul.lo.s32 	%r16, %r3, %r33;

$L__BB21_17:
	shl.b64 	%rd57, %rd62, 3;
	add.s64 	%rd58, %rd2, %rd57;
	ld.global.u64 	%rd59, [%rd58];
	add.s64 	%rd60, %rd1, %rd57;
	st.global.u64 	[%rd60], %rd59;
	add.s32 	%r35, %r35, %r16;
	cvt.u64.u32 	%rd62, %r35;
	setp.lt.u64 	%p18, %rd62, %rd24;
	@%p18 bra 	$L__BB21_17;
	bra.uni 	$L__BB21_18;

$L__BB21_6:
	setp.ge.u64 	%p11, %rd62, %rd24;
	@%p11 bra 	$L__BB21_18;

	mov.u32 	%r23, %nctaid.x;
	mul.lo.s32 	%r5, %r3, %r23;
	@%p3 bra 	$L__BB21_14;

$L__BB21_8:
	mov.u32 	%r36, 0;
	mov.u32 	%r37, %r35;
	mov.u32 	%r38, %r36;

$L__BB21_9:
	not.b32 	%r26, %r36;
	cvt.u64.u32 	%rd38, %r26;
	add.s64 	%rd39, %rd38, %rd25;
	cvt.u64.u32 	%rd10, %r37;
	shl.b64 	%rd40, %rd39, 3;
	and.b64  	%rd41, %rd40, 34359738360;
	add.s64 	%rd11, %rd3, %rd41;
	ld.global.u64 	%rd12, [%rd11];
	and.b64  	%rd42, %rd12, -4294967296;
	setp.eq.s64 	%p13, %rd42, 0;
	@%p13 bra 	$L__BB21_11;

	div.u64 	%rd63, %rd10, %rd12;
	mul.lo.s64 	%rd43, %rd63, %rd12;
	sub.s64 	%rd64, %rd10, %rd43;
	bra.uni 	$L__BB21_12;

$L__BB21_11:
	cvt.u32.u64 	%r27, %rd12;
	cvt.u32.u64 	%r28, %rd10;
	div.u32 	%r29, %r28, %r27;
	mul.lo.s32 	%r30, %r29, %r27;
	sub.s32 	%r31, %r28, %r30;
	cvt.u64.u32 	%rd63, %r29;
	cvt.u64.u32 	%rd64, %r31;

$L__BB21_12:
	shl.b64 	%rd44, %rd25, 3;
	add.s64 	%rd45, %rd11, %rd44;
	ld.global.u64 	%rd46, [%rd45];
	mul.lo.s64 	%rd47, %rd46, %rd64;
	cvt.u32.u64 	%r32, %rd47;
	add.s32 	%r38, %r38, %r32;
	cvt.u32.u64 	%r37, %rd63;
	add.s32 	%r36, %r36, 1;
	cvt.u64.u32 	%rd48, %r36;
	setp.lt.u64 	%p14, %rd48, %rd25;
	@%p14 bra 	$L__BB21_9;

	mul.wide.u32 	%rd49, %r38, 8;
	add.s64 	%rd50, %rd2, %rd49;
	ld.global.u64 	%rd51, [%rd50];
	shl.b64 	%rd52, %rd62, 3;
	add.s64 	%rd53, %rd1, %rd52;
	st.global.u64 	[%rd53], %rd51;
	add.s32 	%r35, %r35, %r5;
	cvt.u64.u32 	%rd62, %r35;
	setp.lt.u64 	%p15, %rd62, %rd24;
	@%p15 bra 	$L__BB21_8;
	bra.uni 	$L__BB21_18;

$L__BB21_14:
	ld.global.u64 	%rd54, [%rd2];
	shl.b64 	%rd55, %rd62, 3;
	add.s64 	%rd56, %rd1, %rd55;
	st.global.u64 	[%rd56], %rd54;
	add.s32 	%r35, %r35, %r5;
	cvt.u64.u32 	%rd62, %r35;
	setp.lt.u64 	%p16, %rd62, %rd24;
	@%p16 bra 	$L__BB21_14;

$L__BB21_18:
	ret;

}
	// .globl	cast_i64_f32
.visible .entry cast_i64_f32(
	.param .u64 cast_i64_f32_param_0,
	.param .u64 cast_i64_f32_param_1,
	.param .u64 cast_i64_f32_param_2,
	.param .u64 cast_i64_f32_param_3,
	.param .u64 cast_i64_f32_param_4
)
{
	.reg .pred 	%p<20>;
	.reg .f32 	%f<4>;
	.reg .b32 	%r<41>;
	.reg .b64 	%rd<68>;


	ld.param.u64 	%rd24, [cast_i64_f32_param_0];
	ld.param.u64 	%rd25, [cast_i64_f32_param_1];
	ld.param.u64 	%rd26, [cast_i64_f32_param_2];
	ld.param.u64 	%rd27, [cast_i64_f32_param_3];
	ld.param.u64 	%rd28, [cast_i64_f32_param_4];
	cvta.to.global.u64 	%rd1, %rd28;
	cvta.to.global.u64 	%rd2, %rd27;
	cvta.to.global.u64 	%rd3, %rd26;
	setp.eq.s64 	%p3, %rd25, 0;
	setp.eq.s64 	%p4, %rd26, 0;
	or.pred  	%p5, %p4, %p3;
	mov.pred 	%p2, -1;
	mov.pred 	%p19, %p2;
	@%p5 bra 	$L__BB22_5;

	mov.u64 	%rd62, 1;
	mov.u32 	%r34, 0;

$L__BB22_2:
	not.b32 	%r20, %r34;
	cvt.u64.u32 	%rd30, %r20;
	add.s64 	%rd31, %rd30, %rd25;
	shl.b64 	%rd32, %rd31, 3;
	and.b64  	%rd33, %rd32, 34359738360;
	add.s64 	%rd5, %rd3, %rd33;
	ld.global.u64 	%rd6, [%rd5];
	setp.lt.u64 	%p6, %rd6, 2;
	@%p6 bra 	$L__BB22_4;

	shl.b64 	%rd34, %rd25, 3;
	add.s64 	%rd35, %rd5, %rd34;
	ld.global.u64 	%rd36, [%rd35];
	setp.ne.s64 	%p8, %rd62, %rd36;
	mov.pred 	%p19, 0;
	@%p8 bra 	$L__BB22_5;

$L__BB22_4:
	mul.lo.s64 	%rd62, %rd6, %rd62;
	add.s32 	%r34, %r34, 1;
	cvt.u64.u32 	%rd37, %r34;
	setp.lt.u64 	%p10, %rd37, %rd25;
	mov.pred 	%p19, %p2;
	@%p10 bra 	$L__BB22_2;

$L__BB22_5:
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r21, %ctaid.x;
	mov.u32 	%r22, %tid.x;
	mad.lo.s32 	%r35, %r21, %r3, %r22;
	cvt.u64.u32 	%rd63, %r35;
	@%p19 bra 	$L__BB22_15;
	bra.uni 	$L__BB22_6;

$L__BB22_15:
	setp.ge.u64 	%p17, %rd63, %rd24;
	@%p17 bra 	$L__BB22_18;

	mov.u32 	%r33, %nctaid.x;
	mul.lo.s32 	%r16, %r3, %r33;

$L__BB22_17:
	shl.b64 	%rd57, %rd63, 3;
	add.s64 	%rd58, %rd2, %rd57;
	ld.global.u64 	%rd59, [%rd58];
	cvt.rn.f32.s64 	%f3, %rd59;
	shl.b64 	%rd60, %rd63, 2;
	add.s64 	%rd61, %rd1, %rd60;
	st.global.f32 	[%rd61], %f3;
	add.s32 	%r35, %r35, %r16;
	cvt.u64.u32 	%rd63, %r35;
	setp.lt.u64 	%p18, %rd63, %rd24;
	@%p18 bra 	$L__BB22_17;
	bra.uni 	$L__BB22_18;

$L__BB22_6:
	setp.ge.u64 	%p11, %rd63, %rd24;
	@%p11 bra 	$L__BB22_18;

	mov.u32 	%r23, %nctaid.x;
	mul.lo.s32 	%r5, %r3, %r23;
	@%p3 bra 	$L__BB22_14;

$L__BB22_8:
	mov.u32 	%r36, 0;
	mov.u32 	%r37, %r35;
	mov.u32 	%r38, %r36;

$L__BB22_9:
	not.b32 	%r26, %r36;
	cvt.u64.u32 	%rd38, %r26;
	add.s64 	%rd39, %rd38, %rd25;
	cvt.u64.u32 	%rd10, %r37;
	shl.b64 	%rd40, %rd39, 3;
	and.b64  	%rd41, %rd40, 34359738360;
	add.s64 	%rd11, %rd3, %rd41;
	ld.global.u64 	%rd12, [%rd11];
	and.b64  	%rd42, %rd12, -4294967296;
	setp.eq.s64 	%p13, %rd42, 0;
	@%p13 bra 	$L__BB22_11;

	div.u64 	%rd64, %rd10, %rd12;
	mul.lo.s64 	%rd43, %rd64, %rd12;
	sub.s64 	%rd65, %rd10, %rd43;
	bra.uni 	$L__BB22_12;

$L__BB22_11:
	cvt.u32.u64 	%r27, %rd12;
	cvt.u32.u64 	%r28, %rd10;
	div.u32 	%r29, %r28, %r27;
	mul.lo.s32 	%r30, %r29, %r27;
	sub.s32 	%r31, %r28, %r30;
	cvt.u64.u32 	%rd64, %r29;
	cvt.u64.u32 	%rd65, %r31;

$L__BB22_12:
	shl.b64 	%rd44, %rd25, 3;
	add.s64 	%rd45, %rd11, %rd44;
	ld.global.u64 	%rd46, [%rd45];
	mul.lo.s64 	%rd47, %rd46, %rd65;
	cvt.u32.u64 	%r32, %rd47;
	add.s32 	%r38, %r38, %r32;
	cvt.u32.u64 	%r37, %rd64;
	add.s32 	%r36, %r36, 1;
	cvt.u64.u32 	%rd48, %r36;
	setp.lt.u64 	%p14, %rd48, %rd25;
	@%p14 bra 	$L__BB22_9;

	mul.wide.u32 	%rd49, %r38, 8;
	add.s64 	%rd50, %rd2, %rd49;
	ld.global.u64 	%rd51, [%rd50];
	cvt.rn.f32.s64 	%f1, %rd51;
	shl.b64 	%rd52, %rd63, 2;
	add.s64 	%rd53, %rd1, %rd52;
	st.global.f32 	[%rd53], %f1;
	add.s32 	%r35, %r35, %r5;
	cvt.u64.u32 	%rd63, %r35;
	setp.lt.u64 	%p15, %rd63, %rd24;
	@%p15 bra 	$L__BB22_8;
	bra.uni 	$L__BB22_18;

$L__BB22_14:
	ld.global.u64 	%rd54, [%rd2];
	cvt.rn.f32.s64 	%f2, %rd54;
	shl.b64 	%rd55, %rd63, 2;
	add.s64 	%rd56, %rd1, %rd55;
	st.global.f32 	[%rd56], %f2;
	add.s32 	%r35, %r35, %r5;
	cvt.u64.u32 	%rd63, %r35;
	setp.lt.u64 	%p16, %rd63, %rd24;
	@%p16 bra 	$L__BB22_14;

$L__BB22_18:
	ret;

}
	// .globl	cast_i64_f64
.visible .entry cast_i64_f64(
	.param .u64 cast_i64_f64_param_0,
	.param .u64 cast_i64_f64_param_1,
	.param .u64 cast_i64_f64_param_2,
	.param .u64 cast_i64_f64_param_3,
	.param .u64 cast_i64_f64_param_4
)
{
	.reg .pred 	%p<20>;
	.reg .b32 	%r<41>;
	.reg .f64 	%fd<4>;
	.reg .b64 	%rd<67>;


	ld.param.u64 	%rd24, [cast_i64_f64_param_0];
	ld.param.u64 	%rd25, [cast_i64_f64_param_1];
	ld.param.u64 	%rd26, [cast_i64_f64_param_2];
	ld.param.u64 	%rd27, [cast_i64_f64_param_3];
	ld.param.u64 	%rd28, [cast_i64_f64_param_4];
	cvta.to.global.u64 	%rd1, %rd28;
	cvta.to.global.u64 	%rd2, %rd27;
	cvta.to.global.u64 	%rd3, %rd26;
	setp.eq.s64 	%p3, %rd25, 0;
	setp.eq.s64 	%p4, %rd26, 0;
	or.pred  	%p5, %p4, %p3;
	mov.pred 	%p2, -1;
	mov.pred 	%p19, %p2;
	@%p5 bra 	$L__BB23_5;

	mov.u64 	%rd61, 1;
	mov.u32 	%r34, 0;

$L__BB23_2:
	not.b32 	%r20, %r34;
	cvt.u64.u32 	%rd30, %r20;
	add.s64 	%rd31, %rd30, %rd25;
	shl.b64 	%rd32, %rd31, 3;
	and.b64  	%rd33, %rd32, 34359738360;
	add.s64 	%rd5, %rd3, %rd33;
	ld.global.u64 	%rd6, [%rd5];
	setp.lt.u64 	%p6, %rd6, 2;
	@%p6 bra 	$L__BB23_4;

	shl.b64 	%rd34, %rd25, 3;
	add.s64 	%rd35, %rd5, %rd34;
	ld.global.u64 	%rd36, [%rd35];
	setp.ne.s64 	%p8, %rd61, %rd36;
	mov.pred 	%p19, 0;
	@%p8 bra 	$L__BB23_5;

$L__BB23_4:
	mul.lo.s64 	%rd61, %rd6, %rd61;
	add.s32 	%r34, %r34, 1;
	cvt.u64.u32 	%rd37, %r34;
	setp.lt.u64 	%p10, %rd37, %rd25;
	mov.pred 	%p19, %p2;
	@%p10 bra 	$L__BB23_2;

$L__BB23_5:
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r21, %ctaid.x;
	mov.u32 	%r22, %tid.x;
	mad.lo.s32 	%r35, %r21, %r3, %r22;
	cvt.u64.u32 	%rd62, %r35;
	@%p19 bra 	$L__BB23_15;
	bra.uni 	$L__BB23_6;

$L__BB23_15:
	setp.ge.u64 	%p17, %rd62, %rd24;
	@%p17 bra 	$L__BB23_18;

	mov.u32 	%r33, %nctaid.x;
	mul.lo.s32 	%r16, %r3, %r33;

$L__BB23_17:
	shl.b64 	%rd57, %rd62, 3;
	add.s64 	%rd58, %rd2, %rd57;
	ld.global.u64 	%rd59, [%rd58];
	cvt.rn.f64.s64 	%fd3, %rd59;
	add.s64 	%rd60, %rd1, %rd57;
	st.global.f64 	[%rd60], %fd3;
	add.s32 	%r35, %r35, %r16;
	cvt.u64.u32 	%rd62, %r35;
	setp.lt.u64 	%p18, %rd62, %rd24;
	@%p18 bra 	$L__BB23_17;
	bra.uni 	$L__BB23_18;

$L__BB23_6:
	setp.ge.u64 	%p11, %rd62, %rd24;
	@%p11 bra 	$L__BB23_18;

	mov.u32 	%r23, %nctaid.x;
	mul.lo.s32 	%r5, %r3, %r23;
	@%p3 bra 	$L__BB23_14;

$L__BB23_8:
	mov.u32 	%r36, 0;
	mov.u32 	%r37, %r35;
	mov.u32 	%r38, %r36;

$L__BB23_9:
	not.b32 	%r26, %r36;
	cvt.u64.u32 	%rd38, %r26;
	add.s64 	%rd39, %rd38, %rd25;
	cvt.u64.u32 	%rd10, %r37;
	shl.b64 	%rd40, %rd39, 3;
	and.b64  	%rd41, %rd40, 34359738360;
	add.s64 	%rd11, %rd3, %rd41;
	ld.global.u64 	%rd12, [%rd11];
	and.b64  	%rd42, %rd12, -4294967296;
	setp.eq.s64 	%p13, %rd42, 0;
	@%p13 bra 	$L__BB23_11;

	div.u64 	%rd63, %rd10, %rd12;
	mul.lo.s64 	%rd43, %rd63, %rd12;
	sub.s64 	%rd64, %rd10, %rd43;
	bra.uni 	$L__BB23_12;

$L__BB23_11:
	cvt.u32.u64 	%r27, %rd12;
	cvt.u32.u64 	%r28, %rd10;
	div.u32 	%r29, %r28, %r27;
	mul.lo.s32 	%r30, %r29, %r27;
	sub.s32 	%r31, %r28, %r30;
	cvt.u64.u32 	%rd63, %r29;
	cvt.u64.u32 	%rd64, %r31;

$L__BB23_12:
	shl.b64 	%rd44, %rd25, 3;
	add.s64 	%rd45, %rd11, %rd44;
	ld.global.u64 	%rd46, [%rd45];
	mul.lo.s64 	%rd47, %rd46, %rd64;
	cvt.u32.u64 	%r32, %rd47;
	add.s32 	%r38, %r38, %r32;
	cvt.u32.u64 	%r37, %rd63;
	add.s32 	%r36, %r36, 1;
	cvt.u64.u32 	%rd48, %r36;
	setp.lt.u64 	%p14, %rd48, %rd25;
	@%p14 bra 	$L__BB23_9;

	mul.wide.u32 	%rd49, %r38, 8;
	add.s64 	%rd50, %rd2, %rd49;
	ld.global.u64 	%rd51, [%rd50];
	cvt.rn.f64.s64 	%fd1, %rd51;
	shl.b64 	%rd52, %rd62, 3;
	add.s64 	%rd53, %rd1, %rd52;
	st.global.f64 	[%rd53], %fd1;
	add.s32 	%r35, %r35, %r5;
	cvt.u64.u32 	%rd62, %r35;
	setp.lt.u64 	%p15, %rd62, %rd24;
	@%p15 bra 	$L__BB23_8;
	bra.uni 	$L__BB23_18;

$L__BB23_14:
	ld.global.u64 	%rd54, [%rd2];
	cvt.rn.f64.s64 	%fd2, %rd54;
	shl.b64 	%rd55, %rd62, 3;
	add.s64 	%rd56, %rd1, %rd55;
	st.global.f64 	[%rd56], %fd2;
	add.s32 	%r35, %r35, %r5;
	cvt.u64.u32 	%rd62, %r35;
	setp.lt.u64 	%p16, %rd62, %rd24;
	@%p16 bra 	$L__BB23_14;

$L__BB23_18:
	ret;

}
	// .globl	cast_f32_u8
.visible .entry cast_f32_u8(
	.param .u64 cast_f32_u8_param_0,
	.param .u64 cast_f32_u8_param_1,
	.param .u64 cast_f32_u8_param_2,
	.param .u64 cast_f32_u8_param_3,
	.param .u64 cast_f32_u8_param_4
)
{
	.reg .pred 	%p<20>;
	.reg .f32 	%f<4>;
	.reg .b32 	%r<44>;
	.reg .b64 	%rd<62>;


	ld.param.u64 	%rd24, [cast_f32_u8_param_0];
	ld.param.u64 	%rd25, [cast_f32_u8_param_1];
	ld.param.u64 	%rd26, [cast_f32_u8_param_2];
	ld.param.u64 	%rd27, [cast_f32_u8_param_3];
	ld.param.u64 	%rd28, [cast_f32_u8_param_4];
	cvta.to.global.u64 	%rd1, %rd28;
	cvta.to.global.u64 	%rd2, %rd27;
	cvta.to.global.u64 	%rd3, %rd26;
	setp.eq.s64 	%p3, %rd25, 0;
	setp.eq.s64 	%p4, %rd26, 0;
	or.pred  	%p5, %p4, %p3;
	mov.pred 	%p2, -1;
	mov.pred 	%p19, %p2;
	@%p5 bra 	$L__BB24_5;

	mov.u64 	%rd56, 1;
	mov.u32 	%r37, 0;

$L__BB24_2:
	not.b32 	%r20, %r37;
	cvt.u64.u32 	%rd30, %r20;
	add.s64 	%rd31, %rd30, %rd25;
	shl.b64 	%rd32, %rd31, 3;
	and.b64  	%rd33, %rd32, 34359738360;
	add.s64 	%rd5, %rd3, %rd33;
	ld.global.u64 	%rd6, [%rd5];
	setp.lt.u64 	%p6, %rd6, 2;
	@%p6 bra 	$L__BB24_4;

	shl.b64 	%rd34, %rd25, 3;
	add.s64 	%rd35, %rd5, %rd34;
	ld.global.u64 	%rd36, [%rd35];
	setp.ne.s64 	%p8, %rd56, %rd36;
	mov.pred 	%p19, 0;
	@%p8 bra 	$L__BB24_5;

$L__BB24_4:
	mul.lo.s64 	%rd56, %rd6, %rd56;
	add.s32 	%r37, %r37, 1;
	cvt.u64.u32 	%rd37, %r37;
	setp.lt.u64 	%p10, %rd37, %rd25;
	mov.pred 	%p19, %p2;
	@%p10 bra 	$L__BB24_2;

$L__BB24_5:
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r21, %ctaid.x;
	mov.u32 	%r22, %tid.x;
	mad.lo.s32 	%r38, %r21, %r3, %r22;
	cvt.u64.u32 	%rd57, %r38;
	@%p19 bra 	$L__BB24_15;
	bra.uni 	$L__BB24_6;

$L__BB24_15:
	setp.ge.u64 	%p17, %rd57, %rd24;
	@%p17 bra 	$L__BB24_18;

	mov.u32 	%r35, %nctaid.x;
	mul.lo.s32 	%r16, %r3, %r35;

$L__BB24_17:
	shl.b64 	%rd53, %rd57, 2;
	add.s64 	%rd54, %rd2, %rd53;
	ld.global.f32 	%f3, [%rd54];
	cvt.rzi.u32.f32 	%r36, %f3;
	add.s64 	%rd55, %rd1, %rd57;
	st.global.u8 	[%rd55], %r36;
	add.s32 	%r38, %r38, %r16;
	cvt.u64.u32 	%rd57, %r38;
	setp.lt.u64 	%p18, %rd57, %rd24;
	@%p18 bra 	$L__BB24_17;
	bra.uni 	$L__BB24_18;

$L__BB24_6:
	setp.ge.u64 	%p11, %rd57, %rd24;
	@%p11 bra 	$L__BB24_18;

	mov.u32 	%r23, %nctaid.x;
	mul.lo.s32 	%r5, %r3, %r23;
	@%p3 bra 	$L__BB24_14;

$L__BB24_8:
	mov.u32 	%r39, 0;
	mov.u32 	%r40, %r38;
	mov.u32 	%r41, %r39;

$L__BB24_9:
	not.b32 	%r26, %r39;
	cvt.u64.u32 	%rd38, %r26;
	add.s64 	%rd39, %rd38, %rd25;
	cvt.u64.u32 	%rd10, %r40;
	shl.b64 	%rd40, %rd39, 3;
	and.b64  	%rd41, %rd40, 34359738360;
	add.s64 	%rd11, %rd3, %rd41;
	ld.global.u64 	%rd12, [%rd11];
	and.b64  	%rd42, %rd12, -4294967296;
	setp.eq.s64 	%p13, %rd42, 0;
	@%p13 bra 	$L__BB24_11;

	div.u64 	%rd58, %rd10, %rd12;
	mul.lo.s64 	%rd43, %rd58, %rd12;
	sub.s64 	%rd59, %rd10, %rd43;
	bra.uni 	$L__BB24_12;

$L__BB24_11:
	cvt.u32.u64 	%r27, %rd12;
	cvt.u32.u64 	%r28, %rd10;
	div.u32 	%r29, %r28, %r27;
	mul.lo.s32 	%r30, %r29, %r27;
	sub.s32 	%r31, %r28, %r30;
	cvt.u64.u32 	%rd58, %r29;
	cvt.u64.u32 	%rd59, %r31;

$L__BB24_12:
	shl.b64 	%rd44, %rd25, 3;
	add.s64 	%rd45, %rd11, %rd44;
	ld.global.u64 	%rd46, [%rd45];
	mul.lo.s64 	%rd47, %rd46, %rd59;
	cvt.u32.u64 	%r32, %rd47;
	add.s32 	%r41, %r41, %r32;
	cvt.u32.u64 	%r40, %rd58;
	add.s32 	%r39, %r39, 1;
	cvt.u64.u32 	%rd48, %r39;
	setp.lt.u64 	%p14, %rd48, %rd25;
	@%p14 bra 	$L__BB24_9;

	mul.wide.u32 	%rd49, %r41, 4;
	add.s64 	%rd50, %rd2, %rd49;
	ld.global.f32 	%f1, [%rd50];
	cvt.rzi.u32.f32 	%r33, %f1;
	add.s64 	%rd51, %rd1, %rd57;
	st.global.u8 	[%rd51], %r33;
	add.s32 	%r38, %r38, %r5;
	cvt.u64.u32 	%rd57, %r38;
	setp.lt.u64 	%p15, %rd57, %rd24;
	@%p15 bra 	$L__BB24_8;
	bra.uni 	$L__BB24_18;

$L__BB24_14:
	ld.global.f32 	%f2, [%rd2];
	cvt.rzi.u32.f32 	%r34, %f2;
	add.s64 	%rd52, %rd1, %rd57;
	st.global.u8 	[%rd52], %r34;
	add.s32 	%r38, %r38, %r5;
	cvt.u64.u32 	%rd57, %r38;
	setp.lt.u64 	%p16, %rd57, %rd24;
	@%p16 bra 	$L__BB24_14;

$L__BB24_18:
	ret;

}
	// .globl	cast_f32_u32
.visible .entry cast_f32_u32(
	.param .u64 cast_f32_u32_param_0,
	.param .u64 cast_f32_u32_param_1,
	.param .u64 cast_f32_u32_param_2,
	.param .u64 cast_f32_u32_param_3,
	.param .u64 cast_f32_u32_param_4
)
{
	.reg .pred 	%p<20>;
	.reg .f32 	%f<4>;
	.reg .b32 	%r<44>;
	.reg .b64 	%rd<64>;


	ld.param.u64 	%rd24, [cast_f32_u32_param_0];
	ld.param.u64 	%rd25, [cast_f32_u32_param_1];
	ld.param.u64 	%rd26, [cast_f32_u32_param_2];
	ld.param.u64 	%rd27, [cast_f32_u32_param_3];
	ld.param.u64 	%rd28, [cast_f32_u32_param_4];
	cvta.to.global.u64 	%rd1, %rd28;
	cvta.to.global.u64 	%rd2, %rd27;
	cvta.to.global.u64 	%rd3, %rd26;
	setp.eq.s64 	%p3, %rd25, 0;
	setp.eq.s64 	%p4, %rd26, 0;
	or.pred  	%p5, %p4, %p3;
	mov.pred 	%p2, -1;
	mov.pred 	%p19, %p2;
	@%p5 bra 	$L__BB25_5;

	mov.u64 	%rd58, 1;
	mov.u32 	%r37, 0;

$L__BB25_2:
	not.b32 	%r20, %r37;
	cvt.u64.u32 	%rd30, %r20;
	add.s64 	%rd31, %rd30, %rd25;
	shl.b64 	%rd32, %rd31, 3;
	and.b64  	%rd33, %rd32, 34359738360;
	add.s64 	%rd5, %rd3, %rd33;
	ld.global.u64 	%rd6, [%rd5];
	setp.lt.u64 	%p6, %rd6, 2;
	@%p6 bra 	$L__BB25_4;

	shl.b64 	%rd34, %rd25, 3;
	add.s64 	%rd35, %rd5, %rd34;
	ld.global.u64 	%rd36, [%rd35];
	setp.ne.s64 	%p8, %rd58, %rd36;
	mov.pred 	%p19, 0;
	@%p8 bra 	$L__BB25_5;

$L__BB25_4:
	mul.lo.s64 	%rd58, %rd6, %rd58;
	add.s32 	%r37, %r37, 1;
	cvt.u64.u32 	%rd37, %r37;
	setp.lt.u64 	%p10, %rd37, %rd25;
	mov.pred 	%p19, %p2;
	@%p10 bra 	$L__BB25_2;

$L__BB25_5:
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r21, %ctaid.x;
	mov.u32 	%r22, %tid.x;
	mad.lo.s32 	%r38, %r21, %r3, %r22;
	cvt.u64.u32 	%rd59, %r38;
	@%p19 bra 	$L__BB25_15;
	bra.uni 	$L__BB25_6;

$L__BB25_15:
	setp.ge.u64 	%p17, %rd59, %rd24;
	@%p17 bra 	$L__BB25_18;

	mov.u32 	%r35, %nctaid.x;
	mul.lo.s32 	%r16, %r3, %r35;

$L__BB25_17:
	shl.b64 	%rd55, %rd59, 2;
	add.s64 	%rd56, %rd2, %rd55;
	ld.global.f32 	%f3, [%rd56];
	cvt.rzi.u32.f32 	%r36, %f3;
	add.s64 	%rd57, %rd1, %rd55;
	st.global.u32 	[%rd57], %r36;
	add.s32 	%r38, %r38, %r16;
	cvt.u64.u32 	%rd59, %r38;
	setp.lt.u64 	%p18, %rd59, %rd24;
	@%p18 bra 	$L__BB25_17;
	bra.uni 	$L__BB25_18;

$L__BB25_6:
	setp.ge.u64 	%p11, %rd59, %rd24;
	@%p11 bra 	$L__BB25_18;

	mov.u32 	%r23, %nctaid.x;
	mul.lo.s32 	%r5, %r3, %r23;
	@%p3 bra 	$L__BB25_14;

$L__BB25_8:
	mov.u32 	%r39, 0;
	mov.u32 	%r40, %r38;
	mov.u32 	%r41, %r39;

$L__BB25_9:
	not.b32 	%r26, %r39;
	cvt.u64.u32 	%rd38, %r26;
	add.s64 	%rd39, %rd38, %rd25;
	cvt.u64.u32 	%rd10, %r40;
	shl.b64 	%rd40, %rd39, 3;
	and.b64  	%rd41, %rd40, 34359738360;
	add.s64 	%rd11, %rd3, %rd41;
	ld.global.u64 	%rd12, [%rd11];
	and.b64  	%rd42, %rd12, -4294967296;
	setp.eq.s64 	%p13, %rd42, 0;
	@%p13 bra 	$L__BB25_11;

	div.u64 	%rd60, %rd10, %rd12;
	mul.lo.s64 	%rd43, %rd60, %rd12;
	sub.s64 	%rd61, %rd10, %rd43;
	bra.uni 	$L__BB25_12;

$L__BB25_11:
	cvt.u32.u64 	%r27, %rd12;
	cvt.u32.u64 	%r28, %rd10;
	div.u32 	%r29, %r28, %r27;
	mul.lo.s32 	%r30, %r29, %r27;
	sub.s32 	%r31, %r28, %r30;
	cvt.u64.u32 	%rd60, %r29;
	cvt.u64.u32 	%rd61, %r31;

$L__BB25_12:
	shl.b64 	%rd44, %rd25, 3;
	add.s64 	%rd45, %rd11, %rd44;
	ld.global.u64 	%rd46, [%rd45];
	mul.lo.s64 	%rd47, %rd46, %rd61;
	cvt.u32.u64 	%r32, %rd47;
	add.s32 	%r41, %r41, %r32;
	cvt.u32.u64 	%r40, %rd60;
	add.s32 	%r39, %r39, 1;
	cvt.u64.u32 	%rd48, %r39;
	setp.lt.u64 	%p14, %rd48, %rd25;
	@%p14 bra 	$L__BB25_9;

	mul.wide.u32 	%rd49, %r41, 4;
	add.s64 	%rd50, %rd2, %rd49;
	ld.global.f32 	%f1, [%rd50];
	cvt.rzi.u32.f32 	%r33, %f1;
	shl.b64 	%rd51, %rd59, 2;
	add.s64 	%rd52, %rd1, %rd51;
	st.global.u32 	[%rd52], %r33;
	add.s32 	%r38, %r38, %r5;
	cvt.u64.u32 	%rd59, %r38;
	setp.lt.u64 	%p15, %rd59, %rd24;
	@%p15 bra 	$L__BB25_8;
	bra.uni 	$L__BB25_18;

$L__BB25_14:
	ld.global.f32 	%f2, [%rd2];
	cvt.rzi.u32.f32 	%r34, %f2;
	shl.b64 	%rd53, %rd59, 2;
	add.s64 	%rd54, %rd1, %rd53;
	st.global.u32 	[%rd54], %r34;
	add.s32 	%r38, %r38, %r5;
	cvt.u64.u32 	%rd59, %r38;
	setp.lt.u64 	%p16, %rd59, %rd24;
	@%p16 bra 	$L__BB25_14;

$L__BB25_18:
	ret;

}
	// .globl	cast_f32_i64
.visible .entry cast_f32_i64(
	.param .u64 cast_f32_i64_param_0,
	.param .u64 cast_f32_i64_param_1,
	.param .u64 cast_f32_i64_param_2,
	.param .u64 cast_f32_i64_param_3,
	.param .u64 cast_f32_i64_param_4
)
{
	.reg .pred 	%p<20>;
	.reg .f32 	%f<4>;
	.reg .b32 	%r<41>;
	.reg .b64 	%rd<68>;


	ld.param.u64 	%rd24, [cast_f32_i64_param_0];
	ld.param.u64 	%rd25, [cast_f32_i64_param_1];
	ld.param.u64 	%rd26, [cast_f32_i64_param_2];
	ld.param.u64 	%rd27, [cast_f32_i64_param_3];
	ld.param.u64 	%rd28, [cast_f32_i64_param_4];
	cvta.to.global.u64 	%rd1, %rd28;
	cvta.to.global.u64 	%rd2, %rd27;
	cvta.to.global.u64 	%rd3, %rd26;
	setp.eq.s64 	%p3, %rd25, 0;
	setp.eq.s64 	%p4, %rd26, 0;
	or.pred  	%p5, %p4, %p3;
	mov.pred 	%p2, -1;
	mov.pred 	%p19, %p2;
	@%p5 bra 	$L__BB26_5;

	mov.u64 	%rd62, 1;
	mov.u32 	%r34, 0;

$L__BB26_2:
	not.b32 	%r20, %r34;
	cvt.u64.u32 	%rd30, %r20;
	add.s64 	%rd31, %rd30, %rd25;
	shl.b64 	%rd32, %rd31, 3;
	and.b64  	%rd33, %rd32, 34359738360;
	add.s64 	%rd5, %rd3, %rd33;
	ld.global.u64 	%rd6, [%rd5];
	setp.lt.u64 	%p6, %rd6, 2;
	@%p6 bra 	$L__BB26_4;

	shl.b64 	%rd34, %rd25, 3;
	add.s64 	%rd35, %rd5, %rd34;
	ld.global.u64 	%rd36, [%rd35];
	setp.ne.s64 	%p8, %rd62, %rd36;
	mov.pred 	%p19, 0;
	@%p8 bra 	$L__BB26_5;

$L__BB26_4:
	mul.lo.s64 	%rd62, %rd6, %rd62;
	add.s32 	%r34, %r34, 1;
	cvt.u64.u32 	%rd37, %r34;
	setp.lt.u64 	%p10, %rd37, %rd25;
	mov.pred 	%p19, %p2;
	@%p10 bra 	$L__BB26_2;

$L__BB26_5:
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r21, %ctaid.x;
	mov.u32 	%r22, %tid.x;
	mad.lo.s32 	%r35, %r21, %r3, %r22;
	cvt.u64.u32 	%rd63, %r35;
	@%p19 bra 	$L__BB26_15;
	bra.uni 	$L__BB26_6;

$L__BB26_15:
	setp.ge.u64 	%p17, %rd63, %rd24;
	@%p17 bra 	$L__BB26_18;

	mov.u32 	%r33, %nctaid.x;
	mul.lo.s32 	%r16, %r3, %r33;

$L__BB26_17:
	shl.b64 	%rd57, %rd63, 2;
	add.s64 	%rd58, %rd2, %rd57;
	ld.global.f32 	%f3, [%rd58];
	cvt.rzi.s64.f32 	%rd59, %f3;
	shl.b64 	%rd60, %rd63, 3;
	add.s64 	%rd61, %rd1, %rd60;
	st.global.u64 	[%rd61], %rd59;
	add.s32 	%r35, %r35, %r16;
	cvt.u64.u32 	%rd63, %r35;
	setp.lt.u64 	%p18, %rd63, %rd24;
	@%p18 bra 	$L__BB26_17;
	bra.uni 	$L__BB26_18;

$L__BB26_6:
	setp.ge.u64 	%p11, %rd63, %rd24;
	@%p11 bra 	$L__BB26_18;

	mov.u32 	%r23, %nctaid.x;
	mul.lo.s32 	%r5, %r3, %r23;
	@%p3 bra 	$L__BB26_14;

$L__BB26_8:
	mov.u32 	%r36, 0;
	mov.u32 	%r37, %r35;
	mov.u32 	%r38, %r36;

$L__BB26_9:
	not.b32 	%r26, %r36;
	cvt.u64.u32 	%rd38, %r26;
	add.s64 	%rd39, %rd38, %rd25;
	cvt.u64.u32 	%rd10, %r37;
	shl.b64 	%rd40, %rd39, 3;
	and.b64  	%rd41, %rd40, 34359738360;
	add.s64 	%rd11, %rd3, %rd41;
	ld.global.u64 	%rd12, [%rd11];
	and.b64  	%rd42, %rd12, -4294967296;
	setp.eq.s64 	%p13, %rd42, 0;
	@%p13 bra 	$L__BB26_11;

	div.u64 	%rd64, %rd10, %rd12;
	mul.lo.s64 	%rd43, %rd64, %rd12;
	sub.s64 	%rd65, %rd10, %rd43;
	bra.uni 	$L__BB26_12;

$L__BB26_11:
	cvt.u32.u64 	%r27, %rd12;
	cvt.u32.u64 	%r28, %rd10;
	div.u32 	%r29, %r28, %r27;
	mul.lo.s32 	%r30, %r29, %r27;
	sub.s32 	%r31, %r28, %r30;
	cvt.u64.u32 	%rd64, %r29;
	cvt.u64.u32 	%rd65, %r31;

$L__BB26_12:
	shl.b64 	%rd44, %rd25, 3;
	add.s64 	%rd45, %rd11, %rd44;
	ld.global.u64 	%rd46, [%rd45];
	mul.lo.s64 	%rd47, %rd46, %rd65;
	cvt.u32.u64 	%r32, %rd47;
	add.s32 	%r38, %r38, %r32;
	cvt.u32.u64 	%r37, %rd64;
	add.s32 	%r36, %r36, 1;
	cvt.u64.u32 	%rd48, %r36;
	setp.lt.u64 	%p14, %rd48, %rd25;
	@%p14 bra 	$L__BB26_9;

	mul.wide.u32 	%rd49, %r38, 4;
	add.s64 	%rd50, %rd2, %rd49;
	ld.global.f32 	%f1, [%rd50];
	cvt.rzi.s64.f32 	%rd51, %f1;
	shl.b64 	%rd52, %rd63, 3;
	add.s64 	%rd53, %rd1, %rd52;
	st.global.u64 	[%rd53], %rd51;
	add.s32 	%r35, %r35, %r5;
	cvt.u64.u32 	%rd63, %r35;
	setp.lt.u64 	%p15, %rd63, %rd24;
	@%p15 bra 	$L__BB26_8;
	bra.uni 	$L__BB26_18;

$L__BB26_14:
	ld.global.f32 	%f2, [%rd2];
	cvt.rzi.s64.f32 	%rd54, %f2;
	shl.b64 	%rd55, %rd63, 3;
	add.s64 	%rd56, %rd1, %rd55;
	st.global.u64 	[%rd56], %rd54;
	add.s32 	%r35, %r35, %r5;
	cvt.u64.u32 	%rd63, %r35;
	setp.lt.u64 	%p16, %rd63, %rd24;
	@%p16 bra 	$L__BB26_14;

$L__BB26_18:
	ret;

}
	// .globl	cast_f32_f32
.visible .entry cast_f32_f32(
	.param .u64 cast_f32_f32_param_0,
	.param .u64 cast_f32_f32_param_1,
	.param .u64 cast_f32_f32_param_2,
	.param .u64 cast_f32_f32_param_3,
	.param .u64 cast_f32_f32_param_4
)
{
	.reg .pred 	%p<20>;
	.reg .f32 	%f<4>;
	.reg .b32 	%r<41>;
	.reg .b64 	%rd<64>;


	ld.param.u64 	%rd24, [cast_f32_f32_param_0];
	ld.param.u64 	%rd25, [cast_f32_f32_param_1];
	ld.param.u64 	%rd26, [cast_f32_f32_param_2];
	ld.param.u64 	%rd27, [cast_f32_f32_param_3];
	ld.param.u64 	%rd28, [cast_f32_f32_param_4];
	cvta.to.global.u64 	%rd1, %rd28;
	cvta.to.global.u64 	%rd2, %rd27;
	cvta.to.global.u64 	%rd3, %rd26;
	setp.eq.s64 	%p3, %rd25, 0;
	setp.eq.s64 	%p4, %rd26, 0;
	or.pred  	%p5, %p4, %p3;
	mov.pred 	%p2, -1;
	mov.pred 	%p19, %p2;
	@%p5 bra 	$L__BB27_5;

	mov.u64 	%rd58, 1;
	mov.u32 	%r34, 0;

$L__BB27_2:
	not.b32 	%r20, %r34;
	cvt.u64.u32 	%rd30, %r20;
	add.s64 	%rd31, %rd30, %rd25;
	shl.b64 	%rd32, %rd31, 3;
	and.b64  	%rd33, %rd32, 34359738360;
	add.s64 	%rd5, %rd3, %rd33;
	ld.global.u64 	%rd6, [%rd5];
	setp.lt.u64 	%p6, %rd6, 2;
	@%p6 bra 	$L__BB27_4;

	shl.b64 	%rd34, %rd25, 3;
	add.s64 	%rd35, %rd5, %rd34;
	ld.global.u64 	%rd36, [%rd35];
	setp.ne.s64 	%p8, %rd58, %rd36;
	mov.pred 	%p19, 0;
	@%p8 bra 	$L__BB27_5;

$L__BB27_4:
	mul.lo.s64 	%rd58, %rd6, %rd58;
	add.s32 	%r34, %r34, 1;
	cvt.u64.u32 	%rd37, %r34;
	setp.lt.u64 	%p10, %rd37, %rd25;
	mov.pred 	%p19, %p2;
	@%p10 bra 	$L__BB27_2;

$L__BB27_5:
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r21, %ctaid.x;
	mov.u32 	%r22, %tid.x;
	mad.lo.s32 	%r35, %r21, %r3, %r22;
	cvt.u64.u32 	%rd59, %r35;
	@%p19 bra 	$L__BB27_15;
	bra.uni 	$L__BB27_6;

$L__BB27_15:
	setp.ge.u64 	%p17, %rd59, %rd24;
	@%p17 bra 	$L__BB27_18;

	mov.u32 	%r33, %nctaid.x;
	mul.lo.s32 	%r16, %r3, %r33;

$L__BB27_17:
	shl.b64 	%rd55, %rd59, 2;
	add.s64 	%rd56, %rd2, %rd55;
	ld.global.f32 	%f3, [%rd56];
	add.s64 	%rd57, %rd1, %rd55;
	st.global.f32 	[%rd57], %f3;
	add.s32 	%r35, %r35, %r16;
	cvt.u64.u32 	%rd59, %r35;
	setp.lt.u64 	%p18, %rd59, %rd24;
	@%p18 bra 	$L__BB27_17;
	bra.uni 	$L__BB27_18;

$L__BB27_6:
	setp.ge.u64 	%p11, %rd59, %rd24;
	@%p11 bra 	$L__BB27_18;

	mov.u32 	%r23, %nctaid.x;
	mul.lo.s32 	%r5, %r3, %r23;
	@%p3 bra 	$L__BB27_14;

$L__BB27_8:
	mov.u32 	%r36, 0;
	mov.u32 	%r37, %r35;
	mov.u32 	%r38, %r36;

$L__BB27_9:
	not.b32 	%r26, %r36;
	cvt.u64.u32 	%rd38, %r26;
	add.s64 	%rd39, %rd38, %rd25;
	cvt.u64.u32 	%rd10, %r37;
	shl.b64 	%rd40, %rd39, 3;
	and.b64  	%rd41, %rd40, 34359738360;
	add.s64 	%rd11, %rd3, %rd41;
	ld.global.u64 	%rd12, [%rd11];
	and.b64  	%rd42, %rd12, -4294967296;
	setp.eq.s64 	%p13, %rd42, 0;
	@%p13 bra 	$L__BB27_11;

	div.u64 	%rd60, %rd10, %rd12;
	mul.lo.s64 	%rd43, %rd60, %rd12;
	sub.s64 	%rd61, %rd10, %rd43;
	bra.uni 	$L__BB27_12;

$L__BB27_11:
	cvt.u32.u64 	%r27, %rd12;
	cvt.u32.u64 	%r28, %rd10;
	div.u32 	%r29, %r28, %r27;
	mul.lo.s32 	%r30, %r29, %r27;
	sub.s32 	%r31, %r28, %r30;
	cvt.u64.u32 	%rd60, %r29;
	cvt.u64.u32 	%rd61, %r31;

$L__BB27_12:
	shl.b64 	%rd44, %rd25, 3;
	add.s64 	%rd45, %rd11, %rd44;
	ld.global.u64 	%rd46, [%rd45];
	mul.lo.s64 	%rd47, %rd46, %rd61;
	cvt.u32.u64 	%r32, %rd47;
	add.s32 	%r38, %r38, %r32;
	cvt.u32.u64 	%r37, %rd60;
	add.s32 	%r36, %r36, 1;
	cvt.u64.u32 	%rd48, %r36;
	setp.lt.u64 	%p14, %rd48, %rd25;
	@%p14 bra 	$L__BB27_9;

	mul.wide.u32 	%rd49, %r38, 4;
	add.s64 	%rd50, %rd2, %rd49;
	ld.global.f32 	%f1, [%rd50];
	shl.b64 	%rd51, %rd59, 2;
	add.s64 	%rd52, %rd1, %rd51;
	st.global.f32 	[%rd52], %f1;
	add.s32 	%r35, %r35, %r5;
	cvt.u64.u32 	%rd59, %r35;
	setp.lt.u64 	%p15, %rd59, %rd24;
	@%p15 bra 	$L__BB27_8;
	bra.uni 	$L__BB27_18;

$L__BB27_14:
	ld.global.f32 	%f2, [%rd2];
	shl.b64 	%rd53, %rd59, 2;
	add.s64 	%rd54, %rd1, %rd53;
	st.global.f32 	[%rd54], %f2;
	add.s32 	%r35, %r35, %r5;
	cvt.u64.u32 	%rd59, %r35;
	setp.lt.u64 	%p16, %rd59, %rd24;
	@%p16 bra 	$L__BB27_14;

$L__BB27_18:
	ret;

}
	// .globl	cast_f32_f64
.visible .entry cast_f32_f64(
	.param .u64 cast_f32_f64_param_0,
	.param .u64 cast_f32_f64_param_1,
	.param .u64 cast_f32_f64_param_2,
	.param .u64 cast_f32_f64_param_3,
	.param .u64 cast_f32_f64_param_4
)
{
	.reg .pred 	%p<20>;
	.reg .f32 	%f<4>;
	.reg .b32 	%r<41>;
	.reg .f64 	%fd<4>;
	.reg .b64 	%rd<65>;


	ld.param.u64 	%rd24, [cast_f32_f64_param_0];
	ld.param.u64 	%rd25, [cast_f32_f64_param_1];
	ld.param.u64 	%rd26, [cast_f32_f64_param_2];
	ld.param.u64 	%rd27, [cast_f32_f64_param_3];
	ld.param.u64 	%rd28, [cast_f32_f64_param_4];
	cvta.to.global.u64 	%rd1, %rd28;
	cvta.to.global.u64 	%rd2, %rd27;
	cvta.to.global.u64 	%rd3, %rd26;
	setp.eq.s64 	%p3, %rd25, 0;
	setp.eq.s64 	%p4, %rd26, 0;
	or.pred  	%p5, %p4, %p3;
	mov.pred 	%p2, -1;
	mov.pred 	%p19, %p2;
	@%p5 bra 	$L__BB28_5;

	mov.u64 	%rd59, 1;
	mov.u32 	%r34, 0;

$L__BB28_2:
	not.b32 	%r20, %r34;
	cvt.u64.u32 	%rd30, %r20;
	add.s64 	%rd31, %rd30, %rd25;
	shl.b64 	%rd32, %rd31, 3;
	and.b64  	%rd33, %rd32, 34359738360;
	add.s64 	%rd5, %rd3, %rd33;
	ld.global.u64 	%rd6, [%rd5];
	setp.lt.u64 	%p6, %rd6, 2;
	@%p6 bra 	$L__BB28_4;

	shl.b64 	%rd34, %rd25, 3;
	add.s64 	%rd35, %rd5, %rd34;
	ld.global.u64 	%rd36, [%rd35];
	setp.ne.s64 	%p8, %rd59, %rd36;
	mov.pred 	%p19, 0;
	@%p8 bra 	$L__BB28_5;

$L__BB28_4:
	mul.lo.s64 	%rd59, %rd6, %rd59;
	add.s32 	%r34, %r34, 1;
	cvt.u64.u32 	%rd37, %r34;
	setp.lt.u64 	%p10, %rd37, %rd25;
	mov.pred 	%p19, %p2;
	@%p10 bra 	$L__BB28_2;

$L__BB28_5:
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r21, %ctaid.x;
	mov.u32 	%r22, %tid.x;
	mad.lo.s32 	%r35, %r21, %r3, %r22;
	cvt.u64.u32 	%rd60, %r35;
	@%p19 bra 	$L__BB28_15;
	bra.uni 	$L__BB28_6;

$L__BB28_15:
	setp.ge.u64 	%p17, %rd60, %rd24;
	@%p17 bra 	$L__BB28_18;

	mov.u32 	%r33, %nctaid.x;
	mul.lo.s32 	%r16, %r3, %r33;

$L__BB28_17:
	shl.b64 	%rd55, %rd60, 2;
	add.s64 	%rd56, %rd2, %rd55;
	ld.global.f32 	%f3, [%rd56];
	cvt.f64.f32 	%fd3, %f3;
	shl.b64 	%rd57, %rd60, 3;
	add.s64 	%rd58, %rd1, %rd57;
	st.global.f64 	[%rd58], %fd3;
	add.s32 	%r35, %r35, %r16;
	cvt.u64.u32 	%rd60, %r35;
	setp.lt.u64 	%p18, %rd60, %rd24;
	@%p18 bra 	$L__BB28_17;
	bra.uni 	$L__BB28_18;

$L__BB28_6:
	setp.ge.u64 	%p11, %rd60, %rd24;
	@%p11 bra 	$L__BB28_18;

	mov.u32 	%r23, %nctaid.x;
	mul.lo.s32 	%r5, %r3, %r23;
	@%p3 bra 	$L__BB28_14;

$L__BB28_8:
	mov.u32 	%r36, 0;
	mov.u32 	%r37, %r35;
	mov.u32 	%r38, %r36;

$L__BB28_9:
	not.b32 	%r26, %r36;
	cvt.u64.u32 	%rd38, %r26;
	add.s64 	%rd39, %rd38, %rd25;
	cvt.u64.u32 	%rd10, %r37;
	shl.b64 	%rd40, %rd39, 3;
	and.b64  	%rd41, %rd40, 34359738360;
	add.s64 	%rd11, %rd3, %rd41;
	ld.global.u64 	%rd12, [%rd11];
	and.b64  	%rd42, %rd12, -4294967296;
	setp.eq.s64 	%p13, %rd42, 0;
	@%p13 bra 	$L__BB28_11;

	div.u64 	%rd61, %rd10, %rd12;
	mul.lo.s64 	%rd43, %rd61, %rd12;
	sub.s64 	%rd62, %rd10, %rd43;
	bra.uni 	$L__BB28_12;

$L__BB28_11:
	cvt.u32.u64 	%r27, %rd12;
	cvt.u32.u64 	%r28, %rd10;
	div.u32 	%r29, %r28, %r27;
	mul.lo.s32 	%r30, %r29, %r27;
	sub.s32 	%r31, %r28, %r30;
	cvt.u64.u32 	%rd61, %r29;
	cvt.u64.u32 	%rd62, %r31;

$L__BB28_12:
	shl.b64 	%rd44, %rd25, 3;
	add.s64 	%rd45, %rd11, %rd44;
	ld.global.u64 	%rd46, [%rd45];
	mul.lo.s64 	%rd47, %rd46, %rd62;
	cvt.u32.u64 	%r32, %rd47;
	add.s32 	%r38, %r38, %r32;
	cvt.u32.u64 	%r37, %rd61;
	add.s32 	%r36, %r36, 1;
	cvt.u64.u32 	%rd48, %r36;
	setp.lt.u64 	%p14, %rd48, %rd25;
	@%p14 bra 	$L__BB28_9;

	mul.wide.u32 	%rd49, %r38, 4;
	add.s64 	%rd50, %rd2, %rd49;
	ld.global.f32 	%f1, [%rd50];
	cvt.f64.f32 	%fd1, %f1;
	shl.b64 	%rd51, %rd60, 3;
	add.s64 	%rd52, %rd1, %rd51;
	st.global.f64 	[%rd52], %fd1;
	add.s32 	%r35, %r35, %r5;
	cvt.u64.u32 	%rd60, %r35;
	setp.lt.u64 	%p15, %rd60, %rd24;
	@%p15 bra 	$L__BB28_8;
	bra.uni 	$L__BB28_18;

$L__BB28_14:
	ld.global.f32 	%f2, [%rd2];
	cvt.f64.f32 	%fd2, %f2;
	shl.b64 	%rd53, %rd60, 3;
	add.s64 	%rd54, %rd1, %rd53;
	st.global.f64 	[%rd54], %fd2;
	add.s32 	%r35, %r35, %r5;
	cvt.u64.u32 	%rd60, %r35;
	setp.lt.u64 	%p16, %rd60, %rd24;
	@%p16 bra 	$L__BB28_14;

$L__BB28_18:
	ret;

}
	// .globl	cast_f64_u8
.visible .entry cast_f64_u8(
	.param .u64 cast_f64_u8_param_0,
	.param .u64 cast_f64_u8_param_1,
	.param .u64 cast_f64_u8_param_2,
	.param .u64 cast_f64_u8_param_3,
	.param .u64 cast_f64_u8_param_4
)
{
	.reg .pred 	%p<20>;
	.reg .b32 	%r<44>;
	.reg .f64 	%fd<4>;
	.reg .b64 	%rd<62>;


	ld.param.u64 	%rd24, [cast_f64_u8_param_0];
	ld.param.u64 	%rd25, [cast_f64_u8_param_1];
	ld.param.u64 	%rd26, [cast_f64_u8_param_2];
	ld.param.u64 	%rd27, [cast_f64_u8_param_3];
	ld.param.u64 	%rd28, [cast_f64_u8_param_4];
	cvta.to.global.u64 	%rd1, %rd28;
	cvta.to.global.u64 	%rd2, %rd27;
	cvta.to.global.u64 	%rd3, %rd26;
	setp.eq.s64 	%p3, %rd25, 0;
	setp.eq.s64 	%p4, %rd26, 0;
	or.pred  	%p5, %p4, %p3;
	mov.pred 	%p2, -1;
	mov.pred 	%p19, %p2;
	@%p5 bra 	$L__BB29_5;

	mov.u64 	%rd56, 1;
	mov.u32 	%r37, 0;

$L__BB29_2:
	not.b32 	%r20, %r37;
	cvt.u64.u32 	%rd30, %r20;
	add.s64 	%rd31, %rd30, %rd25;
	shl.b64 	%rd32, %rd31, 3;
	and.b64  	%rd33, %rd32, 34359738360;
	add.s64 	%rd5, %rd3, %rd33;
	ld.global.u64 	%rd6, [%rd5];
	setp.lt.u64 	%p6, %rd6, 2;
	@%p6 bra 	$L__BB29_4;

	shl.b64 	%rd34, %rd25, 3;
	add.s64 	%rd35, %rd5, %rd34;
	ld.global.u64 	%rd36, [%rd35];
	setp.ne.s64 	%p8, %rd56, %rd36;
	mov.pred 	%p19, 0;
	@%p8 bra 	$L__BB29_5;

$L__BB29_4:
	mul.lo.s64 	%rd56, %rd6, %rd56;
	add.s32 	%r37, %r37, 1;
	cvt.u64.u32 	%rd37, %r37;
	setp.lt.u64 	%p10, %rd37, %rd25;
	mov.pred 	%p19, %p2;
	@%p10 bra 	$L__BB29_2;

$L__BB29_5:
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r21, %ctaid.x;
	mov.u32 	%r22, %tid.x;
	mad.lo.s32 	%r38, %r21, %r3, %r22;
	cvt.u64.u32 	%rd57, %r38;
	@%p19 bra 	$L__BB29_15;
	bra.uni 	$L__BB29_6;

$L__BB29_15:
	setp.ge.u64 	%p17, %rd57, %rd24;
	@%p17 bra 	$L__BB29_18;

	mov.u32 	%r35, %nctaid.x;
	mul.lo.s32 	%r16, %r3, %r35;

$L__BB29_17:
	shl.b64 	%rd53, %rd57, 3;
	add.s64 	%rd54, %rd2, %rd53;
	ld.global.f64 	%fd3, [%rd54];
	cvt.rzi.u32.f64 	%r36, %fd3;
	add.s64 	%rd55, %rd1, %rd57;
	st.global.u8 	[%rd55], %r36;
	add.s32 	%r38, %r38, %r16;
	cvt.u64.u32 	%rd57, %r38;
	setp.lt.u64 	%p18, %rd57, %rd24;
	@%p18 bra 	$L__BB29_17;
	bra.uni 	$L__BB29_18;

$L__BB29_6:
	setp.ge.u64 	%p11, %rd57, %rd24;
	@%p11 bra 	$L__BB29_18;

	mov.u32 	%r23, %nctaid.x;
	mul.lo.s32 	%r5, %r3, %r23;
	@%p3 bra 	$L__BB29_14;

$L__BB29_8:
	mov.u32 	%r39, 0;
	mov.u32 	%r40, %r38;
	mov.u32 	%r41, %r39;

$L__BB29_9:
	not.b32 	%r26, %r39;
	cvt.u64.u32 	%rd38, %r26;
	add.s64 	%rd39, %rd38, %rd25;
	cvt.u64.u32 	%rd10, %r40;
	shl.b64 	%rd40, %rd39, 3;
	and.b64  	%rd41, %rd40, 34359738360;
	add.s64 	%rd11, %rd3, %rd41;
	ld.global.u64 	%rd12, [%rd11];
	and.b64  	%rd42, %rd12, -4294967296;
	setp.eq.s64 	%p13, %rd42, 0;
	@%p13 bra 	$L__BB29_11;

	div.u64 	%rd58, %rd10, %rd12;
	mul.lo.s64 	%rd43, %rd58, %rd12;
	sub.s64 	%rd59, %rd10, %rd43;
	bra.uni 	$L__BB29_12;

$L__BB29_11:
	cvt.u32.u64 	%r27, %rd12;
	cvt.u32.u64 	%r28, %rd10;
	div.u32 	%r29, %r28, %r27;
	mul.lo.s32 	%r30, %r29, %r27;
	sub.s32 	%r31, %r28, %r30;
	cvt.u64.u32 	%rd58, %r29;
	cvt.u64.u32 	%rd59, %r31;

$L__BB29_12:
	shl.b64 	%rd44, %rd25, 3;
	add.s64 	%rd45, %rd11, %rd44;
	ld.global.u64 	%rd46, [%rd45];
	mul.lo.s64 	%rd47, %rd46, %rd59;
	cvt.u32.u64 	%r32, %rd47;
	add.s32 	%r41, %r41, %r32;
	cvt.u32.u64 	%r40, %rd58;
	add.s32 	%r39, %r39, 1;
	cvt.u64.u32 	%rd48, %r39;
	setp.lt.u64 	%p14, %rd48, %rd25;
	@%p14 bra 	$L__BB29_9;

	mul.wide.u32 	%rd49, %r41, 8;
	add.s64 	%rd50, %rd2, %rd49;
	ld.global.f64 	%fd1, [%rd50];
	cvt.rzi.u32.f64 	%r33, %fd1;
	add.s64 	%rd51, %rd1, %rd57;
	st.global.u8 	[%rd51], %r33;
	add.s32 	%r38, %r38, %r5;
	cvt.u64.u32 	%rd57, %r38;
	setp.lt.u64 	%p15, %rd57, %rd24;
	@%p15 bra 	$L__BB29_8;
	bra.uni 	$L__BB29_18;

$L__BB29_14:
	ld.global.f64 	%fd2, [%rd2];
	cvt.rzi.u32.f64 	%r34, %fd2;
	add.s64 	%rd52, %rd1, %rd57;
	st.global.u8 	[%rd52], %r34;
	add.s32 	%r38, %r38, %r5;
	cvt.u64.u32 	%rd57, %r38;
	setp.lt.u64 	%p16, %rd57, %rd24;
	@%p16 bra 	$L__BB29_14;

$L__BB29_18:
	ret;

}
	// .globl	cast_f64_u32
.visible .entry cast_f64_u32(
	.param .u64 cast_f64_u32_param_0,
	.param .u64 cast_f64_u32_param_1,
	.param .u64 cast_f64_u32_param_2,
	.param .u64 cast_f64_u32_param_3,
	.param .u64 cast_f64_u32_param_4
)
{
	.reg .pred 	%p<20>;
	.reg .b32 	%r<44>;
	.reg .f64 	%fd<4>;
	.reg .b64 	%rd<65>;


	ld.param.u64 	%rd24, [cast_f64_u32_param_0];
	ld.param.u64 	%rd25, [cast_f64_u32_param_1];
	ld.param.u64 	%rd26, [cast_f64_u32_param_2];
	ld.param.u64 	%rd27, [cast_f64_u32_param_3];
	ld.param.u64 	%rd28, [cast_f64_u32_param_4];
	cvta.to.global.u64 	%rd1, %rd28;
	cvta.to.global.u64 	%rd2, %rd27;
	cvta.to.global.u64 	%rd3, %rd26;
	setp.eq.s64 	%p3, %rd25, 0;
	setp.eq.s64 	%p4, %rd26, 0;
	or.pred  	%p5, %p4, %p3;
	mov.pred 	%p2, -1;
	mov.pred 	%p19, %p2;
	@%p5 bra 	$L__BB30_5;

	mov.u64 	%rd59, 1;
	mov.u32 	%r37, 0;

$L__BB30_2:
	not.b32 	%r20, %r37;
	cvt.u64.u32 	%rd30, %r20;
	add.s64 	%rd31, %rd30, %rd25;
	shl.b64 	%rd32, %rd31, 3;
	and.b64  	%rd33, %rd32, 34359738360;
	add.s64 	%rd5, %rd3, %rd33;
	ld.global.u64 	%rd6, [%rd5];
	setp.lt.u64 	%p6, %rd6, 2;
	@%p6 bra 	$L__BB30_4;

	shl.b64 	%rd34, %rd25, 3;
	add.s64 	%rd35, %rd5, %rd34;
	ld.global.u64 	%rd36, [%rd35];
	setp.ne.s64 	%p8, %rd59, %rd36;
	mov.pred 	%p19, 0;
	@%p8 bra 	$L__BB30_5;

$L__BB30_4:
	mul.lo.s64 	%rd59, %rd6, %rd59;
	add.s32 	%r37, %r37, 1;
	cvt.u64.u32 	%rd37, %r37;
	setp.lt.u64 	%p10, %rd37, %rd25;
	mov.pred 	%p19, %p2;
	@%p10 bra 	$L__BB30_2;

$L__BB30_5:
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r21, %ctaid.x;
	mov.u32 	%r22, %tid.x;
	mad.lo.s32 	%r38, %r21, %r3, %r22;
	cvt.u64.u32 	%rd60, %r38;
	@%p19 bra 	$L__BB30_15;
	bra.uni 	$L__BB30_6;

$L__BB30_15:
	setp.ge.u64 	%p17, %rd60, %rd24;
	@%p17 bra 	$L__BB30_18;

	mov.u32 	%r35, %nctaid.x;
	mul.lo.s32 	%r16, %r3, %r35;

$L__BB30_17:
	shl.b64 	%rd55, %rd60, 3;
	add.s64 	%rd56, %rd2, %rd55;
	ld.global.f64 	%fd3, [%rd56];
	cvt.rzi.u32.f64 	%r36, %fd3;
	shl.b64 	%rd57, %rd60, 2;
	add.s64 	%rd58, %rd1, %rd57;
	st.global.u32 	[%rd58], %r36;
	add.s32 	%r38, %r38, %r16;
	cvt.u64.u32 	%rd60, %r38;
	setp.lt.u64 	%p18, %rd60, %rd24;
	@%p18 bra 	$L__BB30_17;
	bra.uni 	$L__BB30_18;

$L__BB30_6:
	setp.ge.u64 	%p11, %rd60, %rd24;
	@%p11 bra 	$L__BB30_18;

	mov.u32 	%r23, %nctaid.x;
	mul.lo.s32 	%r5, %r3, %r23;
	@%p3 bra 	$L__BB30_14;

$L__BB30_8:
	mov.u32 	%r39, 0;
	mov.u32 	%r40, %r38;
	mov.u32 	%r41, %r39;

$L__BB30_9:
	not.b32 	%r26, %r39;
	cvt.u64.u32 	%rd38, %r26;
	add.s64 	%rd39, %rd38, %rd25;
	cvt.u64.u32 	%rd10, %r40;
	shl.b64 	%rd40, %rd39, 3;
	and.b64  	%rd41, %rd40, 34359738360;
	add.s64 	%rd11, %rd3, %rd41;
	ld.global.u64 	%rd12, [%rd11];
	and.b64  	%rd42, %rd12, -4294967296;
	setp.eq.s64 	%p13, %rd42, 0;
	@%p13 bra 	$L__BB30_11;

	div.u64 	%rd61, %rd10, %rd12;
	mul.lo.s64 	%rd43, %rd61, %rd12;
	sub.s64 	%rd62, %rd10, %rd43;
	bra.uni 	$L__BB30_12;

$L__BB30_11:
	cvt.u32.u64 	%r27, %rd12;
	cvt.u32.u64 	%r28, %rd10;
	div.u32 	%r29, %r28, %r27;
	mul.lo.s32 	%r30, %r29, %r27;
	sub.s32 	%r31, %r28, %r30;
	cvt.u64.u32 	%rd61, %r29;
	cvt.u64.u32 	%rd62, %r31;

$L__BB30_12:
	shl.b64 	%rd44, %rd25, 3;
	add.s64 	%rd45, %rd11, %rd44;
	ld.global.u64 	%rd46, [%rd45];
	mul.lo.s64 	%rd47, %rd46, %rd62;
	cvt.u32.u64 	%r32, %rd47;
	add.s32 	%r41, %r41, %r32;
	cvt.u32.u64 	%r40, %rd61;
	add.s32 	%r39, %r39, 1;
	cvt.u64.u32 	%rd48, %r39;
	setp.lt.u64 	%p14, %rd48, %rd25;
	@%p14 bra 	$L__BB30_9;

	mul.wide.u32 	%rd49, %r41, 8;
	add.s64 	%rd50, %rd2, %rd49;
	ld.global.f64 	%fd1, [%rd50];
	cvt.rzi.u32.f64 	%r33, %fd1;
	shl.b64 	%rd51, %rd60, 2;
	add.s64 	%rd52, %rd1, %rd51;
	st.global.u32 	[%rd52], %r33;
	add.s32 	%r38, %r38, %r5;
	cvt.u64.u32 	%rd60, %r38;
	setp.lt.u64 	%p15, %rd60, %rd24;
	@%p15 bra 	$L__BB30_8;
	bra.uni 	$L__BB30_18;

$L__BB30_14:
	ld.global.f64 	%fd2, [%rd2];
	cvt.rzi.u32.f64 	%r34, %fd2;
	shl.b64 	%rd53, %rd60, 2;
	add.s64 	%rd54, %rd1, %rd53;
	st.global.u32 	[%rd54], %r34;
	add.s32 	%r38, %r38, %r5;
	cvt.u64.u32 	%rd60, %r38;
	setp.lt.u64 	%p16, %rd60, %rd24;
	@%p16 bra 	$L__BB30_14;

$L__BB30_18:
	ret;

}
	// .globl	cast_f64_i64
.visible .entry cast_f64_i64(
	.param .u64 cast_f64_i64_param_0,
	.param .u64 cast_f64_i64_param_1,
	.param .u64 cast_f64_i64_param_2,
	.param .u64 cast_f64_i64_param_3,
	.param .u64 cast_f64_i64_param_4
)
{
	.reg .pred 	%p<20>;
	.reg .b32 	%r<41>;
	.reg .f64 	%fd<4>;
	.reg .b64 	%rd<67>;


	ld.param.u64 	%rd24, [cast_f64_i64_param_0];
	ld.param.u64 	%rd25, [cast_f64_i64_param_1];
	ld.param.u64 	%rd26, [cast_f64_i64_param_2];
	ld.param.u64 	%rd27, [cast_f64_i64_param_3];
	ld.param.u64 	%rd28, [cast_f64_i64_param_4];
	cvta.to.global.u64 	%rd1, %rd28;
	cvta.to.global.u64 	%rd2, %rd27;
	cvta.to.global.u64 	%rd3, %rd26;
	setp.eq.s64 	%p3, %rd25, 0;
	setp.eq.s64 	%p4, %rd26, 0;
	or.pred  	%p5, %p4, %p3;
	mov.pred 	%p2, -1;
	mov.pred 	%p19, %p2;
	@%p5 bra 	$L__BB31_5;

	mov.u64 	%rd61, 1;
	mov.u32 	%r34, 0;

$L__BB31_2:
	not.b32 	%r20, %r34;
	cvt.u64.u32 	%rd30, %r20;
	add.s64 	%rd31, %rd30, %rd25;
	shl.b64 	%rd32, %rd31, 3;
	and.b64  	%rd33, %rd32, 34359738360;
	add.s64 	%rd5, %rd3, %rd33;
	ld.global.u64 	%rd6, [%rd5];
	setp.lt.u64 	%p6, %rd6, 2;
	@%p6 bra 	$L__BB31_4;

	shl.b64 	%rd34, %rd25, 3;
	add.s64 	%rd35, %rd5, %rd34;
	ld.global.u64 	%rd36, [%rd35];
	setp.ne.s64 	%p8, %rd61, %rd36;
	mov.pred 	%p19, 0;
	@%p8 bra 	$L__BB31_5;

$L__BB31_4:
	mul.lo.s64 	%rd61, %rd6, %rd61;
	add.s32 	%r34, %r34, 1;
	cvt.u64.u32 	%rd37, %r34;
	setp.lt.u64 	%p10, %rd37, %rd25;
	mov.pred 	%p19, %p2;
	@%p10 bra 	$L__BB31_2;

$L__BB31_5:
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r21, %ctaid.x;
	mov.u32 	%r22, %tid.x;
	mad.lo.s32 	%r35, %r21, %r3, %r22;
	cvt.u64.u32 	%rd62, %r35;
	@%p19 bra 	$L__BB31_15;
	bra.uni 	$L__BB31_6;

$L__BB31_15:
	setp.ge.u64 	%p17, %rd62, %rd24;
	@%p17 bra 	$L__BB31_18;

	mov.u32 	%r33, %nctaid.x;
	mul.lo.s32 	%r16, %r3, %r33;

$L__BB31_17:
	shl.b64 	%rd57, %rd62, 3;
	add.s64 	%rd58, %rd2, %rd57;
	ld.global.f64 	%fd3, [%rd58];
	cvt.rzi.s64.f64 	%rd59, %fd3;
	add.s64 	%rd60, %rd1, %rd57;
	st.global.u64 	[%rd60], %rd59;
	add.s32 	%r35, %r35, %r16;
	cvt.u64.u32 	%rd62, %r35;
	setp.lt.u64 	%p18, %rd62, %rd24;
	@%p18 bra 	$L__BB31_17;
	bra.uni 	$L__BB31_18;

$L__BB31_6:
	setp.ge.u64 	%p11, %rd62, %rd24;
	@%p11 bra 	$L__BB31_18;

	mov.u32 	%r23, %nctaid.x;
	mul.lo.s32 	%r5, %r3, %r23;
	@%p3 bra 	$L__BB31_14;

$L__BB31_8:
	mov.u32 	%r36, 0;
	mov.u32 	%r37, %r35;
	mov.u32 	%r38, %r36;

$L__BB31_9:
	not.b32 	%r26, %r36;
	cvt.u64.u32 	%rd38, %r26;
	add.s64 	%rd39, %rd38, %rd25;
	cvt.u64.u32 	%rd10, %r37;
	shl.b64 	%rd40, %rd39, 3;
	and.b64  	%rd41, %rd40, 34359738360;
	add.s64 	%rd11, %rd3, %rd41;
	ld.global.u64 	%rd12, [%rd11];
	and.b64  	%rd42, %rd12, -4294967296;
	setp.eq.s64 	%p13, %rd42, 0;
	@%p13 bra 	$L__BB31_11;

	div.u64 	%rd63, %rd10, %rd12;
	mul.lo.s64 	%rd43, %rd63, %rd12;
	sub.s64 	%rd64, %rd10, %rd43;
	bra.uni 	$L__BB31_12;

$L__BB31_11:
	cvt.u32.u64 	%r27, %rd12;
	cvt.u32.u64 	%r28, %rd10;
	div.u32 	%r29, %r28, %r27;
	mul.lo.s32 	%r30, %r29, %r27;
	sub.s32 	%r31, %r28, %r30;
	cvt.u64.u32 	%rd63, %r29;
	cvt.u64.u32 	%rd64, %r31;

$L__BB31_12:
	shl.b64 	%rd44, %rd25, 3;
	add.s64 	%rd45, %rd11, %rd44;
	ld.global.u64 	%rd46, [%rd45];
	mul.lo.s64 	%rd47, %rd46, %rd64;
	cvt.u32.u64 	%r32, %rd47;
	add.s32 	%r38, %r38, %r32;
	cvt.u32.u64 	%r37, %rd63;
	add.s32 	%r36, %r36, 1;
	cvt.u64.u32 	%rd48, %r36;
	setp.lt.u64 	%p14, %rd48, %rd25;
	@%p14 bra 	$L__BB31_9;

	mul.wide.u32 	%rd49, %r38, 8;
	add.s64 	%rd50, %rd2, %rd49;
	ld.global.f64 	%fd1, [%rd50];
	cvt.rzi.s64.f64 	%rd51, %fd1;
	shl.b64 	%rd52, %rd62, 3;
	add.s64 	%rd53, %rd1, %rd52;
	st.global.u64 	[%rd53], %rd51;
	add.s32 	%r35, %r35, %r5;
	cvt.u64.u32 	%rd62, %r35;
	setp.lt.u64 	%p15, %rd62, %rd24;
	@%p15 bra 	$L__BB31_8;
	bra.uni 	$L__BB31_18;

$L__BB31_14:
	ld.global.f64 	%fd2, [%rd2];
	cvt.rzi.s64.f64 	%rd54, %fd2;
	shl.b64 	%rd55, %rd62, 3;
	add.s64 	%rd56, %rd1, %rd55;
	st.global.u64 	[%rd56], %rd54;
	add.s32 	%r35, %r35, %r5;
	cvt.u64.u32 	%rd62, %r35;
	setp.lt.u64 	%p16, %rd62, %rd24;
	@%p16 bra 	$L__BB31_14;

$L__BB31_18:
	ret;

}
	// .globl	cast_f64_f32
.visible .entry cast_f64_f32(
	.param .u64 cast_f64_f32_param_0,
	.param .u64 cast_f64_f32_param_1,
	.param .u64 cast_f64_f32_param_2,
	.param .u64 cast_f64_f32_param_3,
	.param .u64 cast_f64_f32_param_4
)
{
	.reg .pred 	%p<20>;
	.reg .f32 	%f<4>;
	.reg .b32 	%r<41>;
	.reg .f64 	%fd<4>;
	.reg .b64 	%rd<65>;


	ld.param.u64 	%rd24, [cast_f64_f32_param_0];
	ld.param.u64 	%rd25, [cast_f64_f32_param_1];
	ld.param.u64 	%rd26, [cast_f64_f32_param_2];
	ld.param.u64 	%rd27, [cast_f64_f32_param_3];
	ld.param.u64 	%rd28, [cast_f64_f32_param_4];
	cvta.to.global.u64 	%rd1, %rd28;
	cvta.to.global.u64 	%rd2, %rd27;
	cvta.to.global.u64 	%rd3, %rd26;
	setp.eq.s64 	%p3, %rd25, 0;
	setp.eq.s64 	%p4, %rd26, 0;
	or.pred  	%p5, %p4, %p3;
	mov.pred 	%p2, -1;
	mov.pred 	%p19, %p2;
	@%p5 bra 	$L__BB32_5;

	mov.u64 	%rd59, 1;
	mov.u32 	%r34, 0;

$L__BB32_2:
	not.b32 	%r20, %r34;
	cvt.u64.u32 	%rd30, %r20;
	add.s64 	%rd31, %rd30, %rd25;
	shl.b64 	%rd32, %rd31, 3;
	and.b64  	%rd33, %rd32, 34359738360;
	add.s64 	%rd5, %rd3, %rd33;
	ld.global.u64 	%rd6, [%rd5];
	setp.lt.u64 	%p6, %rd6, 2;
	@%p6 bra 	$L__BB32_4;

	shl.b64 	%rd34, %rd25, 3;
	add.s64 	%rd35, %rd5, %rd34;
	ld.global.u64 	%rd36, [%rd35];
	setp.ne.s64 	%p8, %rd59, %rd36;
	mov.pred 	%p19, 0;
	@%p8 bra 	$L__BB32_5;

$L__BB32_4:
	mul.lo.s64 	%rd59, %rd6, %rd59;
	add.s32 	%r34, %r34, 1;
	cvt.u64.u32 	%rd37, %r34;
	setp.lt.u64 	%p10, %rd37, %rd25;
	mov.pred 	%p19, %p2;
	@%p10 bra 	$L__BB32_2;

$L__BB32_5:
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r21, %ctaid.x;
	mov.u32 	%r22, %tid.x;
	mad.lo.s32 	%r35, %r21, %r3, %r22;
	cvt.u64.u32 	%rd60, %r35;
	@%p19 bra 	$L__BB32_15;
	bra.uni 	$L__BB32_6;

$L__BB32_15:
	setp.ge.u64 	%p17, %rd60, %rd24;
	@%p17 bra 	$L__BB32_18;

	mov.u32 	%r33, %nctaid.x;
	mul.lo.s32 	%r16, %r3, %r33;

$L__BB32_17:
	shl.b64 	%rd55, %rd60, 3;
	add.s64 	%rd56, %rd2, %rd55;
	ld.global.f64 	%fd3, [%rd56];
	cvt.rn.f32.f64 	%f3, %fd3;
	shl.b64 	%rd57, %rd60, 2;
	add.s64 	%rd58, %rd1, %rd57;
	st.global.f32 	[%rd58], %f3;
	add.s32 	%r35, %r35, %r16;
	cvt.u64.u32 	%rd60, %r35;
	setp.lt.u64 	%p18, %rd60, %rd24;
	@%p18 bra 	$L__BB32_17;
	bra.uni 	$L__BB32_18;

$L__BB32_6:
	setp.ge.u64 	%p11, %rd60, %rd24;
	@%p11 bra 	$L__BB32_18;

	mov.u32 	%r23, %nctaid.x;
	mul.lo.s32 	%r5, %r3, %r23;
	@%p3 bra 	$L__BB32_14;

$L__BB32_8:
	mov.u32 	%r36, 0;
	mov.u32 	%r37, %r35;
	mov.u32 	%r38, %r36;

$L__BB32_9:
	not.b32 	%r26, %r36;
	cvt.u64.u32 	%rd38, %r26;
	add.s64 	%rd39, %rd38, %rd25;
	cvt.u64.u32 	%rd10, %r37;
	shl.b64 	%rd40, %rd39, 3;
	and.b64  	%rd41, %rd40, 34359738360;
	add.s64 	%rd11, %rd3, %rd41;
	ld.global.u64 	%rd12, [%rd11];
	and.b64  	%rd42, %rd12, -4294967296;
	setp.eq.s64 	%p13, %rd42, 0;
	@%p13 bra 	$L__BB32_11;

	div.u64 	%rd61, %rd10, %rd12;
	mul.lo.s64 	%rd43, %rd61, %rd12;
	sub.s64 	%rd62, %rd10, %rd43;
	bra.uni 	$L__BB32_12;

$L__BB32_11:
	cvt.u32.u64 	%r27, %rd12;
	cvt.u32.u64 	%r28, %rd10;
	div.u32 	%r29, %r28, %r27;
	mul.lo.s32 	%r30, %r29, %r27;
	sub.s32 	%r31, %r28, %r30;
	cvt.u64.u32 	%rd61, %r29;
	cvt.u64.u32 	%rd62, %r31;

$L__BB32_12:
	shl.b64 	%rd44, %rd25, 3;
	add.s64 	%rd45, %rd11, %rd44;
	ld.global.u64 	%rd46, [%rd45];
	mul.lo.s64 	%rd47, %rd46, %rd62;
	cvt.u32.u64 	%r32, %rd47;
	add.s32 	%r38, %r38, %r32;
	cvt.u32.u64 	%r37, %rd61;
	add.s32 	%r36, %r36, 1;
	cvt.u64.u32 	%rd48, %r36;
	setp.lt.u64 	%p14, %rd48, %rd25;
	@%p14 bra 	$L__BB32_9;

	mul.wide.u32 	%rd49, %r38, 8;
	add.s64 	%rd50, %rd2, %rd49;
	ld.global.f64 	%fd1, [%rd50];
	cvt.rn.f32.f64 	%f1, %fd1;
	shl.b64 	%rd51, %rd60, 2;
	add.s64 	%rd52, %rd1, %rd51;
	st.global.f32 	[%rd52], %f1;
	add.s32 	%r35, %r35, %r5;
	cvt.u64.u32 	%rd60, %r35;
	setp.lt.u64 	%p15, %rd60, %rd24;
	@%p15 bra 	$L__BB32_8;
	bra.uni 	$L__BB32_18;

$L__BB32_14:
	ld.global.f64 	%fd2, [%rd2];
	cvt.rn.f32.f64 	%f2, %fd2;
	shl.b64 	%rd53, %rd60, 2;
	add.s64 	%rd54, %rd1, %rd53;
	st.global.f32 	[%rd54], %f2;
	add.s32 	%r35, %r35, %r5;
	cvt.u64.u32 	%rd60, %r35;
	setp.lt.u64 	%p16, %rd60, %rd24;
	@%p16 bra 	$L__BB32_14;

$L__BB32_18:
	ret;

}
	// .globl	cast_f64_f64
.visible .entry cast_f64_f64(
	.param .u64 cast_f64_f64_param_0,
	.param .u64 cast_f64_f64_param_1,
	.param .u64 cast_f64_f64_param_2,
	.param .u64 cast_f64_f64_param_3,
	.param .u64 cast_f64_f64_param_4
)
{
	.reg .pred 	%p<20>;
	.reg .b32 	%r<41>;
	.reg .f64 	%fd<4>;
	.reg .b64 	%rd<64>;


	ld.param.u64 	%rd24, [cast_f64_f64_param_0];
	ld.param.u64 	%rd25, [cast_f64_f64_param_1];
	ld.param.u64 	%rd26, [cast_f64_f64_param_2];
	ld.param.u64 	%rd27, [cast_f64_f64_param_3];
	ld.param.u64 	%rd28, [cast_f64_f64_param_4];
	cvta.to.global.u64 	%rd1, %rd28;
	cvta.to.global.u64 	%rd2, %rd27;
	cvta.to.global.u64 	%rd3, %rd26;
	setp.eq.s64 	%p3, %rd25, 0;
	setp.eq.s64 	%p4, %rd26, 0;
	or.pred  	%p5, %p4, %p3;
	mov.pred 	%p2, -1;
	mov.pred 	%p19, %p2;
	@%p5 bra 	$L__BB33_5;

	mov.u64 	%rd58, 1;
	mov.u32 	%r34, 0;

$L__BB33_2:
	not.b32 	%r20, %r34;
	cvt.u64.u32 	%rd30, %r20;
	add.s64 	%rd31, %rd30, %rd25;
	shl.b64 	%rd32, %rd31, 3;
	and.b64  	%rd33, %rd32, 34359738360;
	add.s64 	%rd5, %rd3, %rd33;
	ld.global.u64 	%rd6, [%rd5];
	setp.lt.u64 	%p6, %rd6, 2;
	@%p6 bra 	$L__BB33_4;

	shl.b64 	%rd34, %rd25, 3;
	add.s64 	%rd35, %rd5, %rd34;
	ld.global.u64 	%rd36, [%rd35];
	setp.ne.s64 	%p8, %rd58, %rd36;
	mov.pred 	%p19, 0;
	@%p8 bra 	$L__BB33_5;

$L__BB33_4:
	mul.lo.s64 	%rd58, %rd6, %rd58;
	add.s32 	%r34, %r34, 1;
	cvt.u64.u32 	%rd37, %r34;
	setp.lt.u64 	%p10, %rd37, %rd25;
	mov.pred 	%p19, %p2;
	@%p10 bra 	$L__BB33_2;

$L__BB33_5:
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r21, %ctaid.x;
	mov.u32 	%r22, %tid.x;
	mad.lo.s32 	%r35, %r21, %r3, %r22;
	cvt.u64.u32 	%rd59, %r35;
	@%p19 bra 	$L__BB33_15;
	bra.uni 	$L__BB33_6;

$L__BB33_15:
	setp.ge.u64 	%p17, %rd59, %rd24;
	@%p17 bra 	$L__BB33_18;

	mov.u32 	%r33, %nctaid.x;
	mul.lo.s32 	%r16, %r3, %r33;

$L__BB33_17:
	shl.b64 	%rd55, %rd59, 3;
	add.s64 	%rd56, %rd2, %rd55;
	ld.global.f64 	%fd3, [%rd56];
	add.s64 	%rd57, %rd1, %rd55;
	st.global.f64 	[%rd57], %fd3;
	add.s32 	%r35, %r35, %r16;
	cvt.u64.u32 	%rd59, %r35;
	setp.lt.u64 	%p18, %rd59, %rd24;
	@%p18 bra 	$L__BB33_17;
	bra.uni 	$L__BB33_18;

$L__BB33_6:
	setp.ge.u64 	%p11, %rd59, %rd24;
	@%p11 bra 	$L__BB33_18;

	mov.u32 	%r23, %nctaid.x;
	mul.lo.s32 	%r5, %r3, %r23;
	@%p3 bra 	$L__BB33_14;

$L__BB33_8:
	mov.u32 	%r36, 0;
	mov.u32 	%r37, %r35;
	mov.u32 	%r38, %r36;

$L__BB33_9:
	not.b32 	%r26, %r36;
	cvt.u64.u32 	%rd38, %r26;
	add.s64 	%rd39, %rd38, %rd25;
	cvt.u64.u32 	%rd10, %r37;
	shl.b64 	%rd40, %rd39, 3;
	and.b64  	%rd41, %rd40, 34359738360;
	add.s64 	%rd11, %rd3, %rd41;
	ld.global.u64 	%rd12, [%rd11];
	and.b64  	%rd42, %rd12, -4294967296;
	setp.eq.s64 	%p13, %rd42, 0;
	@%p13 bra 	$L__BB33_11;

	div.u64 	%rd60, %rd10, %rd12;
	mul.lo.s64 	%rd43, %rd60, %rd12;
	sub.s64 	%rd61, %rd10, %rd43;
	bra.uni 	$L__BB33_12;

$L__BB33_11:
	cvt.u32.u64 	%r27, %rd12;
	cvt.u32.u64 	%r28, %rd10;
	div.u32 	%r29, %r28, %r27;
	mul.lo.s32 	%r30, %r29, %r27;
	sub.s32 	%r31, %r28, %r30;
	cvt.u64.u32 	%rd60, %r29;
	cvt.u64.u32 	%rd61, %r31;

$L__BB33_12:
	shl.b64 	%rd44, %rd25, 3;
	add.s64 	%rd45, %rd11, %rd44;
	ld.global.u64 	%rd46, [%rd45];
	mul.lo.s64 	%rd47, %rd46, %rd61;
	cvt.u32.u64 	%r32, %rd47;
	add.s32 	%r38, %r38, %r32;
	cvt.u32.u64 	%r37, %rd60;
	add.s32 	%r36, %r36, 1;
	cvt.u64.u32 	%rd48, %r36;
	setp.lt.u64 	%p14, %rd48, %rd25;
	@%p14 bra 	$L__BB33_9;

	mul.wide.u32 	%rd49, %r38, 8;
	add.s64 	%rd50, %rd2, %rd49;
	ld.global.f64 	%fd1, [%rd50];
	shl.b64 	%rd51, %rd59, 3;
	add.s64 	%rd52, %rd1, %rd51;
	st.global.f64 	[%rd52], %fd1;
	add.s32 	%r35, %r35, %r5;
	cvt.u64.u32 	%rd59, %r35;
	setp.lt.u64 	%p15, %rd59, %rd24;
	@%p15 bra 	$L__BB33_8;
	bra.uni 	$L__BB33_18;

$L__BB33_14:
	ld.global.f64 	%fd2, [%rd2];
	shl.b64 	%rd53, %rd59, 3;
	add.s64 	%rd54, %rd1, %rd53;
	st.global.f64 	[%rd54], %fd2;
	add.s32 	%r35, %r35, %r5;
	cvt.u64.u32 	%rd59, %r35;
	setp.lt.u64 	%p16, %rd59, %rd24;
	@%p16 bra 	$L__BB33_14;

$L__BB33_18:
	ret;

}

