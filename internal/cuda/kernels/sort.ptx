//
// Generated by NVIDIA NVVM Compiler
//
// Compiler Build ID: CL-35813241
// Cuda compilation tools, release 12.9, V12.9.41
// Based on NVVM 7.0.1
//

.version 8.8
.target sm_52
.address_size 64

	// .globl	asort_asc_f32
.extern .shared .align 16 .b8 dst_row[];

.visible .entry asort_asc_f32(
	.param .u64 asort_asc_f32_param_0,
	.param .u64 asort_asc_f32_param_1,
	.param .u32 asort_asc_f32_param_2,
	.param .u32 asort_asc_f32_param_3
)
{
	.reg .pred 	%p<15>;
	.reg .f32 	%f<5>;
	.reg .b32 	%r<29>;
	.reg .b64 	%rd<24>;


	ld.param.u64 	%rd4, [asort_asc_f32_param_0];
	ld.param.u64 	%rd3, [asort_asc_f32_param_1];
	ld.param.u32 	%r15, [asort_asc_f32_param_2];
	ld.param.u32 	%r16, [asort_asc_f32_param_3];
	cvta.to.global.u64 	%rd1, %rd4;
	mov.u32 	%r1, %tid.x;
	setp.ge.s32 	%p1, %r1, %r16;
	@%p1 bra 	$L__BB0_19;

	mov.u32 	%r17, %ctaid.x;
	mul.lo.s32 	%r18, %r17, %r15;
	cvt.s64.s32 	%rd2, %r18;
	shl.b32 	%r19, %r1, 2;
	mov.u32 	%r20, dst_row;
	add.s32 	%r2, %r20, %r19;
	st.shared.u32 	[%r2], %r1;
	bar.sync 	0;
	setp.lt.s32 	%p2, %r16, 2;
	@%p2 bra 	$L__BB0_17;

	mov.u32 	%r27, 2;

$L__BB0_3:
	setp.eq.s32 	%p3, %r27, 0;
	@%p3 bra 	$L__BB0_16;

	shr.u32 	%r28, %r27, 1;
	and.b32  	%r5, %r27, %r1;

$L__BB0_5:
	xor.b32  	%r7, %r28, %r1;
	setp.le.s32 	%p4, %r7, %r1;
	@%p4 bra 	$L__BB0_15;

	setp.eq.s32 	%p5, %r5, 0;
	shl.b32 	%r22, %r7, 2;
	add.s32 	%r8, %r20, %r22;
	@%p5 bra 	$L__BB0_11;

	ld.shared.u32 	%r9, [%r8];
	setp.ge.s32 	%p6, %r9, %r15;
	ld.shared.u32 	%r10, [%r2];
	@%p6 bra 	$L__BB0_10;

	setp.ge.s32 	%p7, %r10, %r15;
	@%p7 bra 	$L__BB0_15;

	cvt.s64.s32 	%rd5, %r10;
	add.s64 	%rd6, %rd5, %rd2;
	shl.b64 	%rd7, %rd6, 2;
	add.s64 	%rd8, %rd1, %rd7;
	cvt.s64.s32 	%rd9, %r9;
	add.s64 	%rd10, %rd9, %rd2;
	shl.b64 	%rd11, %rd10, 2;
	add.s64 	%rd12, %rd1, %rd11;
	ld.global.f32 	%f1, [%rd12];
	ld.global.f32 	%f2, [%rd8];
	setp.geu.f32 	%p8, %f2, %f1;
	@%p8 bra 	$L__BB0_15;

$L__BB0_10:
	st.shared.u32 	[%r2], %r9;
	st.shared.u32 	[%r8], %r10;
	bra.uni 	$L__BB0_15;

$L__BB0_11:
	ld.shared.u32 	%r11, [%r2];
	setp.ge.s32 	%p9, %r11, %r15;
	ld.shared.u32 	%r12, [%r8];
	@%p9 bra 	$L__BB0_14;

	setp.ge.s32 	%p10, %r12, %r15;
	@%p10 bra 	$L__BB0_15;

	cvt.s64.s32 	%rd13, %r11;
	add.s64 	%rd14, %rd13, %rd2;
	shl.b64 	%rd15, %rd14, 2;
	add.s64 	%rd16, %rd1, %rd15;
	cvt.s64.s32 	%rd17, %r12;
	add.s64 	%rd18, %rd17, %rd2;
	shl.b64 	%rd19, %rd18, 2;
	add.s64 	%rd20, %rd1, %rd19;
	ld.global.f32 	%f3, [%rd20];
	ld.global.f32 	%f4, [%rd16];
	setp.leu.f32 	%p11, %f4, %f3;
	@%p11 bra 	$L__BB0_15;

$L__BB0_14:
	st.shared.u32 	[%r2], %r12;
	st.shared.u32 	[%r8], %r11;

$L__BB0_15:
	bar.sync 	0;
	shr.u32 	%r28, %r28, 1;
	setp.ne.s32 	%p12, %r28, 0;
	@%p12 bra 	$L__BB0_5;

$L__BB0_16:
	shl.b32 	%r27, %r27, 1;
	setp.le.s32 	%p13, %r27, %r16;
	@%p13 bra 	$L__BB0_3;

$L__BB0_17:
	setp.ge.s32 	%p14, %r1, %r15;
	@%p14 bra 	$L__BB0_19;

	cvt.u32.u64 	%r24, %rd2;
	ld.shared.u32 	%r25, [%r2];
	add.s32 	%r26, %r24, %r1;
	cvta.to.global.u64 	%rd21, %rd3;
	mul.wide.s32 	%rd22, %r26, 4;
	add.s64 	%rd23, %rd21, %rd22;
	st.global.u32 	[%rd23], %r25;

$L__BB0_19:
	ret;

}
	// .globl	asort_desc_f32
.visible .entry asort_desc_f32(
	.param .u64 asort_desc_f32_param_0,
	.param .u64 asort_desc_f32_param_1,
	.param .u32 asort_desc_f32_param_2,
	.param .u32 asort_desc_f32_param_3
)
{
	.reg .pred 	%p<15>;
	.reg .f32 	%f<5>;
	.reg .b32 	%r<29>;
	.reg .b64 	%rd<24>;


	ld.param.u64 	%rd4, [asort_desc_f32_param_0];
	ld.param.u64 	%rd3, [asort_desc_f32_param_1];
	ld.param.u32 	%r15, [asort_desc_f32_param_2];
	ld.param.u32 	%r16, [asort_desc_f32_param_3];
	cvta.to.global.u64 	%rd1, %rd4;
	mov.u32 	%r1, %tid.x;
	setp.ge.s32 	%p1, %r1, %r16;
	@%p1 bra 	$L__BB1_19;

	mov.u32 	%r17, %ctaid.x;
	mul.lo.s32 	%r18, %r17, %r15;
	cvt.s64.s32 	%rd2, %r18;
	shl.b32 	%r19, %r1, 2;
	mov.u32 	%r20, dst_row;
	add.s32 	%r2, %r20, %r19;
	st.shared.u32 	[%r2], %r1;
	bar.sync 	0;
	setp.lt.s32 	%p2, %r16, 2;
	@%p2 bra 	$L__BB1_17;

	mov.u32 	%r27, 2;

$L__BB1_3:
	setp.eq.s32 	%p3, %r27, 0;
	@%p3 bra 	$L__BB1_16;

	shr.u32 	%r28, %r27, 1;
	and.b32  	%r5, %r27, %r1;

$L__BB1_5:
	xor.b32  	%r7, %r28, %r1;
	setp.le.s32 	%p4, %r7, %r1;
	@%p4 bra 	$L__BB1_15;

	setp.eq.s32 	%p5, %r5, 0;
	shl.b32 	%r22, %r7, 2;
	add.s32 	%r8, %r20, %r22;
	@%p5 bra 	$L__BB1_11;

	ld.shared.u32 	%r9, [%r8];
	setp.ge.s32 	%p6, %r9, %r15;
	ld.shared.u32 	%r10, [%r2];
	@%p6 bra 	$L__BB1_10;

	setp.ge.s32 	%p7, %r10, %r15;
	@%p7 bra 	$L__BB1_15;

	cvt.s64.s32 	%rd5, %r10;
	add.s64 	%rd6, %rd5, %rd2;
	shl.b64 	%rd7, %rd6, 2;
	add.s64 	%rd8, %rd1, %rd7;
	cvt.s64.s32 	%rd9, %r9;
	add.s64 	%rd10, %rd9, %rd2;
	shl.b64 	%rd11, %rd10, 2;
	add.s64 	%rd12, %rd1, %rd11;
	ld.global.f32 	%f1, [%rd12];
	ld.global.f32 	%f2, [%rd8];
	setp.leu.f32 	%p8, %f2, %f1;
	@%p8 bra 	$L__BB1_15;

$L__BB1_10:
	st.shared.u32 	[%r2], %r9;
	st.shared.u32 	[%r8], %r10;
	bra.uni 	$L__BB1_15;

$L__BB1_11:
	ld.shared.u32 	%r11, [%r2];
	setp.ge.s32 	%p9, %r11, %r15;
	ld.shared.u32 	%r12, [%r8];
	@%p9 bra 	$L__BB1_14;

	setp.ge.s32 	%p10, %r12, %r15;
	@%p10 bra 	$L__BB1_15;

	cvt.s64.s32 	%rd13, %r11;
	add.s64 	%rd14, %rd13, %rd2;
	shl.b64 	%rd15, %rd14, 2;
	add.s64 	%rd16, %rd1, %rd15;
	cvt.s64.s32 	%rd17, %r12;
	add.s64 	%rd18, %rd17, %rd2;
	shl.b64 	%rd19, %rd18, 2;
	add.s64 	%rd20, %rd1, %rd19;
	ld.global.f32 	%f3, [%rd20];
	ld.global.f32 	%f4, [%rd16];
	setp.geu.f32 	%p11, %f4, %f3;
	@%p11 bra 	$L__BB1_15;

$L__BB1_14:
	st.shared.u32 	[%r2], %r12;
	st.shared.u32 	[%r8], %r11;

$L__BB1_15:
	bar.sync 	0;
	shr.u32 	%r28, %r28, 1;
	setp.ne.s32 	%p12, %r28, 0;
	@%p12 bra 	$L__BB1_5;

$L__BB1_16:
	shl.b32 	%r27, %r27, 1;
	setp.le.s32 	%p13, %r27, %r16;
	@%p13 bra 	$L__BB1_3;

$L__BB1_17:
	setp.ge.s32 	%p14, %r1, %r15;
	@%p14 bra 	$L__BB1_19;

	cvt.u32.u64 	%r24, %rd2;
	ld.shared.u32 	%r25, [%r2];
	add.s32 	%r26, %r24, %r1;
	cvta.to.global.u64 	%rd21, %rd3;
	mul.wide.s32 	%rd22, %r26, 4;
	add.s64 	%rd23, %rd21, %rd22;
	st.global.u32 	[%rd23], %r25;

$L__BB1_19:
	ret;

}
	// .globl	asort_asc_f64
.visible .entry asort_asc_f64(
	.param .u64 asort_asc_f64_param_0,
	.param .u64 asort_asc_f64_param_1,
	.param .u32 asort_asc_f64_param_2,
	.param .u32 asort_asc_f64_param_3
)
{
	.reg .pred 	%p<15>;
	.reg .b32 	%r<29>;
	.reg .f64 	%fd<5>;
	.reg .b64 	%rd<24>;


	ld.param.u64 	%rd4, [asort_asc_f64_param_0];
	ld.param.u64 	%rd3, [asort_asc_f64_param_1];
	ld.param.u32 	%r15, [asort_asc_f64_param_2];
	ld.param.u32 	%r16, [asort_asc_f64_param_3];
	cvta.to.global.u64 	%rd1, %rd4;
	mov.u32 	%r1, %tid.x;
	setp.ge.s32 	%p1, %r1, %r16;
	@%p1 bra 	$L__BB2_19;

	mov.u32 	%r17, %ctaid.x;
	mul.lo.s32 	%r18, %r17, %r15;
	cvt.s64.s32 	%rd2, %r18;
	shl.b32 	%r19, %r1, 2;
	mov.u32 	%r20, dst_row;
	add.s32 	%r2, %r20, %r19;
	st.shared.u32 	[%r2], %r1;
	bar.sync 	0;
	setp.lt.s32 	%p2, %r16, 2;
	@%p2 bra 	$L__BB2_17;

	mov.u32 	%r27, 2;

$L__BB2_3:
	setp.eq.s32 	%p3, %r27, 0;
	@%p3 bra 	$L__BB2_16;

	shr.u32 	%r28, %r27, 1;
	and.b32  	%r5, %r27, %r1;

$L__BB2_5:
	xor.b32  	%r7, %r28, %r1;
	setp.le.s32 	%p4, %r7, %r1;
	@%p4 bra 	$L__BB2_15;

	setp.eq.s32 	%p5, %r5, 0;
	shl.b32 	%r22, %r7, 2;
	add.s32 	%r8, %r20, %r22;
	@%p5 bra 	$L__BB2_11;

	ld.shared.u32 	%r9, [%r8];
	setp.ge.s32 	%p6, %r9, %r15;
	ld.shared.u32 	%r10, [%r2];
	@%p6 bra 	$L__BB2_10;

	setp.ge.s32 	%p7, %r10, %r15;
	@%p7 bra 	$L__BB2_15;

	cvt.s64.s32 	%rd5, %r10;
	add.s64 	%rd6, %rd5, %rd2;
	shl.b64 	%rd7, %rd6, 3;
	add.s64 	%rd8, %rd1, %rd7;
	cvt.s64.s32 	%rd9, %r9;
	add.s64 	%rd10, %rd9, %rd2;
	shl.b64 	%rd11, %rd10, 3;
	add.s64 	%rd12, %rd1, %rd11;
	ld.global.f64 	%fd1, [%rd12];
	ld.global.f64 	%fd2, [%rd8];
	setp.geu.f64 	%p8, %fd2, %fd1;
	@%p8 bra 	$L__BB2_15;

$L__BB2_10:
	st.shared.u32 	[%r2], %r9;
	st.shared.u32 	[%r8], %r10;
	bra.uni 	$L__BB2_15;

$L__BB2_11:
	ld.shared.u32 	%r11, [%r2];
	setp.ge.s32 	%p9, %r11, %r15;
	ld.shared.u32 	%r12, [%r8];
	@%p9 bra 	$L__BB2_14;

	setp.ge.s32 	%p10, %r12, %r15;
	@%p10 bra 	$L__BB2_15;

	cvt.s64.s32 	%rd13, %r11;
	add.s64 	%rd14, %rd13, %rd2;
	shl.b64 	%rd15, %rd14, 3;
	add.s64 	%rd16, %rd1, %rd15;
	cvt.s64.s32 	%rd17, %r12;
	add.s64 	%rd18, %rd17, %rd2;
	shl.b64 	%rd19, %rd18, 3;
	add.s64 	%rd20, %rd1, %rd19;
	ld.global.f64 	%fd3, [%rd20];
	ld.global.f64 	%fd4, [%rd16];
	setp.leu.f64 	%p11, %fd4, %fd3;
	@%p11 bra 	$L__BB2_15;

$L__BB2_14:
	st.shared.u32 	[%r2], %r12;
	st.shared.u32 	[%r8], %r11;

$L__BB2_15:
	bar.sync 	0;
	shr.u32 	%r28, %r28, 1;
	setp.ne.s32 	%p12, %r28, 0;
	@%p12 bra 	$L__BB2_5;

$L__BB2_16:
	shl.b32 	%r27, %r27, 1;
	setp.le.s32 	%p13, %r27, %r16;
	@%p13 bra 	$L__BB2_3;

$L__BB2_17:
	setp.ge.s32 	%p14, %r1, %r15;
	@%p14 bra 	$L__BB2_19;

	cvt.u32.u64 	%r24, %rd2;
	ld.shared.u32 	%r25, [%r2];
	add.s32 	%r26, %r24, %r1;
	cvta.to.global.u64 	%rd21, %rd3;
	mul.wide.s32 	%rd22, %r26, 4;
	add.s64 	%rd23, %rd21, %rd22;
	st.global.u32 	[%rd23], %r25;

$L__BB2_19:
	ret;

}
	// .globl	asort_desc_f64
.visible .entry asort_desc_f64(
	.param .u64 asort_desc_f64_param_0,
	.param .u64 asort_desc_f64_param_1,
	.param .u32 asort_desc_f64_param_2,
	.param .u32 asort_desc_f64_param_3
)
{
	.reg .pred 	%p<15>;
	.reg .b32 	%r<29>;
	.reg .f64 	%fd<5>;
	.reg .b64 	%rd<24>;


	ld.param.u64 	%rd4, [asort_desc_f64_param_0];
	ld.param.u64 	%rd3, [asort_desc_f64_param_1];
	ld.param.u32 	%r15, [asort_desc_f64_param_2];
	ld.param.u32 	%r16, [asort_desc_f64_param_3];
	cvta.to.global.u64 	%rd1, %rd4;
	mov.u32 	%r1, %tid.x;
	setp.ge.s32 	%p1, %r1, %r16;
	@%p1 bra 	$L__BB3_19;

	mov.u32 	%r17, %ctaid.x;
	mul.lo.s32 	%r18, %r17, %r15;
	cvt.s64.s32 	%rd2, %r18;
	shl.b32 	%r19, %r1, 2;
	mov.u32 	%r20, dst_row;
	add.s32 	%r2, %r20, %r19;
	st.shared.u32 	[%r2], %r1;
	bar.sync 	0;
	setp.lt.s32 	%p2, %r16, 2;
	@%p2 bra 	$L__BB3_17;

	mov.u32 	%r27, 2;

$L__BB3_3:
	setp.eq.s32 	%p3, %r27, 0;
	@%p3 bra 	$L__BB3_16;

	shr.u32 	%r28, %r27, 1;
	and.b32  	%r5, %r27, %r1;

$L__BB3_5:
	xor.b32  	%r7, %r28, %r1;
	setp.le.s32 	%p4, %r7, %r1;
	@%p4 bra 	$L__BB3_15;

	setp.eq.s32 	%p5, %r5, 0;
	shl.b32 	%r22, %r7, 2;
	add.s32 	%r8, %r20, %r22;
	@%p5 bra 	$L__BB3_11;

	ld.shared.u32 	%r9, [%r8];
	setp.ge.s32 	%p6, %r9, %r15;
	ld.shared.u32 	%r10, [%r2];
	@%p6 bra 	$L__BB3_10;

	setp.ge.s32 	%p7, %r10, %r15;
	@%p7 bra 	$L__BB3_15;

	cvt.s64.s32 	%rd5, %r10;
	add.s64 	%rd6, %rd5, %rd2;
	shl.b64 	%rd7, %rd6, 3;
	add.s64 	%rd8, %rd1, %rd7;
	cvt.s64.s32 	%rd9, %r9;
	add.s64 	%rd10, %rd9, %rd2;
	shl.b64 	%rd11, %rd10, 3;
	add.s64 	%rd12, %rd1, %rd11;
	ld.global.f64 	%fd1, [%rd12];
	ld.global.f64 	%fd2, [%rd8];
	setp.leu.f64 	%p8, %fd2, %fd1;
	@%p8 bra 	$L__BB3_15;

$L__BB3_10:
	st.shared.u32 	[%r2], %r9;
	st.shared.u32 	[%r8], %r10;
	bra.uni 	$L__BB3_15;

$L__BB3_11:
	ld.shared.u32 	%r11, [%r2];
	setp.ge.s32 	%p9, %r11, %r15;
	ld.shared.u32 	%r12, [%r8];
	@%p9 bra 	$L__BB3_14;

	setp.ge.s32 	%p10, %r12, %r15;
	@%p10 bra 	$L__BB3_15;

	cvt.s64.s32 	%rd13, %r11;
	add.s64 	%rd14, %rd13, %rd2;
	shl.b64 	%rd15, %rd14, 3;
	add.s64 	%rd16, %rd1, %rd15;
	cvt.s64.s32 	%rd17, %r12;
	add.s64 	%rd18, %rd17, %rd2;
	shl.b64 	%rd19, %rd18, 3;
	add.s64 	%rd20, %rd1, %rd19;
	ld.global.f64 	%fd3, [%rd20];
	ld.global.f64 	%fd4, [%rd16];
	setp.geu.f64 	%p11, %fd4, %fd3;
	@%p11 bra 	$L__BB3_15;

$L__BB3_14:
	st.shared.u32 	[%r2], %r12;
	st.shared.u32 	[%r8], %r11;

$L__BB3_15:
	bar.sync 	0;
	shr.u32 	%r28, %r28, 1;
	setp.ne.s32 	%p12, %r28, 0;
	@%p12 bra 	$L__BB3_5;

$L__BB3_16:
	shl.b32 	%r27, %r27, 1;
	setp.le.s32 	%p13, %r27, %r16;
	@%p13 bra 	$L__BB3_3;

$L__BB3_17:
	setp.ge.s32 	%p14, %r1, %r15;
	@%p14 bra 	$L__BB3_19;

	cvt.u32.u64 	%r24, %rd2;
	ld.shared.u32 	%r25, [%r2];
	add.s32 	%r26, %r24, %r1;
	cvta.to.global.u64 	%rd21, %rd3;
	mul.wide.s32 	%rd22, %r26, 4;
	add.s64 	%rd23, %rd21, %rd22;
	st.global.u32 	[%rd23], %r25;

$L__BB3_19:
	ret;

}
	// .globl	asort_asc_u8
.visible .entry asort_asc_u8(
	.param .u64 asort_asc_u8_param_0,
	.param .u64 asort_asc_u8_param_1,
	.param .u32 asort_asc_u8_param_2,
	.param .u32 asort_asc_u8_param_3
)
{
	.reg .pred 	%p<15>;
	.reg .b16 	%rs<5>;
	.reg .b32 	%r<29>;
	.reg .b64 	%rd<20>;


	ld.param.u64 	%rd4, [asort_asc_u8_param_0];
	ld.param.u64 	%rd3, [asort_asc_u8_param_1];
	ld.param.u32 	%r15, [asort_asc_u8_param_2];
	ld.param.u32 	%r16, [asort_asc_u8_param_3];
	cvta.to.global.u64 	%rd1, %rd4;
	mov.u32 	%r1, %tid.x;
	setp.ge.s32 	%p1, %r1, %r16;
	@%p1 bra 	$L__BB4_19;

	mov.u32 	%r17, %ctaid.x;
	mul.lo.s32 	%r18, %r17, %r15;
	cvt.s64.s32 	%rd2, %r18;
	shl.b32 	%r19, %r1, 2;
	mov.u32 	%r20, dst_row;
	add.s32 	%r2, %r20, %r19;
	st.shared.u32 	[%r2], %r1;
	bar.sync 	0;
	setp.lt.s32 	%p2, %r16, 2;
	@%p2 bra 	$L__BB4_17;

	mov.u32 	%r27, 2;

$L__BB4_3:
	setp.eq.s32 	%p3, %r27, 0;
	@%p3 bra 	$L__BB4_16;

	shr.u32 	%r28, %r27, 1;
	and.b32  	%r5, %r27, %r1;

$L__BB4_5:
	xor.b32  	%r7, %r28, %r1;
	setp.le.s32 	%p4, %r7, %r1;
	@%p4 bra 	$L__BB4_15;

	setp.eq.s32 	%p5, %r5, 0;
	shl.b32 	%r22, %r7, 2;
	add.s32 	%r8, %r20, %r22;
	@%p5 bra 	$L__BB4_11;

	ld.shared.u32 	%r9, [%r8];
	setp.ge.s32 	%p6, %r9, %r15;
	ld.shared.u32 	%r10, [%r2];
	@%p6 bra 	$L__BB4_10;

	setp.ge.s32 	%p7, %r10, %r15;
	@%p7 bra 	$L__BB4_15;

	cvt.s64.s32 	%rd5, %r10;
	add.s64 	%rd6, %rd5, %rd2;
	add.s64 	%rd7, %rd1, %rd6;
	cvt.s64.s32 	%rd8, %r9;
	add.s64 	%rd9, %rd8, %rd2;
	add.s64 	%rd10, %rd1, %rd9;
	ld.global.u8 	%rs1, [%rd10];
	ld.global.u8 	%rs2, [%rd7];
	setp.ge.u16 	%p8, %rs2, %rs1;
	@%p8 bra 	$L__BB4_15;

$L__BB4_10:
	st.shared.u32 	[%r2], %r9;
	st.shared.u32 	[%r8], %r10;
	bra.uni 	$L__BB4_15;

$L__BB4_11:
	ld.shared.u32 	%r11, [%r2];
	setp.ge.s32 	%p9, %r11, %r15;
	ld.shared.u32 	%r12, [%r8];
	@%p9 bra 	$L__BB4_14;

	setp.ge.s32 	%p10, %r12, %r15;
	@%p10 bra 	$L__BB4_15;

	cvt.s64.s32 	%rd11, %r11;
	add.s64 	%rd12, %rd11, %rd2;
	add.s64 	%rd13, %rd1, %rd12;
	cvt.s64.s32 	%rd14, %r12;
	add.s64 	%rd15, %rd14, %rd2;
	add.s64 	%rd16, %rd1, %rd15;
	ld.global.u8 	%rs3, [%rd16];
	ld.global.u8 	%rs4, [%rd13];
	setp.le.u16 	%p11, %rs4, %rs3;
	@%p11 bra 	$L__BB4_15;

$L__BB4_14:
	st.shared.u32 	[%r2], %r12;
	st.shared.u32 	[%r8], %r11;

$L__BB4_15:
	bar.sync 	0;
	shr.u32 	%r28, %r28, 1;
	setp.ne.s32 	%p12, %r28, 0;
	@%p12 bra 	$L__BB4_5;

$L__BB4_16:
	shl.b32 	%r27, %r27, 1;
	setp.le.s32 	%p13, %r27, %r16;
	@%p13 bra 	$L__BB4_3;

$L__BB4_17:
	setp.ge.s32 	%p14, %r1, %r15;
	@%p14 bra 	$L__BB4_19;

	cvt.u32.u64 	%r24, %rd2;
	ld.shared.u32 	%r25, [%r2];
	add.s32 	%r26, %r24, %r1;
	cvta.to.global.u64 	%rd17, %rd3;
	mul.wide.s32 	%rd18, %r26, 4;
	add.s64 	%rd19, %rd17, %rd18;
	st.global.u32 	[%rd19], %r25;

$L__BB4_19:
	ret;

}
	// .globl	asort_desc_u8
.visible .entry asort_desc_u8(
	.param .u64 asort_desc_u8_param_0,
	.param .u64 asort_desc_u8_param_1,
	.param .u32 asort_desc_u8_param_2,
	.param .u32 asort_desc_u8_param_3
)
{
	.reg .pred 	%p<15>;
	.reg .b16 	%rs<5>;
	.reg .b32 	%r<29>;
	.reg .b64 	%rd<20>;


	ld.param.u64 	%rd4, [asort_desc_u8_param_0];
	ld.param.u64 	%rd3, [asort_desc_u8_param_1];
	ld.param.u32 	%r15, [asort_desc_u8_param_2];
	ld.param.u32 	%r16, [asort_desc_u8_param_3];
	cvta.to.global.u64 	%rd1, %rd4;
	mov.u32 	%r1, %tid.x;
	setp.ge.s32 	%p1, %r1, %r16;
	@%p1 bra 	$L__BB5_19;

	mov.u32 	%r17, %ctaid.x;
	mul.lo.s32 	%r18, %r17, %r15;
	cvt.s64.s32 	%rd2, %r18;
	shl.b32 	%r19, %r1, 2;
	mov.u32 	%r20, dst_row;
	add.s32 	%r2, %r20, %r19;
	st.shared.u32 	[%r2], %r1;
	bar.sync 	0;
	setp.lt.s32 	%p2, %r16, 2;
	@%p2 bra 	$L__BB5_17;

	mov.u32 	%r27, 2;

$L__BB5_3:
	setp.eq.s32 	%p3, %r27, 0;
	@%p3 bra 	$L__BB5_16;

	shr.u32 	%r28, %r27, 1;
	and.b32  	%r5, %r27, %r1;

$L__BB5_5:
	xor.b32  	%r7, %r28, %r1;
	setp.le.s32 	%p4, %r7, %r1;
	@%p4 bra 	$L__BB5_15;

	setp.eq.s32 	%p5, %r5, 0;
	shl.b32 	%r22, %r7, 2;
	add.s32 	%r8, %r20, %r22;
	@%p5 bra 	$L__BB5_11;

	ld.shared.u32 	%r9, [%r8];
	setp.ge.s32 	%p6, %r9, %r15;
	ld.shared.u32 	%r10, [%r2];
	@%p6 bra 	$L__BB5_10;

	setp.ge.s32 	%p7, %r10, %r15;
	@%p7 bra 	$L__BB5_15;

	cvt.s64.s32 	%rd5, %r10;
	add.s64 	%rd6, %rd5, %rd2;
	add.s64 	%rd7, %rd1, %rd6;
	cvt.s64.s32 	%rd8, %r9;
	add.s64 	%rd9, %rd8, %rd2;
	add.s64 	%rd10, %rd1, %rd9;
	ld.global.u8 	%rs1, [%rd10];
	ld.global.u8 	%rs2, [%rd7];
	setp.le.u16 	%p8, %rs2, %rs1;
	@%p8 bra 	$L__BB5_15;

$L__BB5_10:
	st.shared.u32 	[%r2], %r9;
	st.shared.u32 	[%r8], %r10;
	bra.uni 	$L__BB5_15;

$L__BB5_11:
	ld.shared.u32 	%r11, [%r2];
	setp.ge.s32 	%p9, %r11, %r15;
	ld.shared.u32 	%r12, [%r8];
	@%p9 bra 	$L__BB5_14;

	setp.ge.s32 	%p10, %r12, %r15;
	@%p10 bra 	$L__BB5_15;

	cvt.s64.s32 	%rd11, %r11;
	add.s64 	%rd12, %rd11, %rd2;
	add.s64 	%rd13, %rd1, %rd12;
	cvt.s64.s32 	%rd14, %r12;
	add.s64 	%rd15, %rd14, %rd2;
	add.s64 	%rd16, %rd1, %rd15;
	ld.global.u8 	%rs3, [%rd16];
	ld.global.u8 	%rs4, [%rd13];
	setp.ge.u16 	%p11, %rs4, %rs3;
	@%p11 bra 	$L__BB5_15;

$L__BB5_14:
	st.shared.u32 	[%r2], %r12;
	st.shared.u32 	[%r8], %r11;

$L__BB5_15:
	bar.sync 	0;
	shr.u32 	%r28, %r28, 1;
	setp.ne.s32 	%p12, %r28, 0;
	@%p12 bra 	$L__BB5_5;

$L__BB5_16:
	shl.b32 	%r27, %r27, 1;
	setp.le.s32 	%p13, %r27, %r16;
	@%p13 bra 	$L__BB5_3;

$L__BB5_17:
	setp.ge.s32 	%p14, %r1, %r15;
	@%p14 bra 	$L__BB5_19;

	cvt.u32.u64 	%r24, %rd2;
	ld.shared.u32 	%r25, [%r2];
	add.s32 	%r26, %r24, %r1;
	cvta.to.global.u64 	%rd17, %rd3;
	mul.wide.s32 	%rd18, %r26, 4;
	add.s64 	%rd19, %rd17, %rd18;
	st.global.u32 	[%rd19], %r25;

$L__BB5_19:
	ret;

}
	// .globl	asort_asc_u32
.visible .entry asort_asc_u32(
	.param .u64 asort_asc_u32_param_0,
	.param .u64 asort_asc_u32_param_1,
	.param .u32 asort_asc_u32_param_2,
	.param .u32 asort_asc_u32_param_3
)
{
	.reg .pred 	%p<15>;
	.reg .b32 	%r<33>;
	.reg .b64 	%rd<24>;


	ld.param.u64 	%rd4, [asort_asc_u32_param_0];
	ld.param.u64 	%rd3, [asort_asc_u32_param_1];
	ld.param.u32 	%r15, [asort_asc_u32_param_2];
	ld.param.u32 	%r16, [asort_asc_u32_param_3];
	cvta.to.global.u64 	%rd1, %rd4;
	mov.u32 	%r1, %tid.x;
	setp.ge.s32 	%p1, %r1, %r16;
	@%p1 bra 	$L__BB6_19;

	mov.u32 	%r17, %ctaid.x;
	mul.lo.s32 	%r18, %r17, %r15;
	cvt.s64.s32 	%rd2, %r18;
	shl.b32 	%r19, %r1, 2;
	mov.u32 	%r20, dst_row;
	add.s32 	%r2, %r20, %r19;
	st.shared.u32 	[%r2], %r1;
	bar.sync 	0;
	setp.lt.s32 	%p2, %r16, 2;
	@%p2 bra 	$L__BB6_17;

	mov.u32 	%r31, 2;

$L__BB6_3:
	setp.eq.s32 	%p3, %r31, 0;
	@%p3 bra 	$L__BB6_16;

	shr.u32 	%r32, %r31, 1;
	and.b32  	%r5, %r31, %r1;

$L__BB6_5:
	xor.b32  	%r7, %r32, %r1;
	setp.le.s32 	%p4, %r7, %r1;
	@%p4 bra 	$L__BB6_15;

	setp.eq.s32 	%p5, %r5, 0;
	shl.b32 	%r22, %r7, 2;
	add.s32 	%r8, %r20, %r22;
	@%p5 bra 	$L__BB6_11;

	ld.shared.u32 	%r9, [%r8];
	setp.ge.s32 	%p6, %r9, %r15;
	ld.shared.u32 	%r10, [%r2];
	@%p6 bra 	$L__BB6_10;

	setp.ge.s32 	%p7, %r10, %r15;
	@%p7 bra 	$L__BB6_15;

	cvt.s64.s32 	%rd5, %r10;
	add.s64 	%rd6, %rd5, %rd2;
	shl.b64 	%rd7, %rd6, 2;
	add.s64 	%rd8, %rd1, %rd7;
	cvt.s64.s32 	%rd9, %r9;
	add.s64 	%rd10, %rd9, %rd2;
	shl.b64 	%rd11, %rd10, 2;
	add.s64 	%rd12, %rd1, %rd11;
	ld.global.u32 	%r24, [%rd12];
	ld.global.u32 	%r25, [%rd8];
	setp.ge.u32 	%p8, %r25, %r24;
	@%p8 bra 	$L__BB6_15;

$L__BB6_10:
	st.shared.u32 	[%r2], %r9;
	st.shared.u32 	[%r8], %r10;
	bra.uni 	$L__BB6_15;

$L__BB6_11:
	ld.shared.u32 	%r11, [%r2];
	setp.ge.s32 	%p9, %r11, %r15;
	ld.shared.u32 	%r12, [%r8];
	@%p9 bra 	$L__BB6_14;

	setp.ge.s32 	%p10, %r12, %r15;
	@%p10 bra 	$L__BB6_15;

	cvt.s64.s32 	%rd13, %r11;
	add.s64 	%rd14, %rd13, %rd2;
	shl.b64 	%rd15, %rd14, 2;
	add.s64 	%rd16, %rd1, %rd15;
	cvt.s64.s32 	%rd17, %r12;
	add.s64 	%rd18, %rd17, %rd2;
	shl.b64 	%rd19, %rd18, 2;
	add.s64 	%rd20, %rd1, %rd19;
	ld.global.u32 	%r26, [%rd20];
	ld.global.u32 	%r27, [%rd16];
	setp.le.u32 	%p11, %r27, %r26;
	@%p11 bra 	$L__BB6_15;

$L__BB6_14:
	st.shared.u32 	[%r2], %r12;
	st.shared.u32 	[%r8], %r11;

$L__BB6_15:
	bar.sync 	0;
	shr.u32 	%r32, %r32, 1;
	setp.ne.s32 	%p12, %r32, 0;
	@%p12 bra 	$L__BB6_5;

$L__BB6_16:
	shl.b32 	%r31, %r31, 1;
	setp.le.s32 	%p13, %r31, %r16;
	@%p13 bra 	$L__BB6_3;

$L__BB6_17:
	setp.ge.s32 	%p14, %r1, %r15;
	@%p14 bra 	$L__BB6_19;

	cvt.u32.u64 	%r28, %rd2;
	ld.shared.u32 	%r29, [%r2];
	add.s32 	%r30, %r28, %r1;
	cvta.to.global.u64 	%rd21, %rd3;
	mul.wide.s32 	%rd22, %r30, 4;
	add.s64 	%rd23, %rd21, %rd22;
	st.global.u32 	[%rd23], %r29;

$L__BB6_19:
	ret;

}
	// .globl	asort_desc_u32
.visible .entry asort_desc_u32(
	.param .u64 asort_desc_u32_param_0,
	.param .u64 asort_desc_u32_param_1,
	.param .u32 asort_desc_u32_param_2,
	.param .u32 asort_desc_u32_param_3
)
{
	.reg .pred 	%p<15>;
	.reg .b32 	%r<33>;
	.reg .b64 	%rd<24>;


	ld.param.u64 	%rd4, [asort_desc_u32_param_0];
	ld.param.u64 	%rd3, [asort_desc_u32_param_1];
	ld.param.u32 	%r15, [asort_desc_u32_param_2];
	ld.param.u32 	%r16, [asort_desc_u32_param_3];
	cvta.to.global.u64 	%rd1, %rd4;
	mov.u32 	%r1, %tid.x;
	setp.ge.s32 	%p1, %r1, %r16;
	@%p1 bra 	$L__BB7_19;

	mov.u32 	%r17, %ctaid.x;
	mul.lo.s32 	%r18, %r17, %r15;
	cvt.s64.s32 	%rd2, %r18;
	shl.b32 	%r19, %r1, 2;
	mov.u32 	%r20, dst_row;
	add.s32 	%r2, %r20, %r19;
	st.shared.u32 	[%r2], %r1;
	bar.sync 	0;
	setp.lt.s32 	%p2, %r16, 2;
	@%p2 bra 	$L__BB7_17;

	mov.u32 	%r31, 2;

$L__BB7_3:
	setp.eq.s32 	%p3, %r31, 0;
	@%p3 bra 	$L__BB7_16;

	shr.u32 	%r32, %r31, 1;
	and.b32  	%r5, %r31, %r1;

$L__BB7_5:
	xor.b32  	%r7, %r32, %r1;
	setp.le.s32 	%p4, %r7, %r1;
	@%p4 bra 	$L__BB7_15;

	setp.eq.s32 	%p5, %r5, 0;
	shl.b32 	%r22, %r7, 2;
	add.s32 	%r8, %r20, %r22;
	@%p5 bra 	$L__BB7_11;

	ld.shared.u32 	%r9, [%r8];
	setp.ge.s32 	%p6, %r9, %r15;
	ld.shared.u32 	%r10, [%r2];
	@%p6 bra 	$L__BB7_10;

	setp.ge.s32 	%p7, %r10, %r15;
	@%p7 bra 	$L__BB7_15;

	cvt.s64.s32 	%rd5, %r10;
	add.s64 	%rd6, %rd5, %rd2;
	shl.b64 	%rd7, %rd6, 2;
	add.s64 	%rd8, %rd1, %rd7;
	cvt.s64.s32 	%rd9, %r9;
	add.s64 	%rd10, %rd9, %rd2;
	shl.b64 	%rd11, %rd10, 2;
	add.s64 	%rd12, %rd1, %rd11;
	ld.global.u32 	%r24, [%rd12];
	ld.global.u32 	%r25, [%rd8];
	setp.le.u32 	%p8, %r25, %r24;
	@%p8 bra 	$L__BB7_15;

$L__BB7_10:
	st.shared.u32 	[%r2], %r9;
	st.shared.u32 	[%r8], %r10;
	bra.uni 	$L__BB7_15;

$L__BB7_11:
	ld.shared.u32 	%r11, [%r2];
	setp.ge.s32 	%p9, %r11, %r15;
	ld.shared.u32 	%r12, [%r8];
	@%p9 bra 	$L__BB7_14;

	setp.ge.s32 	%p10, %r12, %r15;
	@%p10 bra 	$L__BB7_15;

	cvt.s64.s32 	%rd13, %r11;
	add.s64 	%rd14, %rd13, %rd2;
	shl.b64 	%rd15, %rd14, 2;
	add.s64 	%rd16, %rd1, %rd15;
	cvt.s64.s32 	%rd17, %r12;
	add.s64 	%rd18, %rd17, %rd2;
	shl.b64 	%rd19, %rd18, 2;
	add.s64 	%rd20, %rd1, %rd19;
	ld.global.u32 	%r26, [%rd20];
	ld.global.u32 	%r27, [%rd16];
	setp.ge.u32 	%p11, %r27, %r26;
	@%p11 bra 	$L__BB7_15;

$L__BB7_14:
	st.shared.u32 	[%r2], %r12;
	st.shared.u32 	[%r8], %r11;

$L__BB7_15:
	bar.sync 	0;
	shr.u32 	%r32, %r32, 1;
	setp.ne.s32 	%p12, %r32, 0;
	@%p12 bra 	$L__BB7_5;

$L__BB7_16:
	shl.b32 	%r31, %r31, 1;
	setp.le.s32 	%p13, %r31, %r16;
	@%p13 bra 	$L__BB7_3;

$L__BB7_17:
	setp.ge.s32 	%p14, %r1, %r15;
	@%p14 bra 	$L__BB7_19;

	cvt.u32.u64 	%r28, %rd2;
	ld.shared.u32 	%r29, [%r2];
	add.s32 	%r30, %r28, %r1;
	cvta.to.global.u64 	%rd21, %rd3;
	mul.wide.s32 	%rd22, %r30, 4;
	add.s64 	%rd23, %rd21, %rd22;
	st.global.u32 	[%rd23], %r29;

$L__BB7_19:
	ret;

}
	// .globl	asort_asc_i64
.visible .entry asort_asc_i64(
	.param .u64 asort_asc_i64_param_0,
	.param .u64 asort_asc_i64_param_1,
	.param .u32 asort_asc_i64_param_2,
	.param .u32 asort_asc_i64_param_3
)
{
	.reg .pred 	%p<15>;
	.reg .b32 	%r<29>;
	.reg .b64 	%rd<28>;


	ld.param.u64 	%rd4, [asort_asc_i64_param_0];
	ld.param.u64 	%rd3, [asort_asc_i64_param_1];
	ld.param.u32 	%r15, [asort_asc_i64_param_2];
	ld.param.u32 	%r16, [asort_asc_i64_param_3];
	cvta.to.global.u64 	%rd1, %rd4;
	mov.u32 	%r1, %tid.x;
	setp.ge.s32 	%p1, %r1, %r16;
	@%p1 bra 	$L__BB8_19;

	mov.u32 	%r17, %ctaid.x;
	mul.lo.s32 	%r18, %r17, %r15;
	cvt.s64.s32 	%rd2, %r18;
	shl.b32 	%r19, %r1, 2;
	mov.u32 	%r20, dst_row;
	add.s32 	%r2, %r20, %r19;
	st.shared.u32 	[%r2], %r1;
	bar.sync 	0;
	setp.lt.s32 	%p2, %r16, 2;
	@%p2 bra 	$L__BB8_17;

	mov.u32 	%r27, 2;

$L__BB8_3:
	setp.eq.s32 	%p3, %r27, 0;
	@%p3 bra 	$L__BB8_16;

	shr.u32 	%r28, %r27, 1;
	and.b32  	%r5, %r27, %r1;

$L__BB8_5:
	xor.b32  	%r7, %r28, %r1;
	setp.le.s32 	%p4, %r7, %r1;
	@%p4 bra 	$L__BB8_15;

	setp.eq.s32 	%p5, %r5, 0;
	shl.b32 	%r22, %r7, 2;
	add.s32 	%r8, %r20, %r22;
	@%p5 bra 	$L__BB8_11;

	ld.shared.u32 	%r9, [%r8];
	setp.ge.s32 	%p6, %r9, %r15;
	ld.shared.u32 	%r10, [%r2];
	@%p6 bra 	$L__BB8_10;

	setp.ge.s32 	%p7, %r10, %r15;
	@%p7 bra 	$L__BB8_15;

	cvt.s64.s32 	%rd5, %r10;
	add.s64 	%rd6, %rd5, %rd2;
	shl.b64 	%rd7, %rd6, 3;
	add.s64 	%rd8, %rd1, %rd7;
	cvt.s64.s32 	%rd9, %r9;
	add.s64 	%rd10, %rd9, %rd2;
	shl.b64 	%rd11, %rd10, 3;
	add.s64 	%rd12, %rd1, %rd11;
	ld.global.u64 	%rd13, [%rd12];
	ld.global.u64 	%rd14, [%rd8];
	setp.ge.s64 	%p8, %rd14, %rd13;
	@%p8 bra 	$L__BB8_15;

$L__BB8_10:
	st.shared.u32 	[%r2], %r9;
	st.shared.u32 	[%r8], %r10;
	bra.uni 	$L__BB8_15;

$L__BB8_11:
	ld.shared.u32 	%r11, [%r2];
	setp.ge.s32 	%p9, %r11, %r15;
	ld.shared.u32 	%r12, [%r8];
	@%p9 bra 	$L__BB8_14;

	setp.ge.s32 	%p10, %r12, %r15;
	@%p10 bra 	$L__BB8_15;

	cvt.s64.s32 	%rd15, %r11;
	add.s64 	%rd16, %rd15, %rd2;
	shl.b64 	%rd17, %rd16, 3;
	add.s64 	%rd18, %rd1, %rd17;
	cvt.s64.s32 	%rd19, %r12;
	add.s64 	%rd20, %rd19, %rd2;
	shl.b64 	%rd21, %rd20, 3;
	add.s64 	%rd22, %rd1, %rd21;
	ld.global.u64 	%rd23, [%rd22];
	ld.global.u64 	%rd24, [%rd18];
	setp.le.s64 	%p11, %rd24, %rd23;
	@%p11 bra 	$L__BB8_15;

$L__BB8_14:
	st.shared.u32 	[%r2], %r12;
	st.shared.u32 	[%r8], %r11;

$L__BB8_15:
	bar.sync 	0;
	shr.u32 	%r28, %r28, 1;
	setp.ne.s32 	%p12, %r28, 0;
	@%p12 bra 	$L__BB8_5;

$L__BB8_16:
	shl.b32 	%r27, %r27, 1;
	setp.le.s32 	%p13, %r27, %r16;
	@%p13 bra 	$L__BB8_3;

$L__BB8_17:
	setp.ge.s32 	%p14, %r1, %r15;
	@%p14 bra 	$L__BB8_19;

	cvt.u32.u64 	%r24, %rd2;
	ld.shared.u32 	%r25, [%r2];
	add.s32 	%r26, %r24, %r1;
	cvta.to.global.u64 	%rd25, %rd3;
	mul.wide.s32 	%rd26, %r26, 4;
	add.s64 	%rd27, %rd25, %rd26;
	st.global.u32 	[%rd27], %r25;

$L__BB8_19:
	ret;

}
	// .globl	asort_desc_i64
.visible .entry asort_desc_i64(
	.param .u64 asort_desc_i64_param_0,
	.param .u64 asort_desc_i64_param_1,
	.param .u32 asort_desc_i64_param_2,
	.param .u32 asort_desc_i64_param_3
)
{
	.reg .pred 	%p<15>;
	.reg .b32 	%r<29>;
	.reg .b64 	%rd<28>;


	ld.param.u64 	%rd4, [asort_desc_i64_param_0];
	ld.param.u64 	%rd3, [asort_desc_i64_param_1];
	ld.param.u32 	%r15, [asort_desc_i64_param_2];
	ld.param.u32 	%r16, [asort_desc_i64_param_3];
	cvta.to.global.u64 	%rd1, %rd4;
	mov.u32 	%r1, %tid.x;
	setp.ge.s32 	%p1, %r1, %r16;
	@%p1 bra 	$L__BB9_19;

	mov.u32 	%r17, %ctaid.x;
	mul.lo.s32 	%r18, %r17, %r15;
	cvt.s64.s32 	%rd2, %r18;
	shl.b32 	%r19, %r1, 2;
	mov.u32 	%r20, dst_row;
	add.s32 	%r2, %r20, %r19;
	st.shared.u32 	[%r2], %r1;
	bar.sync 	0;
	setp.lt.s32 	%p2, %r16, 2;
	@%p2 bra 	$L__BB9_17;

	mov.u32 	%r27, 2;

$L__BB9_3:
	setp.eq.s32 	%p3, %r27, 0;
	@%p3 bra 	$L__BB9_16;

	shr.u32 	%r28, %r27, 1;
	and.b32  	%r5, %r27, %r1;

$L__BB9_5:
	xor.b32  	%r7, %r28, %r1;
	setp.le.s32 	%p4, %r7, %r1;
	@%p4 bra 	$L__BB9_15;

	setp.eq.s32 	%p5, %r5, 0;
	shl.b32 	%r22, %r7, 2;
	add.s32 	%r8, %r20, %r22;
	@%p5 bra 	$L__BB9_11;

	ld.shared.u32 	%r9, [%r8];
	setp.ge.s32 	%p6, %r9, %r15;
	ld.shared.u32 	%r10, [%r2];
	@%p6 bra 	$L__BB9_10;

	setp.ge.s32 	%p7, %r10, %r15;
	@%p7 bra 	$L__BB9_15;

	cvt.s64.s32 	%rd5, %r10;
	add.s64 	%rd6, %rd5, %rd2;
	shl.b64 	%rd7, %rd6, 3;
	add.s64 	%rd8, %rd1, %rd7;
	cvt.s64.s32 	%rd9, %r9;
	add.s64 	%rd10, %rd9, %rd2;
	shl.b64 	%rd11, %rd10, 3;
	add.s64 	%rd12, %rd1, %rd11;
	ld.global.u64 	%rd13, [%rd12];
	ld.global.u64 	%rd14, [%rd8];
	setp.le.s64 	%p8, %rd14, %rd13;
	@%p8 bra 	$L__BB9_15;

$L__BB9_10:
	st.shared.u32 	[%r2], %r9;
	st.shared.u32 	[%r8], %r10;
	bra.uni 	$L__BB9_15;

$L__BB9_11:
	ld.shared.u32 	%r11, [%r2];
	setp.ge.s32 	%p9, %r11, %r15;
	ld.shared.u32 	%r12, [%r8];
	@%p9 bra 	$L__BB9_14;

	setp.ge.s32 	%p10, %r12, %r15;
	@%p10 bra 	$L__BB9_15;

	cvt.s64.s32 	%rd15, %r11;
	add.s64 	%rd16, %rd15, %rd2;
	shl.b64 	%rd17, %rd16, 3;
	add.s64 	%rd18, %rd1, %rd17;
	cvt.s64.s32 	%rd19, %r12;
	add.s64 	%rd20, %rd19, %rd2;
	shl.b64 	%rd21, %rd20, 3;
	add.s64 	%rd22, %rd1, %rd21;
	ld.global.u64 	%rd23, [%rd22];
	ld.global.u64 	%rd24, [%rd18];
	setp.ge.s64 	%p11, %rd24, %rd23;
	@%p11 bra 	$L__BB9_15;

$L__BB9_14:
	st.shared.u32 	[%r2], %r12;
	st.shared.u32 	[%r8], %r11;

$L__BB9_15:
	bar.sync 	0;
	shr.u32 	%r28, %r28, 1;
	setp.ne.s32 	%p12, %r28, 0;
	@%p12 bra 	$L__BB9_5;

$L__BB9_16:
	shl.b32 	%r27, %r27, 1;
	setp.le.s32 	%p13, %r27, %r16;
	@%p13 bra 	$L__BB9_3;

$L__BB9_17:
	setp.ge.s32 	%p14, %r1, %r15;
	@%p14 bra 	$L__BB9_19;

	cvt.u32.u64 	%r24, %rd2;
	ld.shared.u32 	%r25, [%r2];
	add.s32 	%r26, %r24, %r1;
	cvta.to.global.u64 	%rd25, %rd3;
	mul.wide.s32 	%rd26, %r26, 4;
	add.s64 	%rd27, %rd25, %rd26;
	st.global.u32 	[%rd27], %r25;

$L__BB9_19:
	ret;

}

